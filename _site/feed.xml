<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning cơ bản</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 02 Jan 2017 20:48:13 -0500</pubDate>
    <lastBuildDate>Mon, 02 Jan 2017 20:48:13 -0500</lastBuildDate>
    <generator>Jekyll v3.3.1</generator>
    
      <item>
        <title>K-means Clustering</title>
        <description>&lt;div class=&quot;imgcap&quot;&gt;
&lt;div&gt;
&lt;a href=&quot;/2017/01/01/kmeans/&quot;&gt;
    &lt;img src=&quot;/assets/kmeans/kmeans11.gif&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
    &lt;!-- &lt;img src=&quot;/assets/rl/mdp.png&quot; height=&quot;206&quot;&gt; --&gt;
&lt;/div&gt;
&lt;div class=&quot;thecap&quot;&gt; K-means Clustering &lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Trong trang này:&lt;/strong&gt;
&lt;!-- MarkdownTOC --&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#-gioi-thieu&quot;&gt;1. Giới thiệu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-phan-tich-toan-hoc&quot;&gt;2. Phân tích toán học&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mot-so-ky-hieu-toan-hoc&quot;&gt;Một số ký hiệu toán học&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ham-mat-mat-va-bai-toan-toi-uu&quot;&gt;Hàm mất mát và bài toán tối ưu&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#thuat-toan-toi-uu-ham-mat-mat&quot;&gt;Thuật toán tối ưu hàm mất mát&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tom-tat-thuat-toan&quot;&gt;Tóm tắt thuật toán&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vi-du-tren-python&quot;&gt;Ví dụ trên Python&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gioi-thieu-bai-toan&quot;&gt;Giới thiệu bài toán&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hien-thi-du-lieu-tren-do-thi&quot;&gt;Hiển thị dữ liệu trên đồ thị&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cac-ham-so-can-thiet-cho-k-means-clustering&quot;&gt;Các hàm số cần thiết cho K-means clustering&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ket-qua-tim-duoc-bang-thu-vien-scikit-learn&quot;&gt;Kết quả tìm được bằng thư viện scikit-learn&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-thao-luan&quot;&gt;4. Thảo luận&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#han-che&quot;&gt;Hạn chế&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-tai-lieu-tham-khao&quot;&gt;5. Tài liệu tham khảo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-gioi-thieu&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-giới-thiệu&quot;&gt;1. Giới thiệu&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;/2016/12/28/linearregression/&quot;&gt;Trong bài trước&lt;/a&gt;, chúng ta đã làm quen với thuật toán Linear Regression - là thuật toán đơn giản nhất trong &lt;a href=&quot;/2016/12/27/categories/#supervised-learning-hoc-co-giam-sat&quot;&gt;Supervised learning&lt;/a&gt;. Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong &lt;a href=&quot;/2016/12/27/categories/#unsupervised-learning-hoc-khong-giam-sat&quot;&gt;Unsupervised learning&lt;/a&gt; - thuật toán K-means clustering (phân cụm K-means).&lt;/p&gt;

&lt;p&gt;Trong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho &lt;em&gt;dữ liệu trong cùng một cụm có tính chất giống nhau&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ví dụ:&lt;/strong&gt; Một công ty muốn tạo ra những chính sách ưu đãi cho những nhóm khách hàng khác nhau dựa trên sự tương tác giữa mỗi khách hàng với công ty đó (số năm là khách hàng; số tiền khách hàng đã chi trả cho công ty; độ tuổi; giới tính; thành phố; nghề nghiệp; …). Giả sử công ty đó có rất nhiều dữ liệu của rất nhiều khách hàng nhưng chưa có cách nào chia toàn bộ khách hàng đó thành một số nhóm/cụm khác nhau. Nếu một người biết Machine Learning được đặt câu hỏi này, phương pháp đầu tiên anh (chị) ta nghĩ đến sẽ là K-means Clustering. Vì nó là một trong những thuật toán đầu tiên mà anh ấy tìm được trong các cuốn sách, khóa học về Machine Learning. Và tôi cũng chắc rằng anh ấy đã đọc blog &lt;a href=&quot;https://tiepvupsu.github.io&quot;&gt;Machine Learning cơ bản&lt;/a&gt;. Sau khi đã phân ra được từng nhóm, nhân viên công ty đó có thể lựa chọn ra một vài khách hàng trong mỗi nhóm để quyết định xem mỗi nhóm tương ứng với nhóm khách hàng nào. Phần việc cuối cùng này cần sự can thiệp của con người, nhưng lượng công việc đã được rút gọn đi rất nhiều.&lt;/p&gt;

&lt;p&gt;Ý tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm &lt;em&gt;ở gần nhau trong một không gian nào đó&lt;/em&gt; (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ rôi sẽ viết gọn là &lt;em&gt;cluster&lt;/em&gt;).&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/figure_2.png&quot; width=&quot;500&quot; align=&quot;center&quot; /&gt;
&lt;div class=&quot;thecap&quot;&gt; Bài toán với 3 clusters. &lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Giả sử mỗi cluster có một điểm đại diện (&lt;em&gt;center&lt;/em&gt;) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: &lt;em&gt;Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hình dưới đây là một hình minh họa cho việc phân chia lãnh hải nếu có 5 đảo khác nhau được biểu diễn bằng các hình tròn màu đen:&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/figure_1.png&quot; width=&quot;500&quot; align=&quot;center&quot; /&gt;
&lt;div class=&quot;thecap&quot;&gt; Phân vùng lãnh hải của mỗi đảo. Các vùng khác nhau có màu sắc khác nhau. &lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Chúng ta thấy rằng đường phân định giữa các lãnh hải là các đường thẳng (chính xác hơn thì chúng là các đường trung trực của các cặp điểm gần nhau). Vì vậy, lãnh hải của một đảo sẽ là một hình đa giác.&lt;/p&gt;

&lt;p&gt;Cách phân chia này trong toán học được gọi là &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi Diagram&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trong không gian ba chiều, lấy ví dụ là các hành tinh, thì (tạm gọi là) &lt;em&gt;lãnh không&lt;/em&gt; của mỗi hành tinh sẽ là một đa diện. Trong không gian nhiều chiều hơn, chúng ta sẽ có những thứ (mà tôi gọi là) &lt;em&gt;siêu đa diện&lt;/em&gt; (hyperpolygon).&lt;/p&gt;

&lt;p&gt;Quay lại với bài toán phân nhóm và cụ thể là thuật toán K-means clustering, chúng ta cần một chút phân tích toán học trước khi đi tới phần &lt;a href=&quot;#tom-tat-thuat-toan&quot;&gt;tóm tắt thuật toán&lt;/a&gt; ở phần dưới. Nếu bạn không muốn đọc quá nhiều về toán, bạn có thể bỏ qua phần này. (&lt;em&gt;Tốt nhất là đừng bỏ qua, bạn sẽ tiếc đấy&lt;/em&gt;).
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;a name=&quot;-phan-tich-toan-hoc&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-phân-tích-toán-học&quot;&gt;2. Phân tích toán học&lt;/h2&gt;

&lt;p&gt;Mục đích cuối cùng của thuật toán phân nhóm này là: từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;mot-so-ky-hieu-toan-hoc&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;một-số-ký-hiệu-toán-học&quot;&gt;Một số ký hiệu toán học&lt;/h3&gt;
&lt;p&gt;Giả sử có \(N\) điểm dữ liệu là \( \mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N] \in \mathbb{R}^{d \times N}\) và \(K &amp;lt; N\) là số cluster chúng ta muốn phân chia. Chúng ta cần tìm các center \( \mathbf{m}_1, \mathbf{m}_2, \dots, \mathbf{m}_K \in \mathbb{R}^{d \times 1} \) và label của mỗi điểm dữ liệu.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lưu ý về ký hiệu toán học:&lt;/strong&gt; &lt;em&gt;trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \(x_1, N, y, k\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \(\mathbf{m}, \mathbf{x}_1 \). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \(\mathbf{X, M, Y} \). Lưu ý này đã được nêu ở bài &lt;a href=&quot;/2016/12/28/linearregression/&quot;&gt;Linear Regression&lt;/a&gt;. Tôi xin được không nhắc lại trong các bài tiếp theo.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Với mỗi điểm dữ liệu \( \mathbf{x}_i \) đặt \(\mathbf{y}_i = [y_{i1}, y_{i1}, \dots, y_{iK}]\) là label vector của nó, trong đó nếu \( \mathbf{x}_i \) được phân vào cluster \(k\) thì  \(y_{ik} = 1\) và \(y_{ij} = 0, \forall j \neq k \). Điều này có nghĩa là có đúng một phần tử của vector \(\mathbf{y}_i\) là bằng 1 (tương ứng với cluster của \(\mathbf{x}_i \)), các phần tử còn lại bằng 0. Ví dụ: nếu một điểm dữ liệu có label vector là \([1,0,0,\dots,0]\) thì nó thuộc vào cluster 1, là \([0,1,0,\dots,0]\) thì nó thuộc vào cluster 2, \(\dots\). Cách mã hóa label của dữ liệu như thế này được goi là biểu diễn &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;&lt;em&gt;one-hot&lt;/em&gt;&lt;/a&gt;. Chúng ta sẽ thấy cách biểu diễn one-hot này rất phổ biến trong Machine Learning ở các bài tiếp theo.&lt;/p&gt;

&lt;p&gt;Ràng buộc của \(\mathbf{y}_i \) có thể viết dưới dạng toán học như sau:
\[
 y_{ik} \in \{0, 1\},~~~ \sum_{k = 1}^K y_{ik} = 1 ~~~ (1)
\]
&lt;!-- \\[ \mathbf{y}_i = [y_{i}, y_2] \\] --&gt;
&lt;!-- [y_{i1}]\\) --&gt;
&lt;!-- đặt \\(\mathbf{y}_i = [y_{i1}, y_{i2}, \dots, y_{iK} \\) là vector biểu diễn label của một điểm, trong đó nếu nếu điểm dữ liệu --&gt;&lt;/p&gt;

&lt;p&gt;&lt;!-- \\(\mathbf{x}_i\\) được phân vào cluster \\(k\\) thì  \\(y_{ik} = 1\\)  --&gt;
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;a name=&quot;ham-mat-mat-va-bai-toan-toi-uu&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;hàm-mất-mát-và-bài-toán-tối-ưu&quot;&gt;Hàm mất mát và bài toán tối ưu&lt;/h3&gt;

&lt;p&gt;Nếu ta coi center \(\mathbf{m}_k \)  là center (hoặc representative) của mỗi cluster và &lt;em&gt;ước lượng&lt;/em&gt; tất cả các điểm được phân vào cluster này bởi \(\mathbf{m}_k \), thì một điểm dữ liệu \(\mathbf{x}_i \) được phân vào cluster \(k\) sẽ bị sai số là \( (\mathbf{x}_i - \mathbf{m}_k) \). Chúng ta mong muốn sai số này có trị tuyệt đối nhỏ nhất nên (&lt;a href=&quot;/2016/12/28/linearregression/#sai-so-du-doan&quot;&gt;giống như trong bài Linear Regression&lt;/a&gt;) ta sẽ tìm cách để đại lượng sau đây đạt giá trị nhỏ nhất: 
\[
\|\mathbf{x}_i - \mathbf{m}_k\|_2^2
\]&lt;/p&gt;

&lt;p&gt;Hơn nữa, vì \(\mathbf{x}_i \) được phân vào cluster \(k\) nên \(y_{ik} = 1, y_{ij} = 0, ~\forall j \neq k \). Khi đó, biểu thức bên trên sẽ được viết lại là:
\[
y_{ik}\|\mathbf{x}_i - \mathbf{m}_k\|_2^2 =  \sum_{j=1}^K y_{ij}\|\mathbf{x}_i - \mathbf{m}_j\|_2^2
\]&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;Hy vọng chỗ này không quá khó hiểu&lt;/em&gt;)&lt;/p&gt;

&lt;!-- Với mỗi điểm dữ liệu \\(\mathbf{x}\_i \\), vì chưa biết nó thuộc cluster nào, (_ở đây chúng ta tạm thời giả sử rằng ta đã biết các center_), thế thì hàm mất mát của cách phân cụm cho điểm \\(\mathbf{x}\_i \\) sẽ là: --&gt;

&lt;!-- \\[
\sum\_{k=1}^K y\_{ik} \\\|\mathbf{x}\_i - \mathbf{m}\_k \\|_2^2
\\]
 --&gt;

&lt;p&gt;Sai số cho toàn bộ dữ liệu sẽ là: 
\[
\mathcal{L}(\mathbf{Y}, \mathbf{M}) = \sum_{i=1}^N\sum_{j=1}^K y_{ij} \|\mathbf{x}_i - \mathbf{m}_j\|_2^2
\]&lt;/p&gt;

&lt;p&gt;Trong đó \( \mathbf{Y} = [\mathbf{y}_1; \mathbf{y}_2; \dots; \mathbf{y}_N], ~ \mathbf{M} = [\mathbf{m}_1, \mathbf{m}_2, \dots \mathbf{m}_N] \) lần lượt là các ma trận được tạo bởi label vector của mỗi điểm dữ liệu và center của mỗi cluster. Hàm số mất mát trong bài toán K-means clustering của chúng ta là hàm \(\mathcal{L}(\mathbf{Y}, \mathbf{M})\) với ràng buộc như được nêu trong phương trình \((1)\).&lt;/p&gt;

&lt;p&gt;Tóm lại, chúng ta cần tối ưu bài toán sau: 
\[
\mathbf{Y}, \mathbf{M} = \arg\min_{\mathbf{Y}, \mathbf{M}} \sum_{i=1}^N\sum_{j=1}^K y_{ij} \|\mathbf{x}_i - \mathbf{m}_j\|_2^2~~~~~(2)
\]&lt;/p&gt;

&lt;p&gt;\[
\text{subject to:} ~~ y_{ij} \in \{0, 1\}~~ \forall i, j;~~~ \sum_{j = 1}^K y_{ij} = 1~~\forall i
\]&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;subject to&lt;/em&gt; nghĩa là &lt;em&gt;thỏa mãn điều kiện&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nhắc lại khái niệm \(\arg\min\)&lt;/strong&gt;: Chúng ta biết ký hiệu \(\min\) là &lt;em&gt;giá trị nhỏ nhất của hàm số&lt;/em&gt;, \(\arg\min\) chính là &lt;em&gt;giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất đó&lt;/em&gt;. Nếu \(f(x) = x^2 -2x + 1 = (x-1)^2 \) thì giá trị nhỏ nhất của hàm số này bằng 0, đạt được khi \(x = 1\). Trong ví dụ này \(\min_{x} f(x) = 0\) và \(\arg\min_{x} f(x) = 1\). Thêm ví dụ khác, nếu \(x_1 = 0, x_2 = 10, x_3 = 5\) thì ta nói \(\arg\min_{i} x_i = 1\) vì \(1\) là chỉ số để \(x_i\) đạt giá trị nhỏ nhất (bằng \(0\)). Biến số viết bên dưới \(\min\) là biến số cúng ta cần tối ưu. Trong các bài toán tối ưu, ta thường quan tâm tới \(\arg\min\) hơn là \(\min\).
&lt;!-- _K-means còn là một trong những phương pháp thực hiện ước lượng vector [_vector quantization_](https://en.wikipedia.org/wiki/Vector_quantization)_ --&gt;
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;a name=&quot;thuat-toan-toi-uu-ham-mat-mat&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;thuật-toán-tối-ưu-hàm-mất-mát&quot;&gt;Thuật toán tối ưu hàm mất mát&lt;/h3&gt;
&lt;p&gt;Bài toán \((2)\) là một bài toán khó tìm &lt;em&gt;điểm tối ưu&lt;/em&gt; vì nó có thêm các điều kiện ràng buộc. &lt;em&gt;Bài toán này thuộc loại mix-integer programming (điều kiện biến là số nguyên) - là loại rất khó tìm nghiệm tối ưu toàn cục (global optimal point, tức nghiệm làm cho hàm mất mát đạt giá trị nhỏ nhất có thể).&lt;/em&gt; Tuy nhiên, trong một số trường hợp chúng ta vẫn có thể tìm được phương pháp để tìm được nghiệm gần đúng hoặc điểm cực tiểu. (&lt;em&gt;Nếu chúng ta vẫn nhớ chương trình toán ôn thi đại học thì điểm cực tiểu chưa chắc đã phải là điểm làm cho hàm số đạt giá trị nhỏ nhất&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Một cách đơn giản để giải bài toán \((2)\) là xen kẽ giải \(\mathbf{Y}\) và \( \mathbf{M}\) khi biến còn lại được cố định. Đây là một thuật toán lặp, cũng là kỹ thuật phổ biến khi giải bài toán tối ưu. Chúng ta sẽ lật lượt giải quyết hai bài toán sau đây:&lt;/p&gt;

&lt;h4 id=&quot;cố-định-mathbfm--tìm-mathbfy&quot;&gt;Cố định \(\mathbf{M} \), tìm \(\mathbf{Y}\)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Giả sử đã tìm được các centers, hãy tìm các label vector để hàm mất mát đạt giá trị nhỏ nhất.&lt;/strong&gt; Điều này tương đương với việc tìm cluster cho mỗi điểm dữ liệu.&lt;/p&gt;

&lt;p&gt;Khi các centers là cố định, bài toán tìm label vector cho toàn bộ dữ liệu có thể được chia nhỏ thành bài toán tìm label vector cho từng điểm dữ liệu \(\mathbf{x}_i\) như sau:&lt;/p&gt;

&lt;p&gt;\[
\mathbf{y}_i = \arg\min_{\mathbf{y}_i} \sum_{j=1}^K y_{ij}\|\mathbf{x}_i - \mathbf{m}_j\|_2^2 ~~~ (3)
\]
\[
\text{subject to:} ~~ y_{ij} \in \{0, 1\}~~ \forall j;~~~ \sum_{j = 1}^K y_{ij} = 1
\]&lt;/p&gt;

&lt;p&gt;Vì chỉ có một phần tử của label vector \(\mathbf{y}_i\) bằng \(1\) nên bài toán \((3)\) có thể tiếp tục được viết dưới dạng đơn giản hơn: 
\[
j = \arg\min_{j} \|\mathbf{x}_i - \mathbf{m}_j\|_2^2
\]&lt;/p&gt;

&lt;p&gt;Vì \(\|\mathbf{x}_i - \mathbf{m}_j\|_2^2\) chính là bình phương khoảng cách tính từ điểm \(\mathbf{x}_i \) tới center \(\mathbf{m}_j \), ta có thể kết luận rằng &lt;strong&gt;mỗi điểm \(\mathbf{x}_i \) thuộc vào cluster có center gần nó nhất&lt;/strong&gt;! Từ đó ta có thể dễ dàng suy ra label vector của từng điểm dữ liệu.&lt;/p&gt;

&lt;h4 id=&quot;cố-định-mathbfy--tìm-mathbfm&quot;&gt;Cố định \(\mathbf{Y} \), tìm \(\mathbf{M}\)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Giả sử đã tìm được cluster cho từng điểm, hãy tìm center mới cho mỗi cluster để hàm mất mát đạt giá trị nhỏ nhất.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một khi chúng ta đã xác định được label vector cho từng điểm dữ liệu, bài toán tìm center cho mỗi cluster được rút gọn thành:&lt;/p&gt;

&lt;p&gt;\[
\mathbf{m}_j = \arg\min_{\mathbf{m}_j} \sum_{i = 1}^{N} y_{ij}\|\mathbf{x}_i - \mathbf{m}_j \|_2^2.
\]
 Tới đây, ta có thể tìm nghiệm bằng phương pháp giải đạo hàm bằng 0, vì hàm cần tối ưu là một hàm liên tục và có đạo hàm xác định tại mọi điểm. &lt;em&gt;Và quan trọng hơn, hàm này là hàm convex (lồi) theo \(\mathbf{m}_j \) nên chúng ta sẽ tìm được giá trị nhỏ nhất và điểm tối ưu tương ứng. Sau này nếu có dịp, tôi sẽ nói thêm về tối ưu lồi (convex optimization) - một mảng cực kỳ quan trọng trong toán tối ưu&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Đặt \(l(\mathbf{m}_j)\) là hàm bên trong dấu \(\arg\min\), ta có đạo hàm:
\[
\frac{\partial l(\mathbf{m}_j)}{\partial \mathbf{m}_j} = 2\sum_{i=1}^N y_{ij}(\mathbf{m}_j - \mathbf{x}_i) 
\]&lt;/p&gt;

&lt;p&gt;Giải phương trình đạo hàm bằng 0 ta có: 
\[
\mathbf{m}_j \sum_{i=1}^N y_{ij} = \sum_{i=1}^N y_{ij} \mathbf{x}_i 
\]
\[
\Rightarrow \mathbf{m}_j = \frac{ \sum_{i=1}^N y_{ij} \mathbf{x}_i}{\sum_{i=1}^N y_{ij}}
\]&lt;/p&gt;

&lt;p&gt;Nếu để ý một chút, chúng ta sẽ thấy rằng mẫu số chính là phép đếm &lt;em&gt;số lượng các điểm dữ liệu&lt;/em&gt; trong cluster \(j\) (&lt;em&gt;Bạn có nhận ra không?&lt;/em&gt;). Còn tử số chính là &lt;em&gt;tổng các điểm dữ liệu&lt;/em&gt; trong cluster \(j\). (&lt;em&gt;Nếu bạn đọc vẫn nhớ điều kiện ràng buộc của các&lt;/em&gt; \(y_{ij} \) &lt;em&gt;thì sẽ có thể nhanh chóng nhìn ra điều này&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Hay nói một cách đơn giản hơn nhiều: \(\mathbf{m}_j\) &lt;strong&gt;là trung bình cộng của các điểm trong cluster&lt;/strong&gt; \(j\).&lt;/p&gt;

&lt;p&gt;Tên gọi &lt;em&gt;K-means clustering&lt;/em&gt; cũng xuất phát từ đây.
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;a name=&quot;tom-tat-thuat-toan&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;tóm-tắt-thuật-toán&quot;&gt;Tóm tắt thuật toán&lt;/h3&gt;
&lt;p&gt;Tới đây tôi xin được tóm tắt lại thuật toán (&lt;em&gt;đặc biệt quan trọng với các bạn bỏ qua phần toán học bên trên&lt;/em&gt;) như sau:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Đầu vào:&lt;/strong&gt; Dữ liệu \(\mathbf{X}\) và số lượng cluster cần tìm \(K\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Đầu ra:&lt;/strong&gt; Các center \(\mathbf{M}\) và label vector cho từng điểm dữ liệu \(\mathbf{Y}\).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Chọn \(K\) điểm bất kỳ làm các center ban đầu.&lt;/li&gt;
  &lt;li&gt;Phân mỗi điểm dữ liệu vào cluster có center gần nó nhất.&lt;/li&gt;
  &lt;li&gt;Nếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.&lt;/li&gt;
  &lt;li&gt;Cập nhật center cho từng cluster bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó sau bước 2.&lt;/li&gt;
  &lt;li&gt;Quay lại bước 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Chúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: &lt;em&gt;nếu một dãy số giảm và bị chặn dưới thì nó hội tụ!&lt;/em&gt; Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.&lt;/p&gt;

&lt;p&gt;Chúng ta sẽ có một vài &lt;a href=&quot;#-thao-luan&quot;&gt;thảo luận&lt;/a&gt; về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;vi-du-tren-python&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ví-dụ-trên-python&quot;&gt;Ví dụ trên Python&lt;/h2&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;gioi-thieu-bai-toan&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;giới-thiệu-bài-toán&quot;&gt;Giới thiệu bài toán&lt;/h3&gt;

&lt;p&gt;Để kiểm tra mức độ hiểu quả của một thuật toán, chúng ta sẽ làm một ví dụ đơn giản (thường được gọi là &lt;em&gt;toy example&lt;/em&gt;). Trước hết, chúng ta chọn center cho từng cluster và tạo dữ liệu cho từng cluster bằng cách lấy mẫu theo phân phối chuẩn có kỳ vọng là center của cluster đó và ma trận hiệp phương sai (covariance matrix) là ma trận đơn vị.&lt;/p&gt;

&lt;p&gt;Trước tiên, chúng ta cần khai báo các thư viện cần dùng. Chúng ta cần &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt; và &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; như trong bài &lt;a href=&quot;/2016/12/28/linearregression/&quot;&gt;Linear Regression&lt;/a&gt; cho việc tính toán ma trận và hiển thị dữ liệu. Chúng ta cũng cần thêm thư viện &lt;code class=&quot;highlighter-rouge&quot;&gt;scipy.spatial.distance&lt;/code&gt; để tính khoảng cách giữa các cặp điểm trong hai tập hợp một cách hiệu quả.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.spatial.distance&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdist&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Tiếp theo, ta tạo dữ liệu bằng cách lấy các điểm theo phân phối chuẩn có kỳ vọng tại các điểm có tọa độ &lt;code class=&quot;highlighter-rouge&quot;&gt;(2, 2), (8, 3)&lt;/code&gt; và &lt;code class=&quot;highlighter-rouge&quot;&gt;(3, 6)&lt;/code&gt;, ma trận hiệp phương sai giống nhau và là ma trận đơn vị. Mỗi cluster có 500 điểm. (&lt;em&gt;Chú ý rằng mỗi điểm dữ liệu là một hàng của ma trận dữ liệu.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;original_label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;hien-thi-du-lieu-tren-do-thi&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;hiển-thị-dữ-liệu-trên-đồ-thị&quot;&gt;Hiển thị dữ liệu trên đồ thị&lt;/h3&gt;

&lt;p&gt;Chúng ta cần một hàm &lt;code class=&quot;highlighter-rouge&quot;&gt;kmeans_display&lt;/code&gt; để hiển thị dữ liệu. Sau đó hiển thị dữ liệu theo nhãn ban đầu.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kmeans_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b^'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;markersize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'go'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;markersize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;markersize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'equal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;kmeans_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;original_label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;!-- ![png](output_5_0.png) --&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/output_5_0.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;Trong đồ thị trên, mỗi cluster tương ứng với một màu. Có thể nhận thấy rằng có một vài điểm màu đỏ bị lẫn sang phần cluster màu xanh.
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;a name=&quot;cac-ham-so-can-thiet-cho-k-means-clustering&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;các-hàm-số-cần-thiết-cho-k-means-clustering&quot;&gt;Các hàm số cần thiết cho K-means clustering&lt;/h3&gt;
&lt;p&gt;Viết các hàm:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kmeans_init_centers&lt;/code&gt; để khởi tạo các centers ban đầu.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kmeans_asign_labels&lt;/code&gt; để gán nhán mới cho các điểm khi biết các centers.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kmeans_update_centers&lt;/code&gt; để cập nhật các centers mới dữa trên dữ liệu vừa được gán nhãn.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;has_converged&lt;/code&gt; để kiểm tra điều kiện dừng của thuật toán.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kmeans_init_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# randomly pick k rows of X as initial centers&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kmeans_assign_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# calculate pairwise distances btw data and centers&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# return index of the closest center&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kmeans_update_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# collect all points assigned to the k-th cluster &lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Xk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# take average&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;has_converged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# return True if two sets of centers are the same&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; 
        &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Phần chính của K-means clustering:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kmeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmeans_init_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmeans_assign_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_centers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmeans_update_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has_converged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Áp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Centers found by our algorithm:'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kmeans_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Centers found by our algorithm:
[[ 1.97563391  2.01568065]
 [ 8.03643517  3.02468432]
 [ 2.99084705  6.04196062]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/output_11_1.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;!-- ![png](output_11_1.png) --&gt;

&lt;p&gt;Từ kết quả này chúng ta thấy rằng thuật toán K-means clustering làm việc khá thành công, các centers tìm được khá gần với kỳ vọng ban đầu. Các điểm thuộc cùng một cluster hầu như được phân vào cùng một cluster (trừ một số diểm màu đỏ ban đầu đã bị phân nhầm vào cluster màu xanh da trời, nhưng tỉ lệ là nhỏ và có thể chấp nhận được).&lt;/p&gt;

&lt;p&gt;Dưới đây là hình ảnh động minh họa thuật toán qua từng vòng lặp, chúng ta thấy rằng thuật toán trên hội tụ rất nhanh, chỉ cần 6 vòng lặp để có được kết quả cuối cùng:&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/kmeans11.gif&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Các bạn có thể xem thêm các trang web minh họa thuật toán K-means cluster tại:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.naftaliharris.com/blog/visualizing-k-means-clustering/&quot;&gt;Visualizing K-Means Clustering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html&quot;&gt;Visualizing K-Means Clustering - Standford&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;ket-qua-tim-duoc-bang-thu-vien-scikit-learn&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;kết-quả-tìm-được-bằng-thư-viện-scikit-learn&quot;&gt;Kết quả tìm được bằng thư viện scikit-learn&lt;/h3&gt;

&lt;p&gt;Để kiểm tra thêm, chúng ta hãy so sánh kết quả trên với kết quả thu được bằng cách sử dụng thư viện &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.cluster&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KMeans&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmeans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_clusters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Centers found by scikit-learn:'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmeans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_centers_&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred_label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmeans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmeans_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Centers found by scikit-learn:
[[ 8.0410628   3.02094748]
 [ 2.99357611  6.03605255]
 [ 1.97634981  2.01123694]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;!-- ![png](output_14_1.png) --&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/output_14_1.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Thật may mắn (&lt;em&gt;cho tôi&lt;/em&gt;), hai thuật toán cho cùng một đáp số! Với cách thứ nhất, tôi mong muốn các bạn hiểu rõ được thuật toán K-means clustering làm việc như thế nào. Với cách thứ hai, tôi hy vọng các bạn biết áp dụng thư viện sẵn có như thế nào.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-thao-luan&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-thảo-luận&quot;&gt;4. Thảo luận&lt;/h2&gt;
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;han-che&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;hạn-chế&quot;&gt;Hạn chế&lt;/h3&gt;
&lt;p&gt;Có một vài hạn chế của thuật toán K-means clustering:&lt;/p&gt;

&lt;h4 id=&quot;chúng-ta-cần-biết-số-lượng-cluster-cần-clustering&quot;&gt;Chúng ta cần biết số lượng cluster cần clustering&lt;/h4&gt;
&lt;p&gt;Để ý thấy rằng trong &lt;a href=&quot;#tom-tat-thuat-toan&quot;&gt;thuật toán nêu trên&lt;/a&gt;, chúng ta cần biết đại lượng \(K\) là số lượng clusters. Trong thực tế, nhiều trường hợp chúng ta không xác định được giá trị này. Có một số phương pháp giúp xác định số lượng clusters, tôi sẽ dành thời gian nói về chúng sau nếu có dịp. Bạn đọc có thể tham khảo &lt;a href=&quot;https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set&quot;&gt;Elbow method - Determining the number of clusters in a data set&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;nghiệm-cuối-cùng-phụ-thuộc-vào-các-centers-được-khởi-tạo-ban-đầu&quot;&gt;Nghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu&lt;/h4&gt;
&lt;p&gt;Tùy vào các center ban đầu mà thuật toán có thể có tốc độ hội tụ rất chậm, ví dụ:
&lt;!-- ![png](output_14_1.png) --&gt;&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/kmeans_slowconverge.gif&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;hoặc thậm chí cho chúng ta nghiệm không chính xác (chỉ là local minimum - điểm cực tiểu - mà không phải giá trị nhỏ nhất):&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/kmeans_badresult.gif&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Có một vài cách khắc phục đó là:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Chạy K-means clustering nhiều lần với các center ban đầu khác nhau rồi chọn cách có hàm mất mát cuối cùng đạt giá trị nhỏ nhất.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm&quot;&gt;K-means++ -Improve initialization algorithm - wiki&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Bạn nào muốn tìm hiểu sâu hơn có thể xem bài báo khoa học &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0167865504000996&quot;&gt;Cluster center initialization algorithm for K-means clustering&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;các-cluster-cần-có-só-lượng-điểm-gần-bằng-nhau&quot;&gt;Các cluster cần có só lượng điểm gần bằng nhau&lt;/h4&gt;

&lt;p&gt;Dưới đây là một ví dụ với 3 cluster với 20, 50, và 1000 điểm. Kết quả cuối cùng không chính xác.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/kmeans_unbalanced.gif&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;các-cluster-cần-có-dạng-hình-tròn-tuân-theo-phân-phối-chuẩn-và-ma-trận-hiệp-phương-sai-là-ma-trận-đường-chéo-có-các-điểm-trên-đường-chéo-giống-nhau&quot;&gt;Các cluster cần có dạng hình tròn (tuân theo phân phối chuẩn và ma trận hiệp phương sai là ma trận đường chéo có các điểm trên đường chéo giống nhau)&lt;/h4&gt;

&lt;p&gt;Dưới đây là 1 ví dụ khi 1 cluster có dạng hình dẹt.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/kmeans_diffcov.gif&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;khi-một-cluster-nằm-phía-trong-1-cluster-khác&quot;&gt;Khi một cluster nằm phía trong 1 cluster khác&lt;/h4&gt;
&lt;p&gt;Đây là ví dụ kinh điển về việc K-means clustering không thể phân cụm dữ liệu. Một cách tự nhiên, chúng ta sẽ phân ra thành 4 cụm: mắt trái, mắt phải, miệng, xunh quanh mặt. Nhưng vì mắt và miệng nằm trong khuôn mặt nên K-means clustering không thực hiện được:&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/kmeans/smile_face.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;
&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-tai-lieu-tham-khao&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!-- ### Khoảng cách không phải Euclid --&gt;
&lt;!-- Trong nhiều trường hợp, chúng ta có thể định --&gt;

&lt;p&gt;Mặc dù có những hạn chế, K-means clustering vẫn cực kỳ quan trọng trong Machine Learning và là nền tảng cho nhiều thuật toán phức tạp khác sau này. Chúng ta cần bắt đầu từ những thứ đơn giản. &lt;em&gt;Simple is best!&lt;/em&gt;&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-tai-lieu-tham-khao&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-tài-liệu-tham-khảo&quot;&gt;5. Tài liệu tham khảo&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/text/document_clustering.html&quot;&gt;Clustering documents using k-means&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi Diagram - Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0167865504000996&quot;&gt;Cluster center initialization algorithm for K-means clustering&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.naftaliharris.com/blog/visualizing-k-means-clustering/&quot;&gt;Visualizing K-Means Clustering&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html&quot;&gt;Visualizing K-Means Clustering - Standford&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 01 Jan 2017 10:22:00 -0500</pubDate>
        <link>http://localhost:4000/2017/01/01/kmeans/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/01/01/kmeans/</guid>
        
        
      </item>
    
      <item>
        <title>Linear Regression</title>
        <description>&lt;div class=&quot;imgcap&quot;&gt;
&lt;div&gt;
&lt;a href=&quot;/2016/12/28/linearregression/&quot;&gt;
    &lt;img src=&quot;/assets/LR/output_5_1.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;
    &lt;!-- &lt;img src=&quot;/assets/rl/mdp.png&quot; height=&quot;206&quot;&gt; --&gt;
&lt;/div&gt;
&lt;div class=&quot;thecap&quot;&gt; Linear Regression &lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Trong bài này, tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất (và đơn giản nhất) của Machine Learning. Đây là một thuật toán &lt;em&gt;Supervised learning&lt;/em&gt; có tên &lt;strong&gt;Linear Regression&lt;/strong&gt; (Hồi Quy Tuyến Tính). Bài toán này đôi khi được gọi là &lt;em&gt;Linear Fitting&lt;/em&gt; (trong thống kê) hoặc &lt;em&gt;Linear Least Square&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trong trang này:&lt;/strong&gt;
&lt;!-- MarkdownTOC --&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#-gioi-thieu&quot;&gt;1. Giới thiệu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-phan-tich-toan-hoc&quot;&gt;2. Phân tích toán học&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dang-cua-linear-regression&quot;&gt;Dạng của Linear Regression&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sai-so-du-doan&quot;&gt;Sai số dự đoán&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ham-mat-mat&quot;&gt;Hàm mất mát&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nghiem-cho-bai-toan-linear-regression&quot;&gt;Nghiệm cho bài toán Linear Regression&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-vi-du-tren-python&quot;&gt;3. Ví dụ trên Python&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bai-toan&quot;&gt;Bài toán&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hien-thi-du-lieu-tren-do-thi&quot;&gt;Hiển thị dữ liệu trên đồ thị&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nghiem-theo-cong-thuc&quot;&gt;Nghiệm theo công thức&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nghiem-theo-thu-vien-scikit-learn&quot;&gt;Nghiệm theo thư viện scikit-learn&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-thao-luan&quot;&gt;4. Thảo luận&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cac-bai-toan-co-the-giai-bang-linear-regression&quot;&gt;Các bài toán có thể giải bằng Linear Regression&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#han-che-cua-linear-regression&quot;&gt;Hạn chế của Linear Regression&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cac-phuong-phap-toi-uu&quot;&gt;Các phương pháp tối ưu&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-tai-lieu-tham-khao&quot;&gt;5. Tài liệu tham khảo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-gioi-thieu&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-giới-thiệu&quot;&gt;1. Giới thiệu&lt;/h2&gt;

&lt;p&gt;Quay lại &lt;a href=&quot;/2016/12/27/categories/#regression&quot;&gt;ví dụ đơn giản được nêu trong bài trước&lt;/a&gt;: một căn nhà rộng \(x_1 ~ \text{m}^2\), có \(x_2\) phòng ngủ và cách trung tâm thành phố \(x_3~ \text{km}\) có giá là bao nhiêu. Giả sử chúng ta đã có số liệu thống kê từ 1000 căn nhà trong thành phố đó, liệu rằng khi có một căn nhà mới với các thông số về diện tích, số phòng ngủ và khoảng cách tới trung tâm, chúng ta có thể dự đoán được giá của căn nhà đó không? Nếu có thì hàm dự đoán \(y = f(\mathbf{x}) \) sẽ có dạng như thế nào. Ở đây \(\mathbf{x} = [x_1; x_2; x_3] \) là một vector
cột chứa thông tin &lt;em&gt;input&lt;/em&gt;, \(y\) là một số vô hướng (scalar) biểu diễn &lt;em&gt;output&lt;/em&gt; (tức giá của căn nhà trong ví dụ này).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lưu ý về ký hiệu toán học:&lt;/strong&gt; &lt;em&gt;trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \(x_1, N, y, k\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \(\mathbf{y}, \mathbf{x}_1 \). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \(\mathbf{X, Y, W} \).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Một cách đơn giản nhất, chúng ta có thể thấy rằng: i) diện tích nhà càng lớn thì giá nhà càng cao; ii) số lượng phòng ngủ càng lớn thì giá nhà càng cao; iii) càng xa trung tâm thì giá nhà càng giảm. Một hàm số đơn giản nhất có thể mô tả mối quan hệ giữa giá nhà và 3 đại lượng đầu vào là:&lt;/p&gt;

&lt;!-- \\[ \text{ giá nhà } \approx w_1 (\text{diện tích}) + w_2 (\text{số phòng}) + w_3 (\text{ khoảng cách}) + w_0 \\]  --&gt;

&lt;p&gt;\[y \approx  f(\mathbf{x}) = \hat{y}\]
\[f(\mathbf{x}) =w_1 x_1 + w_2 x_2 + w_3 x_3 + w_0 ~~~~ (1)\]
trong đó, \(w_1, w_2, w_3, w_0\) là các hằng số,  \(w_0\) còn được gọi là bias. Mối quan hệ \(y \approx f(\mathbf{x})\) bên trên là một mối quan hệ tuyến tính (linear). Bài toán chúng ta đang làm là một bài toán thuộc loại regression. Bài toán đi tìm các hệ số tối ưu \( \{w_1, w_2, w_3, w_0 \}\) chính vì vậy được gọi là bài toán Linear Regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chú ý 1:&lt;/strong&gt; \(y\) là giá trị thực của &lt;em&gt;outcome&lt;/em&gt; (dựa trên số liệu thống kê chúng ta có trong tập &lt;em&gt;training data&lt;/em&gt;), trong khi \(\hat{y}\) là giá trị mà mô hình Linear Regression dự đoán được. Nhìn chung, \(y\) và \(\hat{y}\) là hai giá trị khác nhau do có sai số mô hình, tuy nhiên, chúng ta mong muốn rằng sự khác nhau này rất nhỏ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chú ý 2:&lt;/strong&gt; &lt;em&gt;Linear&lt;/em&gt; hay &lt;em&gt;tuyến tính&lt;/em&gt; hiểu một cách đơn giản là &lt;em&gt;thẳng, phẳng&lt;/em&gt;. Trong không gian hai chiều, một hàm số được gọi là &lt;em&gt;tuyến tính&lt;/em&gt; nếu đồ thị của nó có dạng một &lt;em&gt;đường thẳng&lt;/em&gt;. Trong không gian ba chiều, một hàm số được goi là &lt;em&gt;tuyến tính&lt;/em&gt; nếu đồ thị của nó có dạng một &lt;em&gt;mặt phẳng&lt;/em&gt;. Trong không gian nhiều hơn 3 chiều, khái niệm &lt;em&gt;mặt phẳng&lt;/em&gt; không còn phù hợp nữa, thay vào đó, một khái niệm khác ra đời được gọi là &lt;em&gt;siêu mặt phẳng&lt;/em&gt; (&lt;em&gt;hyperplane&lt;/em&gt;). Các hàm số tuyến tính là các hàm đơn giản nhất, vì chúng thuận tiện trong việc hình dung và tính toán. Chúng ta sẽ được thấy trong các bài viết sau, &lt;em&gt;tuyến tính&lt;/em&gt; rất quan trọng và hữu ích trong các bài toán Machine Learning. Kinh nghiệm cá nhân tôi cho thấy, trước khi hiểu được các thuật toán &lt;em&gt;phi tuyến&lt;/em&gt; (non-linear, không phẳng), chúng ta cần nắm vững các kỹ thuật cho các mô hình &lt;em&gt;tuyến tính&lt;/em&gt;.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-phan-tich-toan-hoc&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-phân-tích-toán-học&quot;&gt;2. Phân tích toán học&lt;/h2&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;dang-cua-linear-regression&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;dạng-của-linear-regression&quot;&gt;Dạng của Linear Regression&lt;/h3&gt;

&lt;p&gt;Trong phương trình \((1)\) phía trên, nếu chúng ta đặt \(\mathbf{w} = [w_1, w_2, w_3, w_0]^T = \) là vector hệ số cần phải tối ưu và \(\mathbf{\bar{x}} = [x_1, x_2, x_3, 1]^T\) (đọc là &lt;em&gt;x bar&lt;/em&gt; trong tiếng Anh) là vector dữ liệu đầu vào &lt;em&gt;mở rộng&lt;/em&gt;. Số \(1\) ở cuối được thêm vào để phép tính đơn giản hơn và thuận tiện cho việc tính toán. Khi đó, phương trình (1) có thể được viết lại dưới dạng:&lt;/p&gt;

&lt;p&gt;\[y \approx \mathbf{w}^T\mathbf{\bar{x}} = \hat{y}\]&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;sai-so-du-doan&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;sai-số-dự-đoán&quot;&gt;Sai số dự đoán&lt;/h3&gt;

&lt;p&gt;Chúng ta mong muốn rằng sự sai khác \(e\) giữa giá trị thực \(y\) và giá trị dự đoán \(\hat{y}\) (đọc là &lt;em&gt;y hat&lt;/em&gt; trong tiếng Anh) là nhỏ nhất. Nói cách khác, chúng ta muốn giá trị sau đây càng nhỏ càng tốt:&lt;/p&gt;

&lt;p&gt;\[
\frac{1}{2}e^2 = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(y - \mathbf{w}^T\mathbf{\bar{x}})^2
\]&lt;/p&gt;

&lt;p&gt;trong đó hệ số \(\frac{1}{2} \) (&lt;em&gt;lại&lt;/em&gt;) là để thuận tiện cho việc tính toán (khi tính đạo hàm thì số \(\frac{1}{2} \) sẽ bị triệt tiêu). Chúng ta cần \(e^2\) vì \(e = y - \hat{y} \) có thể là một số âm, việc nói \(e\) nhỏ nhất sẽ không đúng vì khi \(e = - \infty\) là rất nhỏ nhưng sự sai lệch là rất lớn. Bạn đọc có thể tự đặt câu hỏi: &lt;strong&gt;tại sao không dùng trị tuyệt đối \( |e| \) mà lại dùng bình phương \(e^2\) ở đây?&lt;/strong&gt; Câu trả lời sẽ có ở phần sau.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;ham-mat-mat&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;hàm-mất-mát&quot;&gt;Hàm mất mát&lt;/h3&gt;

&lt;p&gt;Điều tương tự xảy ra với tất cả các cặp &lt;em&gt;(input, outcome)&lt;/em&gt; \( (\mathbf{x}_i, y_i), i = 1, 2, \dots, N \), với \(N\) là số lượng dữ liệu quan sát được. Điều chúng ta muốn, tổng sai số là nhỏ nhất, tương đương với việc tìm \( \mathbf{w} \) để hàm số sau đạt giá trị nhỏ nhất:&lt;/p&gt;

&lt;p&gt;\[ \mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^N (y_i - \mathbf{w}^T\mathbf{\bar{x}}_i)^2 ~~~~~(2) \]&lt;/p&gt;

&lt;p&gt;Trong đó \(\mathbf{w}^T\) là chuyển vị (transpose) của vector \(\mathbf{w}\).
Hàm số \(\mathcal{L}(\mathbf{w}) \) được gọi là &lt;strong&gt;hàm mất mát&lt;/strong&gt; (loss function) của bài toán Linear Regression. Chúng ta luôn mong muốn rằng sự mất mát (sai số) là nhỏ nhất, điều đó đồng nghĩa với việc  tìm vector hệ số \( \mathbf{w} \)  sao cho 
giá trị của hàm mất mát này càng nhỏ càng tốt. Giá trị của \(\mathbf{w}\) làm cho hàm mất mát đạt giá trị nhỏ nhất được gọi là &lt;em&gt;điểm tối ưu&lt;/em&gt; (optimal point), ký hiệu:&lt;/p&gt;

&lt;p&gt;\[ \mathbf{w}^* = \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w})  \]&lt;/p&gt;

&lt;p&gt;Trước khi đi tìm lời giải, chúng ta đơn giản hóa phép toán trong phương trình hàm mất mát \((2)\). Đặt \(\mathbf{y} = [y_1, y_2, \dots, y_N]\) là một vector hàng chứa tất cả các &lt;em&gt;output&lt;/em&gt; của &lt;em&gt;training data&lt;/em&gt;; \( \mathbf{\bar{X}} = [\mathbf{\bar{x}}_1, \mathbf{\bar{x}}_2, \dots, \mathbf{\bar{x}}_N ] \) là ma trận dữ liệu đầu vào (mở rộng). Khi đó hàm số mất mát \(\mathcal{L}(\mathbf{w})\) được viết dưới dạng ma trận đơn giản hơn:&lt;/p&gt;

&lt;p&gt;\[
\mathcal{L}(\mathbf{w}) 
= \frac{1}{2}\sum_{i=1}^N (y_i - \mathbf{w}^T\mathbf{\bar{x}}_i)^2 \]
\[
= \frac{1}{2} \|\mathbf{y} - \mathbf{w}^T \mathbf{\bar{X}}\|_2^2 
= \frac{1}{2} \|\mathbf{y}^T - \mathbf{\bar{X}}^T\mathbf{w} \|_2^2
~~~(3)
\]&lt;/p&gt;

&lt;p&gt;với \( \| \mathbf{z} \|_2 \) là Euclidean norm (chuẩn Euclid, hay khoảng cách Euclid), nói cách khác \( \| \mathbf{z} \|_2^2 \) là tổng của bình phương mỗi phần tử của vector \(\mathbf{z}\). Tới đây, ta đã có một dạng đơn giản của hàm mất mát được viết như phương trình \((3)\).&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;nghiem-cho-bai-toan-linear-regression&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;nghiệm-cho-bài-toán-linear-regression&quot;&gt;Nghiệm cho bài toán Linear Regression&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Cách phổ biến nhất để tìm nghiệm cho một bài toán tối ưu (chúng ta đã biết từ khi học cấp 3) là giải phương trình đạo hàm (gradient) bằng 0!&lt;/strong&gt; Tất nhiên đó là khi việc tính đạo hàm và việc giải phương trình đạo hàm bằng 0 không quá phức tạp. Thật may mắn, với các mô hình tuyến tính, hai việc này là khả thi.&lt;/p&gt;

&lt;p&gt;Đạo hàm theo \(\mathbf{w} \) của hàm mất mát là: 
\[
\frac{\partial{\mathcal{L}(\mathbf{w})}}{\partial{\mathbf{w}}} 
= \mathbf{\bar{X}}(\mathbf{\bar{X}}^T\mathbf{w} - \mathbf{y}^T) 
\]&lt;/p&gt;

&lt;p&gt;Các bạn có thể tham khảo bảng đạo hàm theo vector hoặc ma trận của một hàm số trong &lt;a href=&quot;https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf&quot;&gt;mục D.2 của tài liệu này&lt;/a&gt;. &lt;em&gt;Đến đây tôi xin quay lại câu hỏi ở phần &lt;a href=&quot;#sai so du doan&quot;&gt;Sai số dự đoán&lt;/a&gt; phía trên về việc tại sao không dùng trị tuyệt đối mà lại dùng bình phương. Câu trả lời là hàm bình phương có đạo hàm tại mọi nơi, trong khi hàm trị tuyệt đối thì không (đạo hàm không xác định tại 0)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Phương trình đạo hàm bằng 0 tương đương với: 
\[
\mathbf{\bar{X}}\mathbf{\bar{X}}^T\mathbf{w} = \mathbf{\bar{X}}\mathbf{y}^T \triangleq \mathbf{b}
~~~ (4)
\]
(ký hiệu \(\mathbf{\bar{X}}\mathbf{y}^T \triangleq \mathbf{b} \) nghĩa là &lt;em&gt;đặt&lt;/em&gt; \(\mathbf{\bar{X}}\mathbf{y}^T\) &lt;em&gt;bằng&lt;/em&gt; \(\mathbf{b}\) ).&lt;/p&gt;

&lt;p&gt;Nếu ma trận vuông \( \mathbf{A} \triangleq \mathbf{\bar{X}}\mathbf{\bar{X}}^T\) khả nghịch (non-singular hay inversable) thì phương trình \((4)\) có nghiệm duy nhất: \( \mathbf{w} = \mathbf{A}^{-1}\mathbf{b}  \).&lt;/p&gt;

&lt;p&gt;Vậy nếu ma trận \(\mathbf{A} \) không khả nghịch (có định thức bằng 0) thì sao? Nếu các bạn vẫn nhớ các kiến thức về hệ phương trình tuyến tính, trong trường hợp này thì hoặc phương trinh \( (4) \) vô nghiệm, hoặc là 
nó
có vô số nghiệm. Khi đó, chúng ta sử dụng khái niệm &lt;a href=&quot;https://vi.wikipedia.org/wiki/Giả_nghịch_đảo_Moore–Penrose&quot;&gt;&lt;em&gt;giả nghịch đảo&lt;/em&gt;&lt;/a&gt; \( \mathbf{A}^{\dagger} \) (đọc là &lt;em&gt;A dagger&lt;/em&gt; trong tiếng Anh). (&lt;em&gt;Giả nghịch đảo (pseudo inverse) là trường hợp tổng quát của nghịch đảo khi ma trận không khả nghịch hoặc thậm chí không vuông. Trong khuôn khổ bài viết này, tôi xin phép được lược bỏ phần này, nếu các bạn thực sự quan tâm, tôi sẽ viết một bài khác chỉ nói về giả nghịch đảo. Xem thêm: &lt;a href=&quot;http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf&quot;&gt;Least Squares, Pseudo-Inverses, PCA
&amp;amp; SVD&lt;/a&gt;.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Với khái niệm giả nghịch đảo, điểm tối ưu của bài toán Linear Regression có dạng:&lt;/p&gt;

&lt;p&gt;\[
\mathbf{w} = \mathbf{A}^{\dagger}\mathbf{b} = (\mathbf{\bar{X}}\mathbf{\bar{X}}^T)^{\dagger} \mathbf{\bar{X}}\mathbf{y}^T
~~~ (5)
\]&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-vi-du-tren-python&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-ví-dụ-trên-python&quot;&gt;3. Ví dụ trên Python&lt;/h2&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;bai-toan&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;bài-toán&quot;&gt;Bài toán&lt;/h3&gt;

&lt;p&gt;Trong phần này, tôi sẽ chọn một ví dụ đơn giản về việc giải bài toán Linear Regression trong Python. Tôi cũng sẽ so sánh nghiệm của bài toán khi giải theo phương trình \((5) \) và nghiệm tìm được khi dùng thư viện &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt; của Python. (&lt;em&gt;Đây là thư viện Machine Learning được sử dụng rộng rãi trong Python&lt;/em&gt;). Trong ví dụ này, dữ liệu đầu vào chỉ có 1 giá trị (1 chiều) để thuận tiện cho việc minh hoạ trong mặt phẳng.&lt;/p&gt;

&lt;p&gt;Chúng ta có 1 bảng dữ liệu về chiều cao và cân nặng của 15 người như dưới đây:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Chiều cao (cm)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;147&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;150&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;153&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;155&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;158&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;160&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;163&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;165&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Cân nặng (kg)&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;49&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;51&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;52&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;54&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;56&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;58&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;59&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Chiều cao (cm)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;168&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;170&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;173&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;175&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;178&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;180&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;183&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Cân nặng (kg)&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;60&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;62&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;63&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;64&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;66&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;68&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Bài toán đặt ra là: liệu có thể dự đoán cân nặng của một người dựa vào chiều cao của họ không? (&lt;em&gt;Trên thực tế, tất nhiên là không, vì cân nặng còn phụ thuộc vào nhiều yếu tố khác nữa, thể tích chẳng hạn&lt;/em&gt;). Vì blog này nói về các thuật toán Machine Learning đơn giản nên tôi sẽ giả sử rằng chúng ta có thể dự đoán được.&lt;/p&gt;

&lt;p&gt;Chúng ta có thể thấy là cân nặng sẽ tỉ lệ thuận với chiều cao (càng cao càng nặng), nên có thể sử dụng Linear Regression model cho việc dự đoán này. Để kiểm tra độ chính xác của model tìm được, chúng ta sẽ giữ lại cột 155 và 160 cm để kiểm thử, các cột còn lại được sử dụng để huấn luyện (train) model.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;hien-thi-du-lieu-tren-do-thi&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;hiển-thị-dữ-liệu-trên-đồ-thị&quot;&gt;Hiển thị dữ liệu trên đồ thị&lt;/h3&gt;
&lt;p&gt;Trước tiên, chúng ta cần có hai thư viện &lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; cho đại số tuyến tính và &lt;a href=&quot;http://matplotlib.org/&quot;&gt;matplotlib&lt;/a&gt; cho việc vẽ hình.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Tiếp theo, chúng ta khai báo và biểu diễn dữ liệu trên một đồ thị.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# height (cm)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;147&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;153&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;158&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;163&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;165&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;170&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;173&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;175&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;178&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;183&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# weight (kg)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;49&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;54&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;62&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;63&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;66&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;67&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;68&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Visualize data &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;140&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;190&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Height (cm)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Weight (kg)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;!-- ![png](/assets/LR/output_3_0.png) --&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/LR/output_3_0.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Từ đồ thị này ta thấy rằng dữ liệu được sắp xếp gần như theo 1 đường thẳng, vậy mô hình Linear Regression nhiều khả năng sẽ cho kết quả tốt:&lt;/p&gt;

&lt;p&gt;(cân nặng) = w_1*(chiều cao) + w_0&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;nghiem-theo-cong-thuc&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;nghiệm-theo-công-thức&quot;&gt;Nghiệm theo công thức&lt;/h3&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ tính toán các hệ số w_1 và w_0 dựa vào công thức \((5)\). Chú ý: giả nghịch đảo của một ma trận &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; trong Python sẽ được tính bằng &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.linalg.pinv(A)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt; là từ viết tắt của &lt;em&gt;pseudo inverse&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Building Xbar &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Xbar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Calculating weights of the fitting line &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xbar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pinv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w = '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Preparing the fitting line &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;145&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;185&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_0&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Drawing the fitting line &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# data &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;               &lt;span class=&quot;c&quot;&gt;# the fitting line&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;140&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;190&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Height (cm)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Weight (kg)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;w =  [[  0.55920496]
 [-33.73541021]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;!-- ![pnet/asset/LR/output_5_1.png) --&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/LR/output_5_1.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Từ đồ thị bên trên ta thấy rằng các điểm dữ liệu màu đỏ nằm khá gần đường thẳng dự đoán màu xanh. Vậy mô hình Linear Regression hoạt động tốt với tập dữ liệu &lt;em&gt;training&lt;/em&gt;. Bây giờ, chúng ta sử dụng mô hình này để dự đoán cân nặng của hai người có chiều cao 155 và 160 cm mà chúng ta đã không dùng khi tính toán nghiệm.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;155&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;160&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Dự đoán cân nặng của người có chiều cao 155 cm: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f (kg), số liệu thật: 52 (kg)'&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Dự đoán cân nặng của người có chiều cao 160 cm: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f (kg), số liệu thật: 56 (kg)'&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dự đoán cân nặng của người có chiều cao 155 cm: 52.94 (kg), số liệu thật: 52 (kg)
Dự đoán cân nặng của người có chiều cao 160 cm: 55.74 (kg), số liệu thật: 56 (kg)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Chúng ta thấy rằng kết quả dự đoán khá gần với số liệu thực tế.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;nghiem-theo-thu-vien-scikit-learn&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;nghiệm-theo-thư-viện-scikit-learn&quot;&gt;Nghiệm theo thư viện scikit-learn&lt;/h3&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ sử dụng thư viện scikit-learn của Python để tìm nghiệm.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# fit the model by Linear Regression&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;regr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# fit_intercept = False for calculating the bias&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;regr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xbar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Compare two results&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Nghiệm tìm được bằng scikit-learn  : '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;regr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Nghiệm tìm được từ phương trình (5): '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Nghiệm tìm được bằng scikit-learn  :  [[  0.55920496 -33.73541021]]
Nghiệm tìm được từ phương trình (5):  [[  0.55920496 -33.73541021]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Chúng ta thấy rằng hai kết quả thu được như nhau! (&lt;em&gt;Nghĩa là tôi đã không mắc lỗi nào trong cách tìm nghiệm ở phần trên&lt;/em&gt;)&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-thao-luan&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-thảo-luận&quot;&gt;4. Thảo luận&lt;/h2&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;cac-bai-toan-co-the-giai-bang-linear-regression&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;các-bài-toán-có-thể-giải-bằng-linear-regression&quot;&gt;Các bài toán có thể giải bằng Linear Regression&lt;/h3&gt;
&lt;p&gt;Hàm số \(y \approx f(\mathbf{x})= \mathbf{w}^T\mathbf{x}\) là một hàm tuyến tính theo cả \( \mathbf{w}\) và \(\mathbf{x}\). Trên thực tế, Linear Regression có thể áp dụng cho các mô hình chỉ cần tuyến tính theo \(\mathbf{w}\). Ví dụ:
\[
y \approx w_1 x_1 + w_2 x_2 + w_3 x_1^2 + 
\]
\[
+w_4 \sin(x_2) + w_5 x_1x_2 + w_0
\]
là một hàm tuyến tính theo \(\mathbf{w}\) và vì vậy cũng có thể được giải bằng Linear Regression. Với mỗi dữ liệu đầu vào \(\mathbf{x}=[x_1; x_2] \), chúng ta tính toán dữ liệu mới \(\tilde{\mathbf{x}} = [x_1, x_2, x_1^2, \sin(x_2), x_1x_2]\) (đọc là &lt;em&gt;x tilde&lt;/em&gt; trong tiếng Anh) rồi áp dụng Linear Regression với dữ liệu mới này.&lt;/p&gt;

&lt;p&gt;Xem thêm ví dụ về &lt;a href=&quot;http://www.varsitytutors.com/hotmath/hotmath_help/topics/quadratic-regression&quot;&gt;Quadratic Regression&lt;/a&gt; (Hồi Quy Bậc Hai).&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;div&gt;
&lt;a href=&quot;http://www.varsitytutors.com/hotmath/hotmath_help/topics/quadratic-regression&quot;&gt;
    &lt;img src=&quot;http://www.varsitytutors.com/assets/vt-hotmath-legacy/hotmath_help/topics/quadratic-regression/f-qr-1-1.gif&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;thecap&quot;&gt; Quadratic Regression (Nguồn: &lt;a href=&quot;http://www.varsitytutors.com/hotmath/hotmath_help/topics/quadratic-regression&quot;&gt; Quadratic Regression&lt;/a&gt;) &lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;han-che-cua-linear-regression&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;hạn-chế-của-linear-regression&quot;&gt;Hạn chế của Linear Regression&lt;/h3&gt;

&lt;p&gt;Hạn chế đầu tiên của Linear Regression là nó rất &lt;strong&gt;nhạy cảm với nhiễu&lt;/strong&gt; (sensitive to noise). Trong ví dụ về mối quan hệ giữa chiều cao và cân nặng bên trên, nếu có chỉ
một cặp dữ liệu &lt;em&gt;nhiễu&lt;/em&gt; (150 cm, 90kg) thì kết quả sẽ sai khác đi rất nhiều. Xem hình dưới đây:&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
&lt;img src=&quot;/assets/LR/output_13_1.png&quot; align=&quot;center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Vì vậy, trước khi thực hiện Linear Regression, các nhiễu (&lt;em&gt;outlier&lt;/em&gt;) cần
 phải được loại bỏ. Bước này được gọi là tiền xử lý (pre-processing).&lt;/p&gt;

&lt;p&gt;Hạn chế thứ hai của Linear Regression là nó &lt;strong&gt;không biễu diễn được các mô hình phức tạp&lt;/strong&gt;. Mặc dù trong phần trên, chúng ta thấy rằng phương pháp này có thể được áp dụng nếu quan hệ giữa &lt;em&gt;outcome&lt;/em&gt; và &lt;em&gt;input&lt;/em&gt; không nhất thiết phải là tuyến tính, nhưng mối quan hệ này vẫn đơn giản nhiều so với các mô hình thực tế. Hơn nữa, chúng ta sẽ tự hỏi: làm thế nào để xác định được các hàm \(x_1^2, \sin(x_2), x_1x_2\) như ở trên?!&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;cac-phuong-phap-toi-uu&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;các-phương-pháp-tối-ưu&quot;&gt;Các phương pháp tối ưu&lt;/h3&gt;
&lt;p&gt;Linear Regression là một mô hình đơn giản, lời giải cho phương trình đạo hàm bằng 0 cũng khá đơn giản. &lt;em&gt;Trong hầu hết các trường hợp, chúng ta không thể giải được phương trình đạo hàm bằng 0.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nhưng có một điều chúng ta nên nhớ, &lt;strong&gt;còn tính được đạo hàm là còn có cơ hội&lt;/strong&gt;.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-tai-lieu-tham-khao&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-tài-liệu-tham-khảo&quot;&gt;5. Tài liệu tham khảo&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;&gt;Linear Regression - Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/&quot;&gt;Simple Linear Regression Tutorial for Machine Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf&quot;&gt;Least Squares, Pseudo-Inverses, PCA &amp;amp; SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 28 Dec 2016 10:22:00 -0500</pubDate>
        <link>http://localhost:4000/2016/12/28/linearregression/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/12/28/linearregression/</guid>
        
        
      </item>
    
      <item>
        <title>Phân nhóm các thuật toán Machine Learning</title>
        <description>&lt;div class=&quot;imgcap&quot;&gt;
&lt;div&gt;
&lt;a href=&quot;/2016/12/27/categories/&quot;&gt;
    &lt;img src=&quot;/assets/categories/alphago.jpeg&quot; width=&quot;800&quot; /&gt;&lt;/a&gt;
    &lt;!-- &lt;img src=&quot;/assets/rl/mdp.png&quot; height=&quot;206&quot;&gt; --&gt;
&lt;/div&gt;
&lt;div class=&quot;thecap&quot;&gt;AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của Reinforcement learning. &lt;br /&gt; (Nguồn: &lt;a href=&quot;http://www.tomshardware.com/news/alphago-defeats-sedol-second-time,31377.html&quot;&gt;AlphaGo AI Defeats Sedol Again, With 'Near Perfect Game')&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Có hai cách phổ biến phân nhóm các thuật toán Machine learning. Một là dựa trên phương thức học (learning style), hai là dựa trên chức năng (function) (của mỗi thuật toán).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trong trang này:&lt;/strong&gt;
&lt;!-- MarkdownTOC --&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#-phan-nhom-dua-tren-phuong-thuc-hoc&quot;&gt;1. Phân nhóm dựa trên phương thức học&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#supervised-learning-hoc-co-giam-sat&quot;&gt;Supervised Learning (Học có giám sát)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#classification-phan-loai&quot;&gt;&lt;strong&gt;Classification&lt;/strong&gt; (Phân loại)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#regression-hoi-quy&quot;&gt;&lt;strong&gt;Regression&lt;/strong&gt; (Hồi quy)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#unsupervised-learning-hoc-khong-giam-sat&quot;&gt;Unsupervised Learning (Học không giám sát)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#clustering-phan-nhom&quot;&gt;&lt;strong&gt;Clustering&lt;/strong&gt; (phân nhóm)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#association&quot;&gt;&lt;strong&gt;Association&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#semi-supervised-learning-hoc-ban-giam-sat&quot;&gt;Semi-Supervised Learning (Học bán giám sát)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforcement-learning-hoc-cung-co&quot;&gt;Reinforcement Learning (Học Củng Cố)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-phan-nhom-dua-tren-chuc-nang&quot;&gt;2. Phân nhóm dựa trên chức năng&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#regression-algorithms&quot;&gt;Regression Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#classification-algorithms&quot;&gt;Classification Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#instance-based-algorithms&quot;&gt;Instance-based Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regularization-algorithms&quot;&gt;Regularization Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#baysian-algorithms&quot;&gt;Baysian Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#clustering-algorithms&quot;&gt;Clustering Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#artificial-neural-network-algorithms&quot;&gt;Artificial Neural Network Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dimensionality-reduction-algorithms&quot;&gt;Dimensionality Reduction Algorithms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ensemble-algorithms&quot;&gt;Ensemble Algorithms&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-tai-lieu-tham-khao&quot;&gt;3. Tài liệu tham khảo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-phan-nhom-dua-tren-phuong-thuc-hoc&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-phân-nhóm-dựa-trên-phương-thức-học&quot;&gt;1. Phân nhóm dựa trên phương thức học&lt;/h2&gt;

&lt;p&gt;Theo phương thức học, các thuật toán Machine Learning thường được chia làm 4 nhóm: Supervise learning, Unsupervised learning, Semi-supervised lerning và Reinforcement learning. &lt;em&gt;Có một số cách phân nhóm không có Semi-supervised learning hoặc Reinforcement learning.&lt;/em&gt;&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;supervised-learning-hoc-co-giam-sat&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;supervised-learning-học-có-giám-sát&quot;&gt;Supervised Learning (Học có giám sát)&lt;/h3&gt;
&lt;p&gt;Supervised learning là thuật toán dự đoán đầu ra (outcome) của một dữ liệu mới (new input) dựa trên các cặp (&lt;em&gt;input, outcome&lt;/em&gt;) đã biết từ trước. Cặp dữ liệu này còn được gọi là (&lt;em&gt;data. label&lt;/em&gt;), tức (&lt;em&gt;dữ liệu, nhãn&lt;/em&gt;). Supervised learning là nhóm phổ biến nhất trong các thuật toán Machine Learning.&lt;/p&gt;

&lt;p&gt;Một cách toán học, Supervised learning là khi chúng ra có một tập hợp biến đầu vào \( \mathcal{X} = \{\mathbf{x}&lt;em&gt;1, \mathbf{x}_2, \dots, \mathbf{x}_N\} \) và một tập hợp nhãn tương ứng \( \mathcal{Y} = \{\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_N\} \), trong đó \( \mathbf{x}_i, \mathbf{y}_i \) là các vector. 
Các cặp dữ liệu biết trước \( (\mathbf{x}_i, \mathbf{y}_i) \in \mathcal{X} \times \mathcal{Y} \) 
được gọi là tập _training data&lt;/em&gt; (dữ liệu huấn luyện). Từ tập traing data này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập \(\mathcal{X}\) sang một phần tử (xấp xỉ) tương ứng của tập \(\mathcal{Y}\):&lt;/p&gt;

&lt;p&gt;\[ \mathbf{y}_i \approx f(\mathbf{x}_i), ~~ \forall i = 1, 2, \dots, N\] 
Mục đích là xấp xỉ hàm số \(f\) thật tốt để khi có một dữ liệu \(\mathbf{x}\) mới, chúng ta có thể tính được nhãn tương ứng của nó \( \mathbf{y} = f(\mathbf{x}) \).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ví dụ 1:&lt;/strong&gt; trong nhận dạng chữ viết tay, ta có ảnh của hàng nghìn ví dụ của mỗi chữ số được viết bởi nhiều người khác nhau. Chúng ta đưa các bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh tương ứng với chữ số nào. Sau khi thuật toán tạo ra (sau khi &lt;em&gt;học&lt;/em&gt;) một mô hình, tức một hàm số mà đầu vào là một bức ảnh và đầu ra là một chữ số, khi nhận được một bức ảnh mới mà mô hình &lt;strong&gt;chưa nhìn thấy bao giờ&lt;/strong&gt;, nó sẽ dự đoán bức ảnh đó chứa chữ số nào.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;!-- &lt;a href = &quot;http://www.rubylab.io/img/mnist.png&quot;&gt; --&gt;
    &lt;img src=&quot;http://www.rubylab.io/img/mnist.png&quot; width=&quot;600&quot; /&gt;
    &lt;!-- &lt;img src=&quot;/assets/rl/mdp.png&quot; height=&quot;206&quot;&gt; --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;div class=&quot;thecap&quot;&gt;&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt;: bộ cơ sở dữ liệu của chữ số viết tay. &lt;br /&gt; (Nguồn: &lt;a href=&quot;http://www.rubylab.io/2015/03/18/simple-neural-network-implenentation-in-ruby/&quot;&gt;Simple Neural Network implementation in Ruby)&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Ví dụ này khá giống với cách học của con người khi còn nhỏ. Ta đưa bảng chữ cái cho một đứa trẻ và chỉ cho chúng đây là chữ A, đây là chữ B. Sau một vài lần được dạy thì trẻ có thể nhận biết được đâu là chữ A, đâu là chữ B trong một cuốn sách mà chúng chưa nhìn thấy bao giờ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ví dụ 2:&lt;/strong&gt; Thuật toán dò các khuôn mặt trong một bức ảnh đã được phát triển từ rất lâu. Thời gian đầu, facebook sử dụng thuật toán này để chỉ ra các khuôn mặt trong một bức ảnh và yêu cầu người dùng &lt;em&gt;tag friends&lt;/em&gt; - tức gán nhãn cho mỗi khuôn mặt. Số lượng cặp dữ liệu (&lt;em&gt;khuôn mặt, tên người&lt;/em&gt;) càng lớn, độ chính xác ở những lần tự động &lt;em&gt;tag&lt;/em&gt; tiếp theo sẽ càng lớn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ví dụ 3:&lt;/strong&gt; Bản thân thuật toán dò tìm các khuôn mặt trong 1 bức ảnh cũng là một thuật toán Supervised learning với training data (dữ liệu học) là hàng ngàn cặp (&lt;em&gt;ảnh, mặt người&lt;/em&gt;) và (&lt;em&gt;ảnh, không phải mặt người&lt;/em&gt;) được đưa vào. Chú ý là dữ liệu này chỉ phân biệt &lt;em&gt;mặt người&lt;/em&gt; và &lt;em&gt;không phải mặt ngưòi&lt;/em&gt; mà không phân biệt khuôn mặt của những người khác nhau.&lt;/p&gt;

&lt;p&gt;Thuật toán supervised learning còn được tiếp tục chia nhỏ ra thành hai loại chính:&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;classification-phan-loai&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;classification-phân-loại&quot;&gt;&lt;strong&gt;Classification&lt;/strong&gt; (Phân loại)&lt;/h4&gt;
&lt;p&gt;Một bài toán được gọi là &lt;em&gt;classification&lt;/em&gt; nếu các &lt;em&gt;label&lt;/em&gt; của &lt;em&gt;input data&lt;/em&gt; được chia thành một số hữu hạn nhóm. Ví dụ: Gmail xác định xem một email có phải là spam hay không; các hãng tín dụng xác định xem một khách hàng có khả năng thanh toán nợ hay không. Ba ví dụ phía trên được chia vào loại này.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;regression&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;regression-hoi-quy&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;regression-hồi-quy&quot;&gt;&lt;strong&gt;Regression&lt;/strong&gt; (Hồi quy)&lt;/h4&gt;
&lt;p&gt;(tiếng Việt dịch là &lt;em&gt;Hồi quy&lt;/em&gt;, tôi không thích cách dịch này vì bản thân không hiểu nó nghĩa là gì)&lt;/p&gt;

&lt;p&gt;Nếu &lt;em&gt;label&lt;/em&gt; không được chia thành các nhóm mà là một giá trị thực cụ thể. Ví dụ: một căn nhà rộng \(x ~ \text{m}^2\), có \(y\) phòng ngủ và cách trung tâm thành phố \(z~ \text{km}\) sẽ có giá là bao nhiêu?&lt;/p&gt;

&lt;p&gt;Gần đây &lt;a href=&quot;http://how-old.net/&quot;&gt;Microsoft có một ứng dụng dự đoán giới tính và tuổi dựa trên khuôn mặt&lt;/a&gt;. Phần dự đoán giới tính có thể coi là thuật toán &lt;strong&gt;Classification&lt;/strong&gt;, phần dự đoán tuổi có thể coi là thuật toán &lt;strong&gt;Regression&lt;/strong&gt;. &lt;em&gt;Chú ý rằng phần dự đoán tuổi cũng có thể coi là &lt;strong&gt;Classification&lt;/strong&gt; nếu ta coi tuổi là một số nguyên dương không lớn hơn 150, chúng ta sẽ có 150 class (lớp) khác nhau.&lt;/em&gt;&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;unsupervised-learning-hoc-khong-giam-sat&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;unsupervised-learning-học-không-giám-sát&quot;&gt;Unsupervised Learning (Học không giám sát)&lt;/h3&gt;
&lt;p&gt;Trong thuật toán này, chúng ta không biết được &lt;em&gt;outcome&lt;/em&gt; hay &lt;em&gt;nhãn&lt;/em&gt; mà chỉ có dữ liệu đầu vào. Thuật toán unsupervised learning sẽ dựa vào cấu trúc của dữ liệu để thực hiện một công việc nào đó, ví dụ như phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimention reduction) để thuận tiện trong việc lưu trữ và tính toán.&lt;/p&gt;

&lt;p&gt;Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu vào \(\mathcal{X} \) mà không biết &lt;em&gt;nhãn&lt;/em&gt; \(\mathcal{Y}\) tương ứng.&lt;/p&gt;

&lt;p&gt;Những thuật toán loại này được gọi là Unsupervised learning vì không giống như Supervised learning, chúng ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào. Giống như khi ta học, không có thầy cô giáo nào chỉ cho ta biết đó là chữ A hay chữ B. Cụm &lt;em&gt;không giám sát&lt;/em&gt; được đặt tên theo nghĩa này.&lt;/p&gt;

&lt;p&gt;Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai loại:&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;clustering-phan-nhom&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;clustering-phân-nhóm&quot;&gt;&lt;strong&gt;Clustering&lt;/strong&gt; (phân nhóm)&lt;/h4&gt;
&lt;p&gt;Một bài toán phân nhóm toàn bộ dữ liệu \(\mathcal{X}\) thành các nhóm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ: phân nhóm khách hàng dựa trên hành vi mua hàng. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình thù và màu sắc khác nhau, ví dụ tam giác, vuông, tròn với màu xanh và đỏ, sau đó yêu cẩu trẻ phân chúng thành từng nhóm. Mặc dù không cho trẻ biết mảnh nào tương ứng với hình nào hoặc màu nào, nhiều khả năng chúng vẫn có thể phân loại các mảnh ghép theo màu hoặc hình dạng.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;association&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;association&quot;&gt;&lt;strong&gt;Association&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví dụ: những khách hàng nam mua quần áo thường có xu hướng mua thêm đồng hồ hoặc thắt lưng; những khán giả xem phim Spider Man thường có xu hướng xem thêm phim Bat Man, dựa vào đó tạo ra một hệ thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua sắm.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;semi-supervised-learning-hoc-ban-giam-sat&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;semi-supervised-learning-học-bán-giám-sát&quot;&gt;Semi-Supervised Learning (Học bán giám sát)&lt;/h3&gt;
&lt;p&gt;Các bài toán khi chúng ta có một lượng lớn dữ liệu \(\mathcal{X}\) nhưng chỉ một phần trong chúng được gán nhãn được gọi là Semi-Supervised Learning. Những bài toán thuộc nhóm này nằm giữa hai nhóm được nêu bên trên.&lt;/p&gt;

&lt;p&gt;Một ví dụ điển hình của nhóm này là chỉ có một phần ảnh hoặc văn bản được gán nhãn (ví dụ bức ảnh về người, động vật hoặc các văn bản khoa học, chính trị) và phần lớn các bức ảnh/văn bản khác chưa được gán nhãn được thu thập từ internet. Thực tế cho thấy rất nhiều các bài toàn Machine Learning thuộc vào nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được (ảnh y học chẳng hạn). Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;reinforcement-learning-hoc-cung-co&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning-học-củng-cố&quot;&gt;Reinforcement Learning (Học Củng Cố)&lt;/h3&gt;
&lt;p&gt;Reinforcement learning là các bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximizing the performance). Hiện tại, Reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nưóc đi tiếp theo để đạt được điểm số cao nhất.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ví dụ 1:&lt;/strong&gt; &lt;a href=&quot;https://gogameguru.com/tag/deepmind-alphago-lee-sedol/&quot;&gt;AlphaGo gần đây nổi tiếng với việc chơi cờ vây thắng cả con người&lt;/a&gt;. &lt;a href=&quot;https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/&quot;&gt;Cờ vây được xem là có độ phức tạp cực kỳ cao&lt;/a&gt; với tổng số nước đi là xấp xỉ \(10^{761} \), so với cờ vua là \(10^{120} \) và tổng số nguyên tử trong toàn vũ trụ là khoảng \(10^{80}\)!! Vì vậy, thuật toán phải chọn ra 1 nước đi tối ưu trong số hàng nhiều tỉ tỉ lựa chọn, và tất nhiên, không thể áp dụng thuật toán tương tự như &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)&quot;&gt;IBM Deep Blue&lt;/a&gt; (IBM Deep Blue đã thắng con người trong môn cờ vua 20 năm trước). Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục đích cuối cùng của AlphaGo không phải là chơi như con người mà phải thậm chí thắng cả con người. Vì vậy, sau khi &lt;em&gt;học&lt;/em&gt; xong các ván cờ của con người, AlphaGo tự chơi với chính nó với hàng triệu ván chơi để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning. (Xem thêm tại &lt;a href=&quot;https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/&quot;&gt;Google DeepMind’s AlphaGo: How it works&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ví dụ 2:&lt;/strong&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=qv6UVOQ0F44&quot;&gt;Huấn luyện cho máy tính chơi game Mario&lt;/a&gt;. Đây là một chương trình thú vị dạy máy tính chơi game Mario. Game này đơn giản hơn cờ vây vì tại một thời điểm, người chơi chỉ phải bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào. Đồng thời, phản ứng của máy cũng đơn giản hơn và lặp lại ở mỗi lần chơi (tại thời điểm cụ thể sẽ xuất hiện một chướng ngại vật cố định ở một vị trí cố định). Đầu vào của thuật toán là sơ đồ của màn hình tại thời điểm hiện tại, nhiệm vụ của thuật toán là với đầu vào đó, tổ hợp phím nào nên được bấm. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa trong thời gian bao lâu trong game, càng xa và càng nhanh thì được điểm thưởng càng cao (điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra). Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa. Click vào hình dưới để xem video.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
&lt;div&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=qv6UVOQ0F44&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/qv6UVOQ0F44/0.jpg&quot; width=&quot;800&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;thecap&quot;&gt;Lập trình cho máy tính chơi game Mario&lt;/div&gt;
&lt;/div&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-phan-nhom-dua-tren-chuc-nang&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-phân-nhóm-dựa-trên-chức-năng&quot;&gt;2. Phân nhóm dựa trên chức năng&lt;/h2&gt;

&lt;p&gt;Có một cách phân nhóm thứ hai dựa trên chức năng của các thuật toán. Trong phần này, tôi xin chỉ liệt kê các thuật toán. Thông tin cụ thể sẽ được trình bày trong các bài viết khác tại blog này. Trong quá trình viết, tôi có thể sẽ thêm bớt một số thuật toán.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;regression-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;regression-algorithms&quot;&gt;Regression Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/2016/12/28/linearregression/&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Logistic Regression&lt;/li&gt;
  &lt;li&gt;Stepwise Regression&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;classification-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;classification-algorithms&quot;&gt;Classification Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Linear Classifier&lt;/li&gt;
  &lt;li&gt;Support Vector Machine (SMV)&lt;/li&gt;
  &lt;li&gt;Kernel SVM&lt;/li&gt;
  &lt;li&gt;Sparse Represntation-based classification (SRC)&lt;/li&gt;
  &lt;li&gt;Neural Networks&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;instance-based-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;instance-based-algorithms&quot;&gt;Instance-based Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;k-Nearest Neighbor (kNN)&lt;/li&gt;
  &lt;li&gt;Learnin Vector Quantization (LVQ)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;regularization-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;regularization-algorithms&quot;&gt;Regularization Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Ridge Regression&lt;/li&gt;
  &lt;li&gt;Least Absolute Shringkage and Selection Operator (LASSO)&lt;/li&gt;
  &lt;li&gt;Least-Angle Regression (LARS)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;baysian-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;baysian-algorithms&quot;&gt;Baysian Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Naive Bayes&lt;/li&gt;
  &lt;li&gt;Gausian Naive Bayes&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;clustering-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;clustering-algorithms&quot;&gt;Clustering Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/2017/01/01/kmeans/&quot;&gt;k-Means clustering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;k-Medians&lt;/li&gt;
  &lt;li&gt;Expectation Maximisation (EM)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;artificial-neural-network-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;artificial-neural-network-algorithms&quot;&gt;Artificial Neural Network Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Perceptron&lt;/li&gt;
  &lt;li&gt;Back-Propagation&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;dimensionality-reduction-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;dimensionality-reduction-algorithms&quot;&gt;Dimensionality Reduction Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Principal Component Analysis (PCA)&lt;/li&gt;
  &lt;li&gt;Linear Discriminant Analysis (LDA)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;ensemble-algorithms&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;ensemble-algorithms&quot;&gt;Ensemble Algorithms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Boosting&lt;/li&gt;
  &lt;li&gt;AdaBoost&lt;/li&gt;
  &lt;li&gt;Random Forest&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Và còn rất nhiều các thuật toán khác.&lt;/p&gt;

&lt;!-- ========================== New Heading ==================== --&gt;
&lt;p&gt;&lt;a name=&quot;-tai-lieu-tham-khao&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-tài-liệu-tham-khảo&quot;&gt;3. Tài liệu tham khảo&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/&quot;&gt;A Tour of Machine Learning Algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ongxuanhong.wordpress.com/2015/10/22/diem-qua-cac-thuat-toan-machine-learning-hien-dai/&quot;&gt;Điểm qua các thuật toán Machine Learning hiện đại&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 27 Dec 2016 10:22:00 -0500</pubDate>
        <link>http://localhost:4000/2016/12/27/categories/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/12/27/categories/</guid>
        
        
      </item>
    
      <item>
        <title>Giới thiệu về Machine Learning</title>
        <description>&lt;div class=&quot;imgcap&quot;&gt;
&lt;div&gt;
    &lt;a href=&quot;/2016/12/26/introduce/&quot;&gt;
    &lt;img src=&quot;/assets/introduce/aimldl.png&quot; width=&quot;800&quot; /&gt;&lt;/a&gt;
    &lt;!-- &lt;img src=&quot;/assets/rl/mdp.png&quot; height=&quot;206&quot;&gt; --&gt;
&lt;/div&gt;
&lt;div class=&quot;thecap&quot;&gt;Mối quan hệ giữa AI, Machine Learning và Deep Learning. &lt;br /&gt; (Nguồn: &lt;a href=&quot;https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/&quot;&gt;What’s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?&lt;/a&gt;)&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Những năm gần đây, AI - Artificial Intelligence (Trí Tuệ Nhân Tạo), và cụ thể hơn là Machine Learning (Học Máy hoặc Máy Học - &lt;em&gt;Với những từ chuyên ngành, tôi sẽ dùng song song cả tiếng Anh và tiếng Việt, tuy nhiên sẽ ưu tiên tiếng Anh vì thuận tiện hơn trong việc tra cứu&lt;/em&gt;) nổi lên như một bằng chứng của cuộc cách mạng công nghiệp lần thứ tư (1 - động cơ hơi nước, 2 - năng lượng điện, 3 - công nghệ thông tin). Trí Tuệ Nhân Tạo đang len lỏi vào mọi lĩnh vực trong đời sống mà có thể chúng ta không nhận ra. Xe tự hành của Google và Tesla, hệ thống tự tag khuôn mặt trong ảnh của Facebook, trợ lý ảo Siri của Apple, hệ thống gợi ý sản phẩm của Amazon, hệ thống gợi ý phim của Netflix, máy chơi cờ vây AlphaGo của Google DeepMind, …, chỉ là một vài trong vô vàn những ứng dụng của AI/Machine Learning. (Xem thêm &lt;a href=&quot;https://www.facebook.com/zuck/posts/10103351073024591&quot;&gt;Jarvis - trợ lý thông minh cho căn nhà của Mark Zuckerberg&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Machine Learning là một tập con của AI. Theo định nghĩa của Wikipedia, &lt;em&gt;Machine learning is the subfield of computer science that “gives computers the ability to learn without being explicitly programmed”&lt;/em&gt;. Nói đơn giản, Machine Learning là một lĩnh vực nhỏ của Khoa Học Máy Tính, nó có khả năng tự học hỏi dựa trên dữ liệu đưa vào mà không cần phải được lập trình cụ thể. Bạn Nguyễn Xuân Khánh tại đại học Maryland đang viết một cuốn sách về Machine Learning bằng tiếng Việt khá thú vị, các bạn có thể tham khảo bài &lt;a href=&quot;https://ml-book-vn.khanhxnguyen.com/intro.html&quot;&gt;Machine Learning là gì?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Những năm gần đây, khi mà khả năng tính toán của các máy tính được nâng lên một tầm cao mới và lượng dữ liệu khổng lồ được thu thập bởi các hãng công nghệ lớn, Machine Learning đã tiến thêm một bước tiến dài và một lĩnh vực mới được ra đời gọi là Deep Learning (Học Sâu - &lt;em&gt;thực sự tôi không muốn dịch từ này ra tiếng Việt&lt;/em&gt;). Deep Learning đã giúp máy tính thực thi những việc tưởng chừng như không thể vào 10 năm trước: phân loại cả ngàn vật thể khác nhau trong các bức ảnh, tự tạo chú thích cho ảnh, bắt chước giọng nói và chữ viết của con người, giao tiếp với con người, hay thậm chí cả sáng tác văn hay âm nhạc (Xem thêm &lt;a href=&quot;http://machinelearningmastery.com/inspirational-applications-deep-learning/&quot;&gt;8 Inspirational Applications of Deep Learning&lt;/a&gt;)&lt;/p&gt;

&lt;!-- &lt;div style=&quot;display:inline-block&quot;&gt;
    &lt;img src=&quot;/assets/introduce/aimldl.png&quot;&gt;
&lt;/div&gt;
 --&gt;

&lt;h2 id=&quot;mục-đích-viết-blog&quot;&gt;Mục đích viết Blog&lt;/h2&gt;
&lt;p&gt;Nhu cầu về nhân lực ngành Machine Learning (Deep Learning) đang ngày một cao, kéo theo đó nhu cầu học Machine Learning trên thế giới và ở Việt Nam ngày một lớn. Cá nhân tôi cũng muốn hệ thống lại kiến thức của mình về lĩnh vực này để chuẩn bị cho tương lai (đây là một trong những mục tiêu của tôi trong năm 2017). Tôi sẽ cố gắng đi từ những thuật toán cơ bản nhất của Machine Learning kèm theo các ví dụ và mã nguồn trong mỗi bài viết. Tôi sẽ viết 1-2 tuần 1 bài (việc viết các công thức toán và code trên blog thực sự tốn nhiều thời gian hơn tôi từng nghĩ). Đồng thơi, tôi cũng mong muốn nhận được phản hồi của bạn đọc để qua những thảo luận, tôi và các bạn có thể nắm bắt được các thuật toán này.&lt;/p&gt;

&lt;p&gt;Khi chuẩn bị các bài viết, tôi sẽ giả định rằng bạn đọc có một chút kiến thức về Đại Số Tuyến Tính (Linear Algebra), Xác Suât Thống Kê (Probability and Statistics) và có kinh nghiệm về lập trình Python. Nếu bạn chưa có nhiều kinh nghiệm về các lĩnh vực này, đừng quá lo lắng vì mỗi bài sẽ chỉ sử dụng một vài kỹ thuật cơ bản. Hãy để lại câu hỏi của bạn ở phần Comment bên dưới mỗi bài, tôi sẽ thảo luận thêm với các bạn.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/2016/12/27/categories/&quot;&gt;Trong bài tiếp theo của blog này&lt;/a&gt;, tôi sẽ giới thiệu về các nhóm thuật toán Machine learning cơ bản. Mời các bạn theo dõi.&lt;/p&gt;

&lt;h2 id=&quot;tham-khảo-thêm&quot;&gt;Tham khảo thêm&lt;/h2&gt;

&lt;h3 id=&quot;các-khóa-học&quot;&gt;Các khóa học&lt;/h3&gt;

&lt;h4 id=&quot;tiếng-anh&quot;&gt;Tiếng Anh&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Machine Learning với thầy Andrew Ng trên Coursera&lt;/a&gt; (&lt;em&gt;Khóa học nổi tiếng nhất về Machine Learning&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.udacity.com/course/deep-learning--ud730&quot;&gt;Deep Learning by Google trên Udacity&lt;/a&gt; (&lt;em&gt;Khóa học nâng cao hơn về Deep Learning với Tensorflow&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/&quot;&gt;Machine Learning mastery&lt;/a&gt; (&lt;em&gt;Các thuật toán Machine Learning cơ bản&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tiếng-việt&quot;&gt;Tiếng Việt&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lưu ý&lt;/strong&gt;: &lt;em&gt;Các khóa học này tôi chưa từng tham gia, chỉ đưa ra để các bạn tham khảo.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://tuanvannguyen.blogspot.com/2016/12/cap-nhat-khoa-hoc-ve-machine-learning.html&quot;&gt;Machine Learning 1/2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://techmaster.vn/khoa-hoc/25511/machine-learning-co-ban&quot;&gt;Nhập môn Machine Learning - Cao Thanh Hà &lt;em&gt;POSTECH&lt;/em&gt;&lt;/a&gt; (&lt;em&gt;Tech Master&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;các-trang-machine-learning-tiếng-việt-khác&quot;&gt;Các trang Machine Learning tiếng Việt khác&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp&quot;&gt;Machine Learning trong Xử Lý Ngôn Ngữ Tự Nhiên - Nhóm Đông Du &lt;em&gt;Nhật Bản&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ongxuanhong.wordpress.com/&quot;&gt;Machine Learning cho người mới bắt đầu - Ông Xuân Hồng &lt;em&gt;JAIST&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml-book-vn.khanhxnguyen.com/&quot;&gt;Machine Learning book for Vietnamese - Nguyễn Xuân Khánh &lt;em&gt;University of Maryland&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 26 Dec 2016 10:22:00 -0500</pubDate>
        <link>http://localhost:4000/2016/12/26/introduce/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/12/26/introduce/</guid>
        
        
      </item>
    
  </channel>
</rss>
