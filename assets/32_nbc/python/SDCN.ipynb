{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## Load data \n",
    "from __future__ import division, print_function, unicode_literals\n",
    "from scipy.io import loadmat \n",
    "import numpy as np\n",
    "from utils import *\n",
    "nb_classes = 2 # target vs confuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_scale = 1\n",
    "\n",
    "## load train \n",
    "(target_data_train, confuser_data_train, ground_train) = load_data_arl_1d(isTrain = 1)\n",
    "\n",
    "target_mix, target_clean = mix_data_ground(target_data_train, ground_train, 1000, c = ground_scale)\n",
    "confuser_mix, confuser_clean = mix_data_ground(confuser_data_train, ground_train, 1000, c = ground_scale)\n",
    "\n",
    "train_data = np.concatenate((target_mix, confuser_mix), axis = 0)\n",
    "train_data_clean = np.concatenate((target_clean, confuser_clean), axis = 0)\n",
    "train_label = np.concatenate((np.zeros((1, target_mix.shape[0]), dtype = int), \n",
    "                            np.ones((1, confuser_mix.shape[0]), dtype = int)), axis = 1).reshape(-1)\n",
    "one_hot_train = label2onehot(train_label, nb_classes)\n",
    "# mixdata\n",
    "\n",
    "## load test \n",
    "(target_data_test, confuser_data_test, ground_test) = load_data_arl_1d(isTrain = 0)\n",
    "\n",
    "# mixdata\n",
    "target_mix = mix_data_ground(target_data_test, ground_test, 200, c = ground_scale)[0]\n",
    "confuser_mix = mix_data_ground(confuser_data_test, ground_test, 200, c = ground_scale)[0]\n",
    "\n",
    "test_data = np.concatenate((target_mix, confuser_mix), axis = 0)\n",
    "test_label = np.concatenate((np.zeros((1, target_mix.shape[0]), dtype = int), \n",
    "                            np.ones((1, confuser_mix.shape[0]), dtype = int)), axis = 1).reshape(-1)\n",
    "one_hot_test = label2onehot(test_label, nb_classes)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost= 0.496683627 Accuracy: 0.6325\n",
      "Epoch: 0200 cost= 0.420668125 Accuracy: 0.645\n",
      "Epoch: 0300 cost= 0.373256117 Accuracy: 0.6525\n",
      "Epoch: 0400 cost= 0.337801903 Accuracy: 0.69\n",
      "Epoch: 0500 cost= 0.308853507 Accuracy: 0.6925\n",
      "Epoch: 0600 cost= 0.284238845 Accuracy: 0.7125\n",
      "Epoch: 0700 cost= 0.262864143 Accuracy: 0.71\n",
      "Epoch: 0800 cost= 0.244015992 Accuracy: 0.705\n",
      "Epoch: 0900 cost= 0.227177784 Accuracy: 0.7125\n",
      "Epoch: 1000 cost= 0.211978063 Accuracy: 0.7275\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = .1\n",
    "training_epochs = 1000\n",
    "batch_size = 10\n",
    "display_step = training_epochs //10\n",
    "\n",
    "data_dim = train_data.shape[1]\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, data_dim]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 2]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([data_dim, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "#         total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: train_data, y: one_hot_train})\n",
    "            # Compute average loss\n",
    "        avg_cost += c / 1\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                  \"Accuracy:\", accuracy.eval({x: test_data, y: one_hot_test}))\n",
    "            \n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add one more hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost= 0.681796670 Accuracy: 0.55\n",
      "Epoch: 0200 cost= 0.620635271 Accuracy: 0.61\n",
      "Epoch: 0300 cost= 0.488341689 Accuracy: 0.65\n",
      "Epoch: 0400 cost= 0.357147902 Accuracy: 0.635\n",
      "Epoch: 0500 cost= 0.253218949 Accuracy: 0.6325\n",
      "Epoch: 0600 cost= 0.174043760 Accuracy: 0.6225\n",
      "Epoch: 0700 cost= 0.116324462 Accuracy: 0.6275\n",
      "Epoch: 0800 cost= 0.078279160 Accuracy: 0.6475\n",
      "Epoch: 0900 cost= 0.054330945 Accuracy: 0.675\n",
      "Epoch: 1000 cost= 0.039182510 Accuracy: 0.6725\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = .1\n",
    "training_epochs = 1000\n",
    "batch_size = 10\n",
    "display_step = training_epochs //10\n",
    "\n",
    "data_dim = train_data.shape[1]\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, data_dim]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 2]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "hid_size = 500\n",
    "W1 = tf.Variable(tf.zeros([data_dim, hid_size]))\n",
    "b1 = tf.Variable(tf.zeros([hid_size]))\n",
    "\n",
    "W2 = tf.Variable(tf.zeros([hid_size, 2]))\n",
    "b2 = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "hid_layer = tf.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "pred = tf.nn.softmax(tf.matmul(hid_layer, W2) + b2) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "#         total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: train_data, y: one_hot_train})\n",
    "            # Compute average loss\n",
    "        avg_cost += c / 1\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                  \"Accuracy:\", accuracy.eval({x: test_data, y: one_hot_test}))\n",
    "            \n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic with scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "lr = LogisticRegression(penalty='l1', C = 1e10)\n",
    "lr.fit(train_data, train_label)\n",
    "pred = lr.predict(test_data)\n",
    "acc = accuracy_score(test_label, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Noisy input, clean hidden, class at output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data = train_data; \n",
    "\n",
    "# training: (train_data, train_data_clean, one_hot_train)\n",
    "# Parameters\n",
    "learning_rate = .1\n",
    "training_epochs = 1000\n",
    "batch_size = 10\n",
    "display_step = training_epochs //10\n",
    "\n",
    "data_dim = train_data.shape[1]\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, data_dim]) # mnist data image of shape 28*28=784\n",
    "x_clean = tf.placeholder(tf.float32, [None, data_dim])\n",
    "y = tf.placeholder(tf.float32, [None, 2]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "#\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([data_dim, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "#         total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: train_data, y: one_hot_train})\n",
    "            # Compute average loss\n",
    "        avg_cost += c / 1\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                  \"Accuracy:\", accuracy.eval({x: test_data, y: one_hot_test}))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
