%!TEX root = book.tex
\chapter{Linear Regression}
\label{cha:linearregression}

\section{1. Giới thiệu}\label{giux1edbi-thiux1ec7u}

Quay lại \href{/2016/12/27/categories/\#regression}{ví dụ đơn giản được
nêu trong bài trước}: một căn rộng $x_1 \text{m}^2$, có
$x_2$ phòng ngủ và cách trung tâm $ x_3 \text{km}$ có
giá là bao nhiêu. Giả sử chúng ta đã có số liệu thống kê từ 1000 căn nhà
trong thành phố đó, liệu rằng khi có một căn nhà mới với các thông số về
diện tích, số phòng ngủ và khoảng cách tới trung tâm, chúng ta có thể dự
đoán được giá của căn nhà đó không? Nếu có thì hàm dự đoán
$y = f(\mathbf{x}) $ sẽ có dạng như thế
nào. Ở đây $\mathbf{x} = {[}x_1, x_2, x_3{]}
$ là một vector hàng chứa thông tin \emph{input},
$y$ là một số vô hướng (scalar) biểu
diễn \emph{output} (tức giá của căn nhà trong ví dụ này).

\textbf{Lưu ý về ký hiệu toán học:} trong các bài viết của tôi, các số
vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể
viết hoa, ví dụ $x_1, N, y, k$. Các
vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ
$\mathbf{y}, \mathbf{x}_1 $. Các ma
trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ
$\mathbf{X, Y, W} $.

Một cách đơn giản nhất, chúng ta có thể thấy rằng: i) diện tích nhà càng
lớn thì giá nhà càng cao; ii) số lượng phòng ngủ càng lớn thì giá nhà
càng cao; iii) càng xa trung tâm thì giá nhà càng giảm. Một hàm số đơn
giản nhất có thể mô tả mối quan hệ giữa giá nhà và 3 đại lượng đầu vào
là:

\begin{equation}y \approx  f(\mathbf{x}) = \hat{y}\end{equation}
\begin{equation}f(\mathbf{x}) =w_1 x_1 + w_2 x_2 + w_0~~~~
(1)\end{equation} trong đó, $w_1, w_2, w_3,
w_0$ là các hằng số,
$w_0$ còn được gọi là bias. Mối quan hệ
$y \approx f(\mathbf{x})$ bên trên là
một mối quan hệ tuyến tính (linear). Bài toán chúng ta đang làm là một
bài toán thuộc loại regression. Bài toán đi tìm các hệ số tối ưu
$ \textbackslash\{w_1, w_2, w_3, w_0
\textbackslash\}$ chính vì vậy được gọi là bài toán
Linear Regression.

\textbf{Chú ý 1:} $y$ là giá trị thực
của \emph{outcome} (dựa trên số liệu thống kê chúng ta có trong tập
\emph{training data}), trong khi
$\hat{y}$ là giá trị mà mô hình Linear
Regression dự đoán được. Nhìn chung, $y$
và $\hat{y}$ là hai giá trị khác nhau do
có sai số mô hình, tuy nhiên, chúng ta mong muốn rằng sự khác nhau này
rất nhỏ.

\textbf{Chú ý 2:} \emph{Linear} hay \emph{tuyến tính} hiểu một cách đơn
giản là \emph{thẳng, phẳng}. Trong không gian hai chiều, một hàm số được
gọi là \emph{tuyến tính} nếu đồ thị của nó có dạng một \emph{đường
thẳng}. Trong không gian ba chiều, một hàm số được goi là \emph{tuyến
tính} nếu đồ thị của nó có dạng một \emph{mặt phẳng}. Trong không gian
nhiều hơn 3 chiều, khái niệm \emph{mặt phẳng} không còn phù hợp nữa,
thay vào đó, một khái niệm khác ra đời được gọi là \emph{siêu mặt phẳng}
(\emph{hyperplane}). Các hàm số tuyến tính là các hàm đơn giản nhất, vì
chúng thuận tiện trong việc hình dung và tính toán. Chúng ta sẽ được
thấy trong các bài viết sau, \emph{tuyến tính} rất quan trọng và hữu ích
trong các bài toán Machine Learning. Kinh nghiệm cá nhân tôi cho thấy,
trước khi hiểu được các thuật toán \emph{phi tuyến} (non-linear, không
phẳng), chúng ta cần nắm vững các kỹ thuật cho các mô hình \emph{tuyến
tính}.

\section{2. Phân tích toán
học}\label{phuxe2n-tuxedch-touxe1n-hux1ecdc}

\subsection{Dạng của Linear
Regression}\label{dux1ea1ng-cux1ee7a-linear-regression}

Trong phương trình $(1)$ phía trên, nếu
chúng ta đặt $\mathbf{w} = {[}w_0, w_1, w_2,
w_3{]}^T = $ là vector (cột) hệ số cần phải tối ưu
và $\mathbf{\bar{x}} = {[}1, x_1, x_2,
x_3{]}$ (đọc là \emph{x bar} trong tiếng Anh) là vector
(hàng) dữ liệu đầu vào \emph{mở rộng}. Số
$1$ ở đầu được thêm vào để phép tính đơn
giản hơn và thuận tiện cho việc tính toán. Khi đó, phương trình (1) có
thể được viết lại dưới dạng:

\begin{equation}y \approx \mathbf{\bar{x}}\mathbf{w} =
\hat{y}\end{equation}

Chú ý rằng $\mathbf{\bar{x}}$ là một
vector hàng. (\href{/math/\#luu-y-ve-ky-hieu}{Xem thêm về ký hiệu vector
hàng và cột tại đây})

\subsection{Sai số dự đoán}\label{sai-sux1ed1-dux1ef1-ux111ouxe1n}

Chúng ta mong muốn rằng sự sai khác $e$
giữa giá trị thực $y$ và giá trị dự đoán
$\hat{y}$ (đọc là \emph{y hat} trong
tiếng Anh) là nhỏ nhất. Nói cách khác, chúng ta muốn giá trị sau đây
càng nhỏ càng tốt:

\begin{equation} \frac{1}{2}e^2 = \frac{1}{2}(y - \hat{y})^2 =
\frac{1}{2}(y - \mathbf{\bar{x}}\mathbf{w})^2 \end{equation}

trong đó hệ số $\frac{1}{2} $
(\emph{lại}) là để thuận tiện cho việc tính toán (khi tính đạo hàm thì
số $\frac{1}{2} $ sẽ bị triệt tiêu).
Chúng ta cần $e^2$ vì
$e = y - \hat{y} $ có thể là một số âm,
việc nói $e$ nhỏ nhất sẽ không đúng vì
khi $e = - \infty$ là rất nhỏ nhưng sự
sai lệch là rất lớn. 
%Bạn đọc có thể tự đặt câu hỏi: \textbf{tại sao không dùng trị tuyệt đối} $ |e|$ \textbf{mà lại dùng bình phương} $e^2$ ở đây? Câu trả lời sẽ có ở
phần sau.

\subsection{Hàm mất mát}\label{huxe0m-mux1ea5t-muxe1t}

Điều tương tự xảy ra với tất cả các cặp \emph{(input, outcome)}
$ (\mathbf{x}_i, y_i), i = 1, 2, \dots, N
$, với $N$ là số lượng
dữ liệu quan sát được. Điều chúng ta muốn, tổng sai số là nhỏ nhất,
tương đương với việc tìm $ \mathbf{w} $
để hàm số sau đạt giá trị nhỏ nhất:

\begin{equation} \mathcal{L}(\mathbf{w}) =
\frac{1}{2}\sum_\{i=1\}^N (y_i -
\mathbf{\bar{x}_i}\mathbf{w})^2
~~~~2)
\end{equation}

Hàm số $\mathcal{L}(\mathbf{w}) $ được
gọi là \textbf{hàm mất mát} (loss function) của bài toán Linear
Regression. Chúng ta luôn mong muốn rằng sự mất mát (sai số) là nhỏ
nhất, điều đó đồng nghĩa với việc tìm vector hệ số $
\mathbf{w} $ sao cho giá trị của hàm mất mát này càng
nhỏ càng tốt. Giá trị của $\mathbf{w}$
làm cho hàm mất mát đạt giá trị nhỏ nhất được gọi là \emph{điểm tối ưu}
(optimal point), ký hiệu:

\begin{equation} \mathbf{w}^* = \arg\min_\{\mathbf{w}\}
\mathcal{L}(\mathbf{w}) \end{equation}

Trước khi đi tìm lời giải, chúng ta đơn giản hóa phép toán trong phương
trình hàm mất mát $(2)$. Đặt
$\mathbf{y} = {[}y_1; y_2; \dots;
y_N{]}$ là một vector cột chứa tất cả các \emph{output}
của \emph{training data}; $ \mathbf{\bar{X}} =
{[}\mathbf{\bar{x}}_1; \mathbf{\bar{x}}_2; \dots; \mathbf{\bar{x}}_N
{]} $ là ma trận dữ liệu đầu vào (mở rộng) mà mỗi hàng
của nó là một điểm dữ liệu. Khi đó hàm số mất mát
$\mathcal{L}(\mathbf{w})$ được viết dưới
dạng ma trận đơn giản hơn:

\begin{equation} \mathcal{L}(\mathbf{w}) =
\frac{1}{2}\sum_\{i=1\}^N (y_i -
\mathbf{\bar{x}}_i\mathbf{w})^2 \end{equation}
\begin{equation} = \frac{1}{2} \|\mathbf{y} -
\mathbf{\bar{X}}\mathbf{w} \|_2^2
~~~~(3) \end{equation}

với $ \| \mathbf{z}\|_2 $ là Euclidean norm (chuẩn
Euclid, hay khoảng cách Euclid), nói cách khác $
\| \mathbf{z} \|_2^2
$ là tổng của bình phương mỗi phần tử của vector
$\mathbf{z}$. Tới đây, ta đã có một dạng
đơn giản của hàm mất mát được viết như phương trình
$(3)$.

\subsection{Nghiệm cho bài toán Linear
Regression}\label{nghiux1ec7m-cho-buxe0i-touxe1n-linear-regression}

\textbf{Cách phổ biến nhất để tìm nghiệm cho một bài toán tối ưu (chúng
ta đã biết từ khi học cấp 3) là giải phương trình đạo hàm (gradient)
bằng 0!} Tất nhiên đó là khi việc tính đạo hàm và việc giải phương trình
đạo hàm bằng 0 không quá phức tạp. Thật may mắn, với các mô hình tuyến
tính, hai việc này là khả thi.

Đạo hàm theo $\mathbf{w} $ của hàm mất
mát là: \begin{equation}
\frac{\partial{\mathcal{L}(\mathbf{w})}}{\partial{\mathbf{w}}} =
\mathbf{\bar{X}}^T(\mathbf{\bar{X}}\mathbf{w} - \mathbf{y})
\end{equation}

Các bạn có thể tham khảo bảng đạo hàm theo vector hoặc ma trận của một
hàm số trong
\href{https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf}{mục D.2 của
tài liệu này}. \emph{Đến đây tôi xin quay lại câu hỏi ở phần
\protect\hyperlink{saiux5cux2520soux5cux2520duux5cux2520doan}{Sai số dự
đoán} phía trên về việc tại sao không dùng trị tuyệt đối mà lại dùng
bình phương. Câu trả lời là hàm bình phương có đạo hàm tại mọi nơi,
trong khi hàm trị tuyệt đối thì không (đạo hàm không xác định tại 0)}.

Phương trình đạo hàm bằng 0 tương đương với: \begin{equation}
\mathbf{\bar{X}}^T\mathbf{\bar{X}}\mathbf{w} =
\mathbf{\bar{X}}^T\mathbf{y} \triangleq \mathbf{b}
~~~~ (4) \end{equation} (ký hiệu
$\mathbf{\bar{X}}^T\mathbf{y} \triangleq \mathbf{b}
$ nghĩa là \emph{đặt}
$\mathbf{\bar{X}}^T\mathbf{y}$
\emph{bằng} $\mathbf{b}$ ).

Nếu ma trận vuông $ \mathbf{A}
\triangleq \mathbf{\bar{X}}^T\mathbf{\bar{X}}$ khả
nghịch (non-singular hay inversable) thì phương trình
$(4)$ có nghiệm duy nhất:
$ \mathbf{w} = \mathbf{A}^\{-1\}\mathbf{b}
$.

Vậy nếu ma trận $\mathbf{A} $ không khả
nghịch (có định thức bằng 0) thì sao? Nếu các bạn vẫn nhớ các kiến thức
về hệ phương trình tuyến tính, trong trường hợp này thì hoặc phương
trinh $ (4) $ vô nghiệm, hoặc làp nó có
vô số nghiệm. Khi đó, chúng ta sử dụng khái niệm
\href{https://vi.wikipedia.org/wiki/Giả_nghịch_đảo_Moore–Penrose}{\emph{giả
nghịch đảo}} $ \mathbf{A}^\{\dagger\}
$ (đọc là \emph{A dagger} trong tiếng Anh). (\emph{Giả
nghịch đảo (pseudo inverse) là trường hợp tổng quát của nghịch đảo khi
ma trận không khả nghịch hoặc thậm chí không vuông. Trong khuôn khổ bài
viết này, tôi xin phép được lược bỏ phần này, nếu các bạn thực sự quan
tâm, tôi sẽ viết một bài khác chỉ nói về giả nghịch đảo. Xem thêm:
\href{http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf}{Least
Squares, Pseudo-Inverses, PCA \& SVD}.})

Với khái niệm giả nghịch đảo, điểm tối ưu của bài toán Linear Regression
có dạng:

\begin{equation} \mathbf{w} = \mathbf{A}^{\dagger}\mathbf{b} =
(\mathbf{\bar{X}}^T\mathbf{\bar{X}})^{\dagger}
\mathbf{\bar{X}}^T\mathbf{y} ~~~~ (5)
\end{equation}

\section{3. Ví dụ trên Python}\label{vuxed-dux1ee5-truxean-python}

\subsection{Bài toán}\label{buxe0i-touxe1n}

Trong phần này, tôi sẽ chọn một ví dụ đơn giản về việc giải bài toán
Linear Regression trong Python. Tôi cũng sẽ so sánh nghiệm của bài toán
khi giải theo phương trình $(5) $ và
nghiệm tìm được khi dùng thư viện
\href{http://scikit-learn.org/stable/}{scikit-learn} của Python.
(\emph{Đây là thư viện Machine Learning được sử dụng rộng rãi trong
Python}). Trong ví dụ này, dữ liệu đầu vào chỉ có 1 giá trị (1 chiều) để
thuận tiện cho việc minh hoạ trong mặt phẳng.

Chúng ta có 1 bảng dữ liệu về chiều cao và cân nặng của 15 người như
dưới đây:

% \begin{longtable}[]{@cccc@}
% \toprule
% Chiều cao (cm) & Cân nặng (km) & Chiều cao (cm) & Cân nặng
% (kg)\tabularnewline
% \midrule
% \endhead
% 147 & 49 & 168 & 60\tabularnewline
% 150 & 50 & 170 & 72\tabularnewline
% 153 & 51 & 173 & 63\tabularnewline
% 155 & 52 & 175 & 64\tabularnewline
% 158 & 54 & 178 & 66\tabularnewline
% 160 & 56 & 180 & 67\tabularnewline
% 163 & 58 & 183 & 68\tabularnewline
% 165 & 59 & &\tabularnewline
% \bottomrule
% \end{longtable}

Bài toán đặt ra là: liệu có thể dự đoán cân nặng của một người dựa vào
chiều cao của họ không? (\emph{Trên thực tế, tất nhiên là không, vì cân
nặng còn phụ thuộc vào nhiều yếu tố khác nữa, thể tích chẳng hạn}). Vì
blog này nói về các thuật toán Machine Learning đơn giản nên tôi sẽ giả
sử rằng chúng ta có thể dự đoán được.

Chúng ta có thể thấy là cân nặng sẽ tỉ lệ thuận với chiều cao (càng cao
càng nặng), nên có thể sử dụng Linear Regression model cho việc dự đoán
này. Để kiểm tra độ chính xác của model tìm được, chúng ta sẽ giữ lại
cột 155 và 160 cm để kiểm thử, các cột còn lại được sử dụng để huấn
luyện (train) model.

\subsection{Hiển thị dữ liệu trên đồ
thị}\label{hiux1ec3n-thux1ecb-dux1eef-liux1ec7u-truxean-ux111ux1ed3-thux1ecb}

Trước tiên, chúng ta cần có hai thư viện
\href{http://www.numpy.org/}{numpy} cho đại số tuyến tính và
\href{http://matplotlib.org/}{matplotlib} cho việc vẽ hình.

% \begin{Shaded}
% \begin{Highlighting}[]
% \CommentTok{# To support both python 2 and python 3}
% \ImportTok{from}\NormalTok{ __future__ }\ImportTok{import}\NormalTok{ division, print_function, unicode_literals}
% \ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np }
% \ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
% \end{Highlighting}
% \end{Shaded}

% Tiếp theo, chúng ta khai báo và biểu diễn dữ liệu trên một đồ thị.

% \begin{Shaded}
% \begin{Highlighting}[]
% \CommentTok{# height (cm)}
% \NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{147}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{153}\NormalTok{, }\DecValTok{158}\NormalTok{, }\DecValTok{163}\NormalTok{, }\DecValTok{165}\NormalTok{, }\DecValTok{168}\NormalTok{, }\DecValTok{170}\NormalTok{, }\DecValTok{173}\NormalTok{, }\DecValTok{175}\NormalTok{, }\DecValTok{178}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{183}\NormalTok{]]).T}
% \CommentTok{# weight (kg)}
% \NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([[ }\DecValTok{49}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{51}\NormalTok{,  }\DecValTok{54}\NormalTok{, }\DecValTok{58}\NormalTok{, }\DecValTok{59}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{62}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{67}\NormalTok{, }\DecValTok{68}\NormalTok{]]).T}
% \CommentTok{# Visualize data }
% \NormalTok{plt.plot(X, y, }\StringTok{'ro'}\NormalTok{)}
% \NormalTok{plt.axis([}\DecValTok{140}\NormalTok{, }\DecValTok{190}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{75}\NormalTok{])}
% \NormalTok{plt.xlabel(}\StringTok{'Height (cm)'}\NormalTok{)}
% \NormalTok{plt.ylabel(}\StringTok{'Weight (kg)'}\NormalTok{)}
% \NormalTok{plt.show()}
% \end{Highlighting}
% \end{Shaded}

% Từ đồ thị này ta thấy rằng dữ liệu được sắp xếp gần như theo 1 đường
% thẳng, vậy mô hình Linear Regression nhiều khả năng sẽ cho kết quả tốt:

% (cân nặng) = w_1*(chiều cao) + w_0

% \subsection{Nghiệm theo công
% thức}\label{nghiux1ec7m-theo-cuxf4ng-thux1ee9c}

% Tiếp theo, chúng ta sẽ tính toán các hệ số w_1 và w_0 dựa vào công
% thức $(5)$. Chú ý: giả nghịch đảo của
% một ma trận \texttt{A} trong Python sẽ được tính bằng
% \texttt{numpy.linalg.pinv(A)}, \texttt{pinv} là từ viết tắt của
% \emph{pseudo inverse}.

% \begin{Shaded}
% \begin{Highlighting}[]
% \CommentTok{# Building Xbar }
% \NormalTok{one }\OperatorTok{=}\NormalTok{ np.ones((X.shape[}\DecValTok{0}\NormalTok{], }\DecValTok{1}\NormalTok{))}
% \NormalTok{Xbar }\OperatorTok{=}\NormalTok{ np.concatenate((one, X), axis }\OperatorTok{=} \DecValTok{1}\NormalTok{)}

% \CommentTok{# Calculating weights of the fitting line }
% \NormalTok{A }\OperatorTok{=}\NormalTok{ np.dot(Xbar.T, Xbar)}
% \NormalTok{b }\OperatorTok{=}\NormalTok{ np.dot(Xbar.T, y)}
% \NormalTok{w }\OperatorTok{=}\NormalTok{ np.dot(np.linalg.pinv(A), b)}
% \BuiltInTok{print}\NormalTok{(}\StringTok{'w = '}\NormalTok{, w)}
% \CommentTok{# Preparing the fitting line }
% \NormalTok{w_0 }\OperatorTok{=}\NormalTok{ w[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}
% \NormalTok{w_1 }\OperatorTok{=}\NormalTok{ w[}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{]}
% \NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{145}\NormalTok{, }\DecValTok{185}\NormalTok{, }\DecValTok{2}\NormalTok{)}
% \NormalTok{y0 }\OperatorTok{=}\NormalTok{ w_0 }\OperatorTok{+}\NormalTok{ w_1}\OperatorTok{*}\NormalTok{x0}

% \CommentTok{# Drawing the fitting line }
% \NormalTok{plt.plot(X.T, y.T, }\StringTok{'ro'}\NormalTok{)     }\CommentTok{# data }
% \NormalTok{plt.plot(x0, y0)               }\CommentTok{# the fitting line}
% \NormalTok{plt.axis([}\DecValTok{140}\NormalTok{, }\DecValTok{190}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{75}\NormalTok{])}
% \NormalTok{plt.xlabel(}\StringTok{'Height (cm)'}\NormalTok{)}
% \NormalTok{plt.ylabel(}\StringTok{'Weight (kg)'}\NormalTok{)}
% \NormalTok{plt.show()}
% \end{Highlighting}
% \end{Shaded}

% w = {[}{[}-33.73541021{]}{[} 0.55920496{]}{]}

% Từ đồ thị bên trên ta thấy rằng các điểm dữ liệu màu đỏ nằm khá gần
% đường thẳng dự đoán màu xanh. Vậy mô hình Linear Regression hoạt động
% tốt với tập dữ liệu \emph{training}. Bây giờ, chúng ta sử dụng mô hình
% này để dự đoán cân nặng của hai người có chiều cao 155 và 160 cm mà
% chúng ta đã không dùng khi tính toán nghiệm.

% \begin{Shaded}
% \begin{Highlighting}[]
% \NormalTok{y1 }\OperatorTok{=}\NormalTok{ w_1}\OperatorTok{*}\DecValTok{155} \OperatorTok{+}\NormalTok{ w_0}
% \NormalTok{y2 }\OperatorTok{=}\NormalTok{ w_1}\OperatorTok{*}\DecValTok{160} \OperatorTok{+}\NormalTok{ w_0}

% \BuiltInTok{print}\NormalTok{( }\StringTok{u'Dự đoán cân nặng của người có chiều cao 155 cm: }\SpecialCharTok{%.2f}\StringTok{ (kg), số liệu thật: 52 (kg)'}  \OperatorTok{%}\NormalTok{(y1) )}
% \BuiltInTok{print}\NormalTok{( }\StringTok{u'Dự đoán cân nặng của người có chiều cao 160 cm: }\SpecialCharTok{%.2f}\StringTok{ (kg), số liệu thật: 56 (kg)'}  \OperatorTok{%}\NormalTok{(y2) )}
% \end{Highlighting}
% \end{Shaded}

% \begin{verbatim}
% Dự đoán cân nặng của người có chiều cao 155 cm: 52.94 (kg), số liệu thật: 52 (kg)
% Dự đoán cân nặng của người có chiều cao 160 cm: 55.74 (kg), số liệu thật: 56 (kg)
% \end{verbatim}

% Chúng ta thấy rằng kết quả dự đoán khá gần với số liệu thực tế.

% \subsection{Nghiệm theo thư viện
% scikit-learn}\label{nghiux1ec7m-theo-thux1b0-viux1ec7n-scikit-learn}

% Tiếp theo, chúng ta sẽ sử dụng thư viện scikit-learn của Python để tìm
% nghiệm.

% \begin{Shaded}
% \begin{Highlighting}[]
% \ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ datasets, linear_model}

% \CommentTok{# fit the model by Linear Regression}
% \NormalTok{regr }\OperatorTok{=}\NormalTok{ linear_model.LinearRegression(fit_intercept}\OperatorTok{=}\VariableTok{False}\NormalTok{) }\CommentTok{# fit_intercept = False for calculating the bias}
% \NormalTok{regr.fit(Xbar, y)}

% \CommentTok{# Compare two results}
% \BuiltInTok{print}\NormalTok{( }\StringTok{u'Nghiệm tìm được bằng scikit-learn  : '}\NormalTok{, regr.coef_ )}
% \BuiltInTok{print}\NormalTok{( }\StringTok{u'Nghiệm tìm được từ phương trình (5): '}\NormalTok{, w.T)}
% \end{Highlighting}
% \end{Shaded}

% \begin{verbatim}
% Nghiệm tìm được bằng scikit-learn  :  [[  -33.73541021 0.55920496]]
% Nghiệm tìm được từ phương trình (5):  [[  -33.73541021 0.55920496 ]]
% \end{verbatim}

% Chúng ta thấy rằng hai kết quả thu được như nhau! (\emph{Nghĩa là tôi đã
% không mắc lỗi nào trong cách tìm nghiệm ở phần trên})

% \href{https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/LR/LR.ipynb}{Source
% code Jupyter Notebook cho bài này.}

\section{4. Thảo luận}\label{thux1ea3o-luux1eadn}

\subsection{Các bài toán có thể giải bằng Linear
Regression}\label{cuxe1c-buxe0i-touxe1n-cuxf3-thux1ec3-giux1ea3i-bux1eb1ng-linear-regression}

Hàm số $y \approx f(\mathbf{x})=
\mathbf{w}^T\mathbf{x}$ là một hàm tuyến tính theo cả
$ \mathbf{w}$ và
$\mathbf{x}$. Trên thực tế, Linear
Regression có thể áp dụng cho các mô hình chỉ cần tuyến tính theo
$\mathbf{w}$. Ví dụ: \begin{equation}
y \approx w_1 x_1 + w_2 x_2 + w_3 x_1^2 + \end{equation}
\begin{equation} +w_4 \sin(x_2) + w_5 x_1x_2 + w_0
\end{equation} là một hàm tuyến tính theo
$\mathbf{w}$ và vì vậy cũng có thể được
giải bằng Linear Regression. Với mỗi dữ liệu đầu vào
$\mathbf{x}={[}x_1; x_2{]} $, chúng ta
tính toán dữ liệu mới $\tilde{\mathbf{x}} = {[}x_1,
x_2, x_1^2, \sin(x_2), x_1x_2{]}$ (đọc là
\emph{x tilde} trong tiếng Anh) rồi áp dụng Linear Regression với dữ
liệu mới này.

Xem thêm ví dụ về
\href{http://www.varsitytutors.com/hotmath/hotmath_help/topics/quadratic-regression}{Quadratic
Regression} (Hồi Quy Bậc Hai).

Quadratic Regression (Nguồn: Quadratic Regression)

\subsection{Hạn chế của Linear
Regression}\label{hux1ea1n-chux1ebf-cux1ee7a-linear-regression}

Hạn chế đầu tiên của Linear Regression là nó rất \textbf{nhạy cảm với
nhiễu} (sensitive to noise). Trong ví dụ về mối quan hệ giữa chiều cao
và cân nặng bên trên, nếu có chỉ một cặp dữ liệu \emph{nhiễu} (150 cm,
90kg) thì kết quả sẽ sai khác đi rất nhiều. Xem hình dưới đây:

Vì vậy, trước khi thực hiện Linear Regression, các nhiễu
(\emph{outlier}) cần phải được loại bỏ. Bước này được gọi là tiền xử lý
(pre-processing).

Hạn chế thứ hai của Linear Regression là nó \textbf{không biễu diễn được
các mô hình phức tạp}. Mặc dù trong phần trên, chúng ta thấy rằng phương
pháp này có thể được áp dụng nếu quan hệ giữa \emph{outcome} và
\emph{input} không nhất thiết phải là tuyến tính, nhưng mối quan hệ này
vẫn đơn giản nhiều so với các mô hình thực tế. Hơn nữa, chúng ta sẽ tự
hỏi: làm thế nào để xác định được các hàm $x_1^2,
\sin(x_2), x_1x_2$ như ở trên?!

\subsection{Các phương pháp tối
ưu}\label{cuxe1c-phux1b0ux1a1ng-phuxe1p-tux1ed1i-ux1b0u}

Linear Regression là một mô hình đơn giản, lời giải cho phương trình đạo
hàm bằng 0 cũng khá đơn giản. \emph{Trong hầu hết các trường hợp, chúng
ta không thể giải được phương trình đạo hàm bằng 0.}

Nhưng có một điều chúng ta nên nhớ, \textbf{còn tính được đạo hàm là còn
có cơ hội}.

\section{5. Tài liệu tham
khảo}\label{tuxe0i-liux1ec7u-tham-khux1ea3o}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \href{https://en.wikipedia.org/wiki/Linear_regression}{Linear
  Regression - Wikipedia}
\item
  \href{http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/}{Simple
  Linear Regression Tutorial for Machine Learning}
\item
  \href{http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf}{Least
  Squares, Pseudo-Inverses, PCA \& SVD}
\end{enumerate}
