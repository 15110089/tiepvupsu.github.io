%!TEX root = book.tex

\chapter{Phân nhóm các thuật toán Machine Learning}

Có hai cách phổ biến phân nhóm các thuật toán Machine learning. Một là dựa trên phương thức học (learning style), hai là dựa trên chức năng (function) (của mỗi thuật toán). 
 
 
\section{Phân nhóm dựa trên phương thức học}
 
Theo phương thức học, các thuật toán Machine Learning thường được chia làm 4 nhóm: Supervise learning, Unsupervised learning, Semi-supervised lerning và Reinforcement learning. \textit{Có một số cách phân nhóm không có Semi-supervised learning hoặc Reinforcement learning.} 
 
\index{Supervised Learning}
\subsection{Supervised Learning (Học có giám sát) }
Supervised learning là thuật toán dự đoán đầu ra (outcome) của một dữ liệu mới (new input) dựa trên các cặp (\textit{input, outcome}) đã biết từ trước. Cặp dữ liệu này còn được gọi là (\textit{data. label}), tức (\textit{dữ liệu, nhãn}). Supervised learning là nhóm phổ biến nhất trong các thuật toán Machine Learning.  
 
Một cách toán học, Supervised learning là khi chúng ra có một tập hợp biến đầu vào $ \mathcal{X} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\} $ và một tập hợp nhãn tương ứng $ \mathcal{Y} = \{\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_N\} $, trong đó $ \mathbf{x}_i, \mathbf{y}_i $ là các vector.  
Các cặp dữ liệu biết trước $ (\mathbf{x}_i, \mathbf{y}_i) \in \mathcal{X} \times \mathcal{Y} $  
được gọi là tập \textit{training data} (dữ liệu huấn luyện). Từ tập traing data này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập $\mathcal{X}$ sang một phần tử (xấp xỉ) tương ứng của tập $\mathcal{Y}$: 
$$ \mathbf{y}_i \approx f(\mathbf{x}_i), ~~ \forall i = 1, 2, \dots, N$$  
Mục đích là xấp xỉ hàm số $f$ thật tốt để khi có một dữ liệu $\mathbf{x}$ mới, chúng ta có thể tính được nhãn tương ứng của nó $ \mathbf{y} = f(\mathbf{x}) $. 
     
\textbf{Ví dụ 1:} trong nhận dạng chữ viết tay (Hình \ref{fig:categories_mnist}), ta có ảnh của hàng nghìn ví dụ của mỗi chữ số được viết bởi nhiều người khác nhau. Chúng ta đưa các bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh tương ứng với chữ số nào. Sau khi thuật toán tạo ra (sau khi \textit{học}) một mô hình, tức một hàm số mà đầu vào là một bức ảnh và đầu ra là một chữ số, khi nhận được một bức ảnh mới mà mô hình \textbf{chưa nhìn thấy bao giờ}, nó sẽ dự đoán bức ảnh đó chứa chữ số nào. 
 
% <div class="imgcap"> 
%     <img src="http://www.rubylab.io/img/mnist.png" width = "600"> 
% <div class="thecap"><a href = "http://yann.lecun.com/exdb/mnist/">MNIST</a>: bộ cơ sở dữ liệu của chữ số viết tay. <br> (Nguồn: <a href ="http://www.rubylab.io/2015/03/18/simple-neural-network-implenentation-in-ruby/">Simple Neural Network implementation in Ruby)</a></div> 
% </div> 
 
 \begin{figure}
   \centering
   \includegraphics[width = .7\textwidth]{../categories/mnist.png}
   \caption{\href{http://yann.lecun.com/exdb/mnist/}{MNIST}: bộ cơ sở dữ liệu của chữ số viết tay. (Nguồn: \href{http://www.rubylab.io/2015/03/18/simple-neural-network-implenentation-in-ruby/}{Simple Neural Network implementation in Ruby})}
   \label{fig:categories_mnist}
 \end{figure}
 
Ví dụ này khá giống với cách học của con người khi còn nhỏ. Ta đưa bảng chữ cái cho một đứa trẻ và chỉ cho chúng đây là chữ A, đây là chữ B. Sau một vài lần được dạy thì trẻ có thể nhận biết được đâu là chữ A, đâu là chữ B trong một cuốn sách mà chúng chưa nhìn thấy bao giờ.  
 
\textbf{Ví dụ 2:} Thuật toán dò các khuôn mặt trong một bức ảnh đã được phát triển từ rất lâu. Thời gian đầu, facebook sử dụng thuật toán này để chỉ ra các khuôn mặt trong một bức ảnh và yêu cầu người dùng \textit{tag friends} - tức gán nhãn cho mỗi khuôn mặt. Số lượng cặp dữ liệu (\textit{khuôn mặt, tên người}) càng lớn, độ chính xác ở những lần tự động \textit{tag} tiếp theo sẽ càng lớn. 
 
\textbf{Ví dụ 3:} Bản thân thuật toán dò tìm các khuôn mặt trong 1 bức ảnh cũng là một thuật toán Supervised learning với training data (dữ liệu học) là hàng ngàn cặp (\textit{ảnh, mặt người}) và (\textit{ảnh, không phải mặt người}) được đưa vào. Chú ý là dữ liệu này chỉ phân biệt \textit{mặt người} và \textit{không phải mặt ngưòi} mà không phân biệt khuôn mặt của những người khác nhau. 
 
Thuật toán supervised learning còn được tiếp tục chia nhỏ ra thành hai loại chính:  
 
\index{Classification problems}
\subsubsection{Classification (Phân loại)}
 Một bài toán được gọi là \textit{classification} nếu các \textit{label} của \textit{input data} được chia thành một số hữu hạn nhóm. Ví dụ: Gmail xác định xem một email có phải là spam hay không; các hãng tín dụng xác định xem một khách hàng có khả năng thanh toán nợ hay không. Ba ví dụ phía trên được chia vào loại này.  
 
\index{Regression problems}
\subsubsection{Regression (Hồi quy)}
(tiếng Việt dịch là \textit{Hồi quy}, tôi không thích cách dịch này vì bản thân không hiểu nó nghĩa là gì) 
 
Nếu \textit{label} không được chia thành các nhóm mà là một giá trị thực cụ thể. Ví dụ: một căn nhà rộng $x ~ \text{m}^2$, có $y$ phòng ngủ và cách trung tâm thành phố $z~ \text{km}$ sẽ có giá là bao nhiêu? 
  
Gần đây \href{http://how-old.net/}{Microsoft có một ứng dụng dự đoán giới tính và tuổi dựa trên khuôn mặt}. Phần dự đoán giới tính có thể coi là thuật toán \textbf{Classification}, phần dự đoán tuổi có thể coi là thuật toán \textbf{Regression}. \textit{Chú ý rằng phần dự đoán tuổi cũng có thể coi là \textbf{Classification} nếu ta coi tuổi là một số nguyên dương không lớn hơn 150, chúng ta sẽ có 150 class (lớp) khác nhau.} 
 
\index{Unsupervised Learning}
\subsection{Unsupervised Learning (Học không giám sát)}
Trong thuật toán này, chúng ta không biết được \textit{outcome} hay \textit{nhãn} mà chỉ có dữ liệu đầu vào. Thuật toán unsupervised learning sẽ dựa vào cấu trúc của dữ liệu để thực hiện một công việc nào đó, ví dụ như phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimention reduction) để thuận tiện trong việc lưu trữ và tính toán. 
 
Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu vào $\mathcal{X} $ mà không biết \textit{nhãn} $\mathcal{Y}$ tương ứng.  
 
Những thuật toán loại này được gọi là Unsupervised learning vì không giống như Supervised learning, chúng ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào. Giống như khi ta học, không có thầy cô giáo nào chỉ cho ta biết đó là chữ A hay chữ B. Cụm \textit{không giám sát} được đặt tên theo nghĩa này.  
 
Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai loại:  
 
\index{Clustering problems}
\subsubsection{Clustering (phân nhóm)}
Một bài toán phân nhóm toàn bộ dữ liệu $\mathcal{X}$ thành các nhóm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ: phân nhóm khách hàng dựa trên hành vi mua hàng. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình thù và màu sắc khác nhau, ví dụ tam giác, vuông, tròn với màu xanh và đỏ, sau đó yêu cẩu trẻ phân chúng thành từng nhóm. Mặc dù không cho trẻ biết mảnh nào tương ứng với hình nào hoặc màu nào, nhiều khả năng chúng vẫn có thể phân loại các mảnh ghép theo màu hoặc hình dạng.  
 
\index{Association problems}
\subsubsection{Association}
Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví dụ: những khách hàng nam mua quần áo thường có xu hướng mua thêm đồng hồ hoặc thắt lưng; những khán giả xem phim Spider Man thường có xu hướng xem thêm phim Bat Man, dựa vào đó tạo ra một hệ thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua sắm.  
 

\index{Semi-Supervised Learning}
\subsection{Semi-Supervised Learning (Học bán giám sát)}
Các bài toán khi chúng ta có một lượng lớn dữ liệu $\mathcal{X}$ nhưng chỉ một phần trong chúng được gán nhãn được gọi là Semi-Supervised Learning. Những bài toán thuộc nhóm này nằm giữa hai nhóm được nêu bên trên.  
 
Một ví dụ điển hình của nhóm này là chỉ có một phần ảnh hoặc văn bản được gán nhãn (ví dụ bức ảnh về người, động vật hoặc các văn bản khoa học, chính trị) và phần lớn các bức ảnh/văn bản khác chưa được gán nhãn được thu thập từ internet. Thực tế cho thấy rất nhiều các bài toàn Machine Learning thuộc vào nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được (ảnh y học chẳng hạn). Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet.  
 
 
\index{Reinforcement Learning}
\subsection{Reinforcement Learning (Học Củng Cố)}
Reinforcement learning là các bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximizing the performance). Hiện tại, Reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nưóc đi tiếp theo để đạt được điểm số cao nhất. 
 
% <div class="imgcap"> 
% <div > 
% <a href = "/2016/12/27/categories/"> 
%     <img src="/assets/categories/alphago.jpeg" width = "800"></a> 
% </div> 
% <div class="thecap">AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của Reinforcement learning. <br> (Nguồn: <a href ="http://www.tomshardware.com/news/alphago-defeats-sedol-second-time,31377.html">AlphaGo AI Defeats Sedol Again, With 'Near Perfect Game')</a></div> 
% </div> 

 \begin{figure}
   \centering
   \includegraphics[width = .7\textwidth]{../categories/alphago.jpeg}
   \caption{AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của Reinforcement learning (Nguồn \href{http://www.tomshardware.com/news/alphago-defeats-sedol-second-time,31377.html}{AlphaGo AI Defeats Sedol Again, With 'Near Perfect Game'})}
   \label{fig:categories_alphago}
 \end{figure}
 
 
\textbf{Ví dụ 1:} AlphaGo (Hình \ref{fig:categories_alphago}) \href{https://gogameguru.com/tag/deepmind-alphago-lee-sedol/}{gần đây nổi tiếng với việc chơi cờ vây thắng cả con người}. \href{https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/}{Cờ vây được xem là có độ phức tạp cực kỳ cao} với tổng số nước đi là xấp xỉ $10^{761} $, so với cờ vua là $10^{120} $ và tổng số nguyên tử trong toàn vũ trụ là khoảng $10^{80}$!! Vì vậy, thuật toán phải chọn ra 1 nước đi tối ưu trong số hàng nhiều tỉ tỉ lựa chọn, và tất nhiên, không thể áp dụng thuật toán tương tự như \href{https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer}{IBM Deep Blue}) (IBM Deep Blue đã thắng con người trong môn cờ vua 20 năm trước). Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục đích cuối cùng của AlphaGo không phải là chơi như con người mà phải thậm chí thắng cả con người. Vì vậy, sau khi \textit{học} xong các ván cờ của con người, AlphaGo tự chơi với chính nó với hàng triệu ván chơi để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning. (Xem thêm tại \href{https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/}{Google DeepMind's AlphaGo: How it works}). 
 
 
\textbf{Ví dụ 2:} \href{https://www.youtube.com/watch?v=qv6UVOQ0F44}{Huấn luyện cho máy tính chơi game Mario}. Đây là một chương trình thú vị dạy máy tính chơi game Mario. Game này đơn giản hơn cờ vây vì tại một thời điểm, người chơi chỉ phải bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào. Đồng thời, phản ứng của máy cũng đơn giản hơn và lặp lại ở mỗi lần chơi (tại thời điểm cụ thể sẽ xuất hiện một chướng ngại vật cố định ở một vị trí cố định). Đầu vào của thuật toán là sơ đồ của màn hình tại thời điểm hiện tại, nhiệm vụ của thuật toán là với đầu vào đó, tổ hợp phím nào nên được bấm. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa trong thời gian bao lâu trong game, càng xa và càng nhanh thì được điểm thưởng càng cao (điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra). Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa. 
 
 
 
\section{Phân nhóm dựa trên chức năng }
 
Có một cách phân nhóm thứ hai dựa trên chức năng của các thuật toán. Trong phần này, tôi xin chỉ liệt kê các thuật toán. Thông tin cụ thể sẽ được trình bày trong các bài viết khác tại blog này. Trong quá trình viết, tôi có thể sẽ thêm bớt một số thuật toán.  
 
 
\subsection{Regression Algorithms}
\begin{enumerate}
  \item \href{http://machinelearningcoban.com/2016/12/28/linearregression/}{Linear Regression} 

  \item \href{http://machinelearningcoban.com/2017/01/27/logisticregression/#sigmoid-function}{Logistic Regression} 

  
  \item Stepwise Regression 
\end{enumerate}
 
 
\subsection{Classification Algorithms }

\begin{enumerate}
  \item Linear Classifier

  \item  Support Vector Machine (SVM)

  \item  Kernel SVM  

  \item Sparse Represntation-based classification (SRC) 
\end{enumerate}
 
 
\subsection{Instance-based Algorithms }
 
 \begin{enumerate}
   \item \href{http://machinelearningcoban.com/2017/01/08/knn/}{k-Nearest Neighbor (kNN)} 

   \item Learnin Vector Quantization (LVQ) 
 \end{enumerate}
 
 
\subsection{Regularization Algorithms }
 
 \begin{enumerate}
   \item  Ridge Regression  

   \item  Least Absolute Shringkage and Selection Operator (LASSO) 

   \item  Least-Angle Regression (LARS) 
 \end{enumerate}
 
 
\subsection{Bayesian Algorithms}
 \begin{enumerate}
   \item Naive Bayes 

   \item Gausian Naive Bayes  
 \end{enumerate}
 
 
\subsection{Clustering Algorithms}
\begin{enumerate}
  \item \href{http://machinelearningcoban.com/2017/01/01/kmeans/}{k-Means clustering}  

  \item k-Medians  

  \item Expectation Maximisation (EM)  
\end{enumerate}
 
 
\subsection{Artificial Neural Network Algorithms }
\begin{enumerate}
  \item \href{http://machinelearningcoban.com/2017/01/21/perceptron/}{Perceptron} 

  \item \href{http://machinelearningcoban.com/2017/02/17/softmax/}{Softmax Regression} 

  \item  \href{http://machinelearningcoban.com/2017/02/24/mlp/}{Multi-layer Perceptron} 

  \item \href{http://machinelearningcoban.com/2017/02/24/mlp/#-backpropagation}{Back-Propagation } 
 
\end{enumerate}
 
\subsection{Dimensionality Reduction Algorithms }
\begin{enumerate}
  \item Principal Component Analysis (PCA) 

  \item Linear Discriminant Analysis (LDA) 
 
\end{enumerate}
 
\subsection{Ensemble Algorithms }
 \begin{enumerate}
   \item Boosting 

   \item AdaBoost  

  \item Random Forest  
 \end{enumerate}
 
Và còn rất nhiều các thuật toán khác.  
 
 
\section{Tài liệu tham khảo }
\begin{enumerate}
  \item \href{http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/}{A Tour of Machine Learning Algorithms} 
  
  \item \href{https://ongxuanhong.wordpress.com/2015/10/22/diem-qua-cac-thuat-toan-machine-learning-hien-dai/}{Điểm qua các thuật toán Machine Learning hiện đại} 
 
 
\end{enumerate}
