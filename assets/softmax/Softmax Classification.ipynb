{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import linear_model           # for logistic regression\n",
    "from sklearn.metrics import accuracy_score # for evaluation\n",
    "from scipy import misc                     # for loading image\n",
    "from scipy import sparse\n",
    "np.random.seed(1)                          # for fixing random values\n",
    "\n",
    "\n",
    "\n",
    "# randomly generate data \n",
    "N = 10 # number of training sample \n",
    "d = 5 # data dimension \n",
    "C = 3 # number of classes \n",
    "\n",
    "X = np.random.randn(d, N)\n",
    "y = np.random.randint(0, 3, (N,))\n",
    "\n",
    "print(y)\n",
    "\n",
    "def convert_labels(y):\n",
    "    \"\"\"\n",
    "    convert 1d label to a matrix label: each column of this matrix \n",
    "    coresponding with 1 element in y. In i-th column of Y, only one non-zeros\n",
    "    element located in the y[i]-th position, and = 1 \n",
    "    ex: y = [0, 2, 1, 0], and 3 classes then return \n",
    "\n",
    "            [[1, 0, 0, 1],\n",
    "             [0, 0, 1, 0],\n",
    "             [0, 1, 0, 0]]\n",
    "    \"\"\"\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), (y, np.arange(len(y)))), \n",
    "                          shape = (np.amax(y) + 1, len(y))).toarray()\n",
    "    return Y \n",
    "\n",
    "Y = convert_labels(y)\n",
    "print(convert_labels(y))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "scores = [3.0, 1.0, 0.2]\n",
    "print(softmax(scores))\n",
    "    \n",
    "\n",
    "# cost \n",
    "def cost(X, Y, W):\n",
    "    Z = softmax(np.exp(W.T.dot(X)))\n",
    "    return np.sum(Y*np.log(np.exp(Z)))\n",
    "\n",
    "W_init = np.random.randn(d, C)\n",
    "\n",
    "def grad(X, Y, W):\n",
    "    Z = softmax(np.exp(W.T.dot(X)))\n",
    "    V = Z - Y\n",
    "    return X.dot(V.T)\n",
    "    \n",
    "def numerical_grad(X, Y, W, cost):\n",
    "    eps = 1e-4\n",
    "    g = np.zeros_like(W)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_p = W.copy()\n",
    "            W_n = W.copy()\n",
    "            W_p[i, j] += eps \n",
    "            W_n[i, j] -= eps\n",
    "        g[i] = (cost(X, Y, W_p) - cost(X, Y, W_n))/(2*eps)\n",
    "    return g \n",
    "\n",
    "g1 = grad(X, Y, W_init)\n",
    "g2 = numerical_grad(X, Y, W_init, cost)\n",
    "\n",
    "print(g1)\n",
    "print(g2)\n",
    "print(np.linalg.norm(g1 - g2))\n",
    "# print(cost(X, Y, W_init))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
