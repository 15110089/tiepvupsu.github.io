{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bạn hẳn thấy hình vẽ dưới đây quen thuộc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"imgcap\">\n",
    " <img src =\"https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/gradient_descent.png?raw=true\" align = \"center\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điểm màu xanh lục là điểm cực tiểu, và cũng là điểm _global minimum_, của hàm số \\\\(f(x) = \\frac{1}{2}(x-1)^2 - 2\\\\).\n",
    "\n",
    "Giả sử chúng ta đang quan tâm đến một hàm số một biến có đạo hàm mọi nơi. Xin cho tôi được nhắc lại vài điều đã quá quen thuộc:\n",
    "\n",
    "1. Điểm cực tiểu \\\\(x^*\\\\) của hàm số là điểm có đạo hàm \\\\(f'(x(t))\\\\) bằng 0. Hơn thế nữa, trong lân cận của nó, đạo hàm của các điểm phía bên trái \\\\(x^*\\\\) là không dương, đạo hàm của các điểm phía bên phải \\\\(x^*\\\\) là không âm.\n",
    "2. Đường tiếp tuyến với đồ thị hàm số đó tại 1 điểm bất kỳ có hệ số góc chính bằng đạo hàm của hàm số tại điểm đó. \n",
    "\n",
    "Trong hình phía trên, các điểm bên trái của điểm cực tiểu màu xanh lục có đạo hàm âm, các điểm bên phải có đạo hàm âm. Và đối với hàm số này, càng xa về phía trái của điểm cực tiểu thì đạo hàm càng âm, càng xa về phía phải thì đạo hàm càng âm. \n",
    "\n",
    "### Gradient Descent Method\n",
    "Trong Machine Learning nói riêng và Toán Tối Ưu nói chung, chúng ta thường xuyên phải tìm giá trị nhỏ nhất (hoặc đôi khi là lớn nhất) của một hàm số nào đó. Ví dụ như các hàm mất mát trong hai bài [Linear Regression](/2016/12/28/linearregression/) và [K-means Clustering](/2017/01/01/kmeans/). Nhìn chung, việc tìm giá trị nhỏ nhất (global minimum) của hàm số là không khả thi trong các bài toán này. Thay vào đó, người ta thường cố gắng tìm các điểm cực tiểu (local minimum), và ở một mức độ nào đó, coi đó là nghiệm cần tìm của bài toán. Các điểm cực tiểu là nghiệm của phương trình đạo hàm bằng 0. Nếu bằng một cách nào đó có thể tìm được toàn bộ (hữu hạn) các điểm cực tiểu, ta chỉ cần thay từng điểm cực tiểu đó vào hàm số rồi tìm điểm làm cho hàm có giá trị nhỏ nhất (_đoạn này nghe rất quen thuộc, đúng không?_). Tuy nhiên, trong hầu hết các trường hợp, việc giải phương trình đạo hàm bằng 0 trong thực tế vẫn là bất khả thi. Nguyên nhân có thể đến từ sự phức tạp của dạng của đạo hàm, từ việc các điểm dữ liệu có số chiều lớn, hoặc từ việc có quá nhiều điểm dữ liệu. \n",
    "\n",
    "Hướng tiếp cận phổ biến nhất là xuất phát từ một điểm mà chúng ta coi là _gần_ với nghiệm của bài toán, sau đó dùng một phép toán lặp để tiến dần đến điểm cần tìm, tức đến khi đạo hàm gần với 0. Gradient Descent và các biến thể của nó là một phương pháp được dùng nhiều nhất. \n",
    "\n",
    "\n",
    "Quay trở lại hình vẽ ban đầu và một vài quan sát tôi đã nêu. Giả sử  \\\\(x^*\\\\) là điểm global minimum màu xanh lục, \\\\(x(t)\\\\) là điểm ta tìm được sau vòng lặp thứ \\\\(t\\\\). Ta cần tìm một thuật toán để đưa \\\\(x(t)\\\\) về càng gần \\\\(x^*\\\\) càng tốt. \n",
    "Chúng ta lại có thêm hai quan sát nữa:\n",
    "\n",
    "1. Nếu đạo hàm của hàm số tại \\\\(x(t)\\\\): \\\\(f'(x(t)) > 0\\\\) thì \\\\(x(t)\\\\) nằm về bên phải so với \\\\(x^*\\\\) (và ngược lại). Để điểm tiếp theo \\\\(x(t+1)\\\\) gần với \\\\(x^*\\\\) hơn, chúng ta cần di chuyển \\\\(x(t)\\\\) về phía bên trái, tức về phía _âm_. Nói các khác, __chúng ta cần di chuyển ngược dấu với đạo hàm__:\n",
    "\\\\[\n",
    "x(t+1) = x(t) + \\delta\n",
    "\\\\]\n",
    "Trong đó \\\\(\\delta\\\\) ngược dấu với đạo hàm.\n",
    "\n",
    "2. \\\\(f'(x(t))\\\\) càng lớn hơn 0 thì \\\\(x(t)\\\\) càng xa \\\\(x^*\\\\) về phía bên phải (và ngược lại). Vậy, lượng di chuyển \\delta, một cách trực quan nhất, là tỉ lệ với \\\\(-f'(x(t))\\\\).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_sin = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.25252365698 5\n"
     ]
    }
   ],
   "source": [
    "def grad(x):\n",
    "    return 2*x+ c_sin*np.cos(x)\n",
    "\n",
    "def cost(x):\n",
    "    return x**2 + c_sin*np.sin(x)\n",
    "\n",
    "def myGD1(eta, x0):\n",
    "    x = [x0]\n",
    "    for it in range(100):\n",
    "        x_new = x[-1] - eta*grad(x[-1])\n",
    "        if abs(grad(x_new)) < 1e-3:\n",
    "            break\n",
    "        x.append(x_new)\n",
    "    return (x, it)\n",
    "\n",
    "(w, it) = myGD1(.1, -5)\n",
    "print(w[-1], it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minh họa thuật toán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation \n",
    "def save_gif(eta, x0):\n",
    "    (x, it) = myGD1(eta, x0)\n",
    "    xx = np.linspace(-4.5, 5.5, 1000)\n",
    "    yy = cost(xx)\n",
    "    x = np.asarray(x)\n",
    "    y = cost(x)\n",
    "    g = grad(x)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    title = '$f(x) = x^2 + %d\\sin(x)$; ' %c_sin\n",
    "    title += '$x_0 =  %f$; ' %x0\n",
    "    title += r'$\\alpha = %f$ ' % eta\n",
    "    \n",
    "    def update(ii):\n",
    "        label2 = 'iter %d/%d: ' %(ii, it) + 'cost = %.2f' % y[ii] + ', grad = %.4f' %g[ii]\n",
    "\n",
    "        animlist = plt.cla()        \n",
    "        animlist = plt.axis([-6, 6, -8, 30])\n",
    "        animlist = plt.plot(xx, yy)\n",
    "        animlist = plt.title(title)\n",
    "        if ii == 0:\n",
    "            animlist = plt.plot(x[ii], y[ii], 'ro', markersize = 7)\n",
    "        else:\n",
    "            animlist = plt.plot(x[ii-1], y[ii-1], 'ko', markersize = 7)\n",
    "            animlist = plt.plot([x[ii-1], x[ii]], [y[ii-1], y[ii]], 'k-')\n",
    "            animlist = plt.plot(x[ii], y[ii], 'ro', markersize = 7)\n",
    "\n",
    "        ax.set_xlabel(label2)\n",
    "        return animlist, ax\n",
    "    \n",
    "    file_name = '1dimg_' + str(c_sin) + '_' \\\n",
    "        + str(eta) + '_'+ str(x0) + '.gif'\n",
    "    anim = FuncAnimation(fig, update, \n",
    "                frames=np.arange(0, it), interval=500)\n",
    "    anim.save(file_name, dpi=100, writer='imagemagick')\n",
    "\n",
    "save_gif(.01, -5)\n",
    "save_gif(.1, -5)\n",
    "save_gif(.5, -5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### GD cho bài toán Linear Regression \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thảo luận\n",
    "### Newton's method\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/NewtonIteration_Ani.gif/300px-NewtonIteration_Ani.gif\" align = \"center\" width = \"500\">\n",
    "</div>\n",
    "\n",
    "### Line search\n",
    "\n",
    "### Stochastic Gradient Descent \n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "\n",
    "## Tài liệu tham khảo\n",
    "1. http://sebastianruder.com/optimizing-gradient-descent/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
