{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong [phần 1](/2017/01/12/gradientdescent/) của Gradient Descent (GD), tôi đã giới thiệu với bạn đọc về thuật toán Gradient Descent. Tôi xin nhắc lại rằng nghiệm cuối cùng của Gradient Descent phụ thuộc vào điểm khởi tạo và learning rate. Trong bài này, tôi xin đề cập một vài phương pháp thường được dùng để khắc phục những hạn chế của GD . Đồng thời, các thuật toán biến thể của GD thường được áp dụng trong các mô hình Deep Learning cũng sẽ được tổng hợp. \n",
    "\n",
    "**Trong trang này:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các thuật toán tối ưu Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "#### Nhắc lại thuạt toán Gradient Descent\n",
    "Dành cho các bạn chưa đọc [phần 1](/2017/01/12/gradientdescent/) của Gradient Descent. Để giải bài toán tìm điểm _global optimal_ của hàm mất mát \\\\(J(\\theta)\\\\) (Hàm mất mát cũng thường được ký hiệu là \\\\(J()\\\\) với \\\\(\\theta\\\\) là tập hợp các tham số của mô hình, thuật tóan GD được phát biểu như sau:\n",
    "\n",
    "-------------\n",
    "** Thuật toán Gradient Descent:**\n",
    "1. Dự đoán một điểm khởi tạo \\\\(\\theta = \\theta_0\\\\).\n",
    "2. Cập nhật \\\\(\\theta\\\\) đến khi đạt được kết quả chấp nhận được: \n",
    "\\\\[\n",
    "\\theta = \\theta - \\eta \\nabla_{\\theta}J(\\theta)\n",
    "\\\\]\n",
    "\n",
    "với \\\\(\\nabla_{\\theta}J(\\theta)\\\\) là đạo hàm của hàm mất mát tại \\\\(\\theta\\\\).\n",
    "\n",
    "------------\n",
    "\n",
    "#### Gradient dưới góc nhìn vật lý \n",
    "\n",
    "Thuật toán GD thường được ví với tác dụng của trọng lực lên một hòn bi đặt trên một mặt có dạng như hình một thung lũng giống như hình 1a) dưới đây. Khi có một điểm ở đáy thung lũng như điểm C, bất kể ta đặt hòn bi ở A hay B thì cuối cùng hòn bi cũng sẽ lăn xuống và kết thúc ở vị trí C.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"/assets/GD/momentum.png\" align = \"center\" width = \"800\">\n",
    "</div>\n",
    "\n",
    "Tuy nhiên, nếu như mặt phẳng có hai đáy thung lũng như Hình 1b) thì tùy vào việc đặt bi ở A hay B, vị trí cuối cùng của bi sẽ ở C hoặc D. Điểm D là một điểm local minimum chúng ta không mong muốn. \n",
    "\n",
    "Nếu suy nghĩ một cách vật lý hơn, vẫn trong Hình 1b), nếu vận tốc ban đầu của bi khi ở điểm B đủ lớn, khi bi lăn đến điểm D, theo _đà_, bi có thể tiếp tục di chuyển lên dốc phía bên trái của D. Và nếu vận tốc ban đầu lớn hơn nữa, bi có thể vượt dốc tới điểm E rồi lăn xuống C như trong Hình 1c). Đây chính là điều chúng ta mong muốn. Bạn đọc có thể đặt câu hỏi rằng liệu bi lăn từ A tới C có theo _đà_ lăn tới E rồi tới D không. Xin trả lời rằng điều này khó xảy ra hơn vì nếu so với đôc DE thì dốc CE cao hơn nhiều.\n",
    "\n",
    "Dựa trên hiện tượng này, một thuật toán được ra đời nhằm khắc phục việc nghiệm của GD rơi vào một điểm cực tiểu không mong muốn. Thuật toán đó có tên là Momentum (tức _theo đà_ trong tiếng Việt).\n",
    "\n",
    "### Gradient Descent với Momentum\n",
    "Để biểu diễn _momentum_ bằng toán học thì chúng ta phải làm thế nào?\n",
    "\n",
    "Trong GD, chúng ta cần tính lượng thay đổi ở thời điểm \\\\(t\\\\) để cập nhật vị trí mới cho nghiệm (tức _hòn bi_). Nếu chúng ta coi đại lượng này như vận tốc \\\\(v_t\\\\) trong vật lý, vị trí mới của _hòn bi_ sẽ là \\\\(\\theta\\_{t+1} = \\theta\\_{t} - v\\_t\\\\). Dấu trừ thể hiện việc phải di chuyển ngược với đạo hàm. Công việc của chúng ta bây giờ là tính đại lượng \\\\(v\\_t\\\\) sao cho nó vừa mang thông tin của _độ dốc_ (tức đạo hàm), vừa mang thông tin của _đà_, tức vận tốc trước đó \\\\(v\\_{t-1}\\\\) (chúng ta coi như vận tốc ban đầu \\\\(v\\_0=0\\\\)). Một cách đơn giản nhất, chúng ta cộng (có trọng số) hai đại lượng này lại:\n",
    "\\\\[\n",
    "v\\_{t}= \\gamma v\\_{t-1} + \\eta \\nabla\\_{\\theta}J(\\theta)\n",
    "\\\\]\n",
    "\n",
    "Trong đó \\\\(\\gamma\\\\) thường được chọn là một giá trị khoảng 0.9. Sau đó vị trí mới của _hòn bi_ được xác định như sau:\n",
    "\\\\[\n",
    "\\theta = \\theta - v_t\n",
    "\\\\]\n",
    "\n",
    "Thuật toán đơn giản này tỏ ra rất hiệu quả trong các bài toán thực tế. Dưới đây là một ví dụ với python.\n",
    "#### Một ví dụ nhỏ\n",
    "Chúng ta xem xét một hàm đơn giản có hai điểm local minimum, trong đó 1 điểm là global minimum:\n",
    "\\\\[\n",
    "f(x) = x^2 + 10\\sin(x)\n",
    "\\\\]\n",
    "Có đạo hàm là: \\\\(f'(x) = 2x + 10\\cos(x)\\\\). Hình 2 dưới đây thể hiện sự khác nhau giữa thuật toán GD và thuật toán GD với Momentum:\n",
    "\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/nomomentum1d.gif\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/momentum1d.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình bên trái là đường đi của nghiệm khi không sử dụng Momentum, thuật toán hội tụ sau chỉ 5 vòng lặp nhưng nghiệm tìm được là nghiệm local minimun.\n",
    "\n",
    "Hình bên phải là đường đi của nghiệm khi có sử dụng Momentum, _hòn bi_ đã có thể vượt dốc tới khu vực gần điểm global minimun, sau đó dao động xung quanh điểm này, giảm tốc rồi cuối cùng tới đích. Mặc dù mất nhiều vòng lặp hơn, GD với Momentum cho chúng ta nghiệm chính xác hơn. Quan sát đường đi của _hòn bi_ trong trường hợp này, chúng ta thấy rằng điều này giống với vật lý hơn!\n",
    "\n",
    "Nếu biết trước điểm _đặt bi_ ban đầu `theta`, đạo hàm của hàm mất mát tại một điểm bất kỳ `grad(theta)`, `gamma` và learning rate `eta`, chúng ta có thể viết hàm số `GD_momentum` trong Python như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GD_momentum(theta_init, grad, eta, gamma):\n",
    "    theta = [theta_init]\n",
    "    v = [np.zeros_like(theta_init)]\n",
    "    for it in range(100):\n",
    "        theta_new = gamma*theta[-1] + eta*grad(theta[-1])\n",
    "        theta_new = theta[-1] - v_new\n",
    "        if np.linalg.norm(grad(theta_new))/len(theta_new) < 1e-3:\n",
    "            break\n",
    "        w.append(theta_new)\n",
    "        v.append(v_new)\n",
    "    return (w, it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biến thể của Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tóm tắt các thuật toán bằng Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thảo luận\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD với Momentum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
