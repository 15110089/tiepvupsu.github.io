{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong [phần 1](/2017/01/12/gradientdescent/) của Gradient Descent (GD), tôi đã giới thiệu với bạn đọc về thuật toán Gradient Descent. Tôi xin nhắc lại rằng nghiệm cuối cùng của Gradient Descent phụ thuộc vào điểm khởi tạo và learning rate. Trong bài này, tôi xin đề cập một vài phương pháp thường được dùng để khắc phục những hạn chế của GD . Đồng thời, các thuật toán biến thể của GD thường được áp dụng trong các mô hình Deep Learning cũng sẽ được tổng hợp. \n",
    "\n",
    "**Trong trang này:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các thuật toán tối ưu Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "#### Nhắc lại thuạt toán Gradient Descent\n",
    "Dành cho các bạn chưa đọc [phần 1](/2017/01/12/gradientdescent/) của Gradient Descent. Để giải bài toán tìm điểm _global optimal_ của hàm mất mát \\\\(J(\\theta)\\\\) (Hàm mất mát cũng thường được ký hiệu là \\\\(J()\\\\) với \\\\(\\theta\\\\) là tập hợp các tham số của mô hình, thuật tóan GD được phát biểu như sau:\n",
    "\n",
    "-------------\n",
    "** Thuật toán Gradient Descent:**\n",
    "1. Dự đoán một điểm khởi tạo \\\\(\\theta = \\theta_0\\\\).\n",
    "2. Cập nhật \\\\(\\theta\\\\) đến khi đạt được kết quả chấp nhận được: \n",
    "\\\\[\n",
    "\\theta = \\theta - \\eta \\nabla_{\\theta}J(\\theta)\n",
    "\\\\]\n",
    "\n",
    "với \\\\(\\nabla_{\\theta}J(\\theta)\\\\) là đạo hàm của hàm mất mát tại \\\\(\\theta\\\\).\n",
    "\n",
    "------------\n",
    "\n",
    "#### Gradient dưới góc nhìn vật lý \n",
    "\n",
    "Thuật toán GD thường được ví với tác dụng của trọng lực lên một hòn bi đặt trên một mặt có dạng như hình một thung lũng giống như hình 1a) dưới đây. Khi có một điểm ở đáy thung lũng như điểm C, bất kể ta đặt hòn bi ở A hay B thì cuối cùng hòn bi cũng sẽ lăn xuống và kết thúc ở vị trí C.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"/assets/GD/momentum.png\" align = \"center\" width = \"800\">\n",
    "</div>\n",
    "\n",
    "Tuy nhiên, nếu như mặt phẳng có hai đáy thung lũng như Hình 1b) thì tùy vào việc đặt bi ở A hay B, vị trí cuối cùng của bi sẽ ở C hoặc D. Điểm D là một điểm local minimum chúng ta không mong muốn. \n",
    "\n",
    "Nếu suy nghĩ một cách vật lý hơn, vẫn trong Hình 1b), nếu vận tốc ban đầu của bi khi ở điểm B đủ lớn, khi bi lăn đến điểm D, theo _đà_, bi có thể tiếp tục di chuyển lên dốc phía bên trái của D. Và nếu vận tốc ban đầu lớn hơn nữa, bi có thể vượt dốc tới điểm E rồi lăn xuống C như trong Hình 1c). Đây chính là điều chúng ta mong muốn. Bạn đọc có thể đặt câu hỏi rằng liệu bi lăn từ A tới C có theo _đà_ lăn tới E rồi tới D không. Xin trả lời rằng điều này khó xảy ra hơn vì nếu so với đôc DE thì dốc CE cao hơn nhiều.\n",
    "\n",
    "Dựa trên hiện tượng này, một thuật toán được ra đời nhằm khắc phục việc nghiệm của GD rơi vào một điểm cực tiểu không mong muốn. Thuật toán đó có tên là Momentum (tức _theo đà_ trong tiếng Việt).\n",
    "\n",
    "### Gradient Descent với Momentum\n",
    "Để biểu diễn _momentum_ bằng toán học thì chúng ta phải làm thế nào?\n",
    "\n",
    "Trong GD, chúng ta cần tính lượng thay đổi ở thời điểm \\\\(t\\\\) để cập nhật vị trí mới cho nghiệm (tức _hòn bi_). Nếu chúng ta coi đại lượng này như vận tốc \\\\(v_t\\\\) trong vật lý, vị trí mới của _hòn bi_ sẽ là \\\\(\\theta\\_{t+1} = \\theta\\_{t} - v\\_t\\\\). Dấu trừ thể hiện việc phải di chuyển ngược với đạo hàm. Công việc của chúng ta bây giờ là tính đại lượng \\\\(v\\_t\\\\) sao cho nó vừa mang thông tin của _độ dốc_ (tức đạo hàm), vừa mang thông tin của _đà_, tức vận tốc trước đó \\\\(v\\_{t-1}\\\\) (chúng ta coi như vận tốc ban đầu \\\\(v\\_0=0\\\\)). Một cách đơn giản nhất, chúng ta cộng (có trọng số) hai đại lượng này lại:\n",
    "\\\\[\n",
    "v\\_{t}= \\gamma v\\_{t-1} + \\eta \\nabla\\_{\\theta}J(\\theta)\n",
    "\\\\]\n",
    "\n",
    "Trong đó \\\\(\\gamma\\\\) thường được chọn là một giá trị khoảng 0.9. Sau đó vị trí mới của _hòn bi_ được xác định như sau:\n",
    "\\\\[\n",
    "\\theta = \\theta - v_t\n",
    "\\\\]\n",
    "\n",
    "Thuật toán đơn giản này tỏ ra rất hiệu quả trong các bài toán thực tế (trong không gian nhiều chiều, cách tính toán cũng hoàn tòan tương tự). Dưới đây là một ví dụ trong không gian một chiều.\n",
    "\n",
    "#### Một ví dụ nhỏ\n",
    "Chúng ta xem xét một hàm đơn giản có hai điểm local minimum, trong đó 1 điểm là global minimum:\n",
    "\\\\[\n",
    "f(x) = x^2 + 10\\sin(x)\n",
    "\\\\]\n",
    "Có đạo hàm là: \\\\(f'(x) = 2x + 10\\cos(x)\\\\). Hình 2 dưới đây thể hiện sự khác nhau giữa thuật toán GD và thuật toán GD với Momentum:\n",
    "\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/nomomentum1d.gif\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/momentum1d.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình bên trái là đường đi của nghiệm khi không sử dụng Momentum, thuật toán hội tụ sau chỉ 5 vòng lặp nhưng nghiệm tìm được là nghiệm local minimun.\n",
    "\n",
    "Hình bên phải là đường đi của nghiệm khi có sử dụng Momentum, _hòn bi_ đã có thể vượt dốc tới khu vực gần điểm global minimun, sau đó dao động xung quanh điểm này, giảm tốc rồi cuối cùng tới đích. Mặc dù mất nhiều vòng lặp hơn, GD với Momentum cho chúng ta nghiệm chính xác hơn. Quan sát đường đi của _hòn bi_ trong trường hợp này, chúng ta thấy rằng điều này giống với vật lý hơn!\n",
    "\n",
    "Nếu biết trước điểm _đặt bi_ ban đầu `theta`, đạo hàm của hàm mất mát tại một điểm bất kỳ `grad(theta)`, `gamma` và learning rate `eta`, chúng ta có thể viết hàm số `GD_momentum` trong Python như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_converged(theta_new, grad):\n",
    "    return np.linalg.norm(grad(theta_new))/\n",
    "                            len(theta_new) < 1e-3\n",
    "\n",
    "def GD_momentum(theta_init, grad, eta, gamma):\n",
    "    # Suppose we want to store history of theta\n",
    "    theta = [theta_init]\n",
    "    v_old = np.zeros_like(theta_init)\n",
    "    for it in range(100):\n",
    "        v_new = gamma*v_old + eta*grad(theta[-1])\n",
    "        theta_new = theta[-1] - v_new\n",
    "        if has_converge(theta_new, grad):\n",
    "            break \n",
    "        w.append(theta_new)\n",
    "        v_old = v_new\n",
    "    return theta \n",
    "    # this variable includes every points in the path\n",
    "    # if you just want the final answer, \n",
    "    # use `return theta[-1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method\n",
    "Nhắc lại rằng, cho tới thời điểm này, chúng ta luôn giải phương trình đạo hàm của hàm mất mát bằng 0 để tìm các điểm local minimun. (Và trong nhiều trường hợp, coi nghiệm tìm được là nghiệm của bài toán tìm giá trị nhỏ nhất của hàm mất mát). Có một thuật toán nối tiếng giúp giải bài toán \\\\(f(x) = 0\\\\), thuật toán đó có tên là Newton's method.\n",
    "\n",
    "\n",
    "#### Newton's method cho giải phương trình \\\\(f(x) = 0\\\\)\n",
    "\n",
    "Thuật toán Newton's method được mô tả trong hình động minh họa dưới đây:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif\" align = \"center\" width = \"500\">\n",
    " <div class = \"thecap\"> Minh họa thuật toán Newton's method trong giải phương trình. (Nguồn: <a = href = \"https://en.wikipedia.org/wiki/Newton's_method\">Wikipedia</a>).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ý tưởng giải bài toán \\\\(f(x) = 0\\\\) bằng phương pháp Newton's method như sau. Chọn một điểm \\\\(x\\_0\\\\) mà ta dự đoán là gần với nghiệm. Sau đó vẽ đường tiếp tuyến (mặt tiếp tuyến trong không gian nhiều chiều) với đồ thị hàm số \\\\(y = f(x)\\\\) tại điểm trên đồ thị có hoành độ \\\\(x_0\\\\). Giao điểm \\\\(x_1\\\\) của đường tiếp tuyến này với trục hoành được xem như gần với nghiệm hơn. Thuật toán lặp lại với điểm mới \\\\(x_1\\\\) và cứ như vậy đến khi ta được \\\\(f(x_t) \\approx 0\\\\).\n",
    "\n",
    "Đó là ý nghĩa hình học của Newton's method, chúng ta cần một công thức để có thể dựa vào đó để lập trình. Việc này không quá phức tạp với các bạn thi đại học môn toán ở VN. Thật vậy, phương trình tiếp tuyến với đồ thị của hàm \\\\(f(x)\\\\) tại điểm có hoành độ \\\\(x_t\\\\) là:\n",
    "\\\\[\n",
    "y = f'(x)(x - x_t) + f(x_t)\n",
    "\\\\]\n",
    "Giao điểm của đường thẳng này với trục \\\\(x\\\\) tìm được bằng cách giải phương trình vế phải của biểu thức trên bằng 0, tức là:\n",
    "\\\\[\n",
    "x = x\\_t - \\frac{f(x\\_t)}{f'(x\\_t)} \\triangleq x\\_{t+1}\n",
    "\\\\]\n",
    "\n",
    "### Newton's method trong bài toán tìm local minimun\n",
    "Áp dụng phương pháp này cho việc giải phương trình \\\\(f'(x) = 0\\\\) ta có:\n",
    "\\\\[\n",
    "x\\_{t+1} = x\\_t - \\frac{1}{f''(x_t)}{f'(x_t)}\n",
    "\\\\]\n",
    "\n",
    "Và trong không gian nhiều chiều với \\\\(\\theta\\\\) là biến:\n",
    "\\\\[\n",
    "\\theta = \\theta - \\frac{1}{\\|\\nabla^2\\_{\\theta} J(\\theta)\\|} \\nabla\\_{\\theta} J(\\theta)\n",
    "\\\\]\n",
    "trong đó \\\\(\\nabla^2\\_{\\theta} J(\\theta)\\\\) là đạo hàm bậc hai của hàm mất mất. Biểu thức này là một ma trận nếu \\\\(\\theta\\\\) là một vector. Và \\\\(\\|\\nabla^2\\_{\\theta} J(\\theta)\\|\\\\) chính là định thức (determinant) của ma trận đó. \n",
    "\n",
    "Bạn đọc có thể nhận thấy rằng đây chính là trường hợp đặc biệt của Gradient Descent với learning rate được tính chính xác:\n",
    "\\\\[\n",
    "eta = \\frac{1}{\\|\\nabla^2\\_{\\theta} J(\\theta)\\|}\n",
    "\\\\]\n",
    "\n",
    "Nếu có một phương pháp hiệu quả để tính \\\\(\\|\\nabla^2\\_{\\theta} J(\\theta)\\|\\\\), Newton's method thường cho nghiệm sau ít vòng lặp hơn so với GD vì tại mỗi vòng lặp, nó cho biết chính xác _quãng đường cần di chuyển_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hạn chế của Newton's method\n",
    "1. Điểm khởi tạo phải *rất* gần với nghiệm \\\\(x^\\*\\\\).\n",
    "Ý tưởng sâu xa hơn của Newton's method là dựa trên khai triển Taylor của hàm số \\\\(f(x)\\\\) tới đạo hàm thứ nhất:\n",
    "\\\\[\n",
    "0 = f(x^\\*) \\approx f(x\\_t) + f'(x\\_t)(x\\_t - x^\\*)\n",
    "\\\\]\n",
    "Từ đó suy ra: \\\\(x^\\* \\approx x_t - \\frac{f(x_t)}{f'(x_t)}\\\\). \n",
    "Một điểm rất quan trọng, khai triển Taylor chỉ đúng nếu \\\\(x_t\\\\) rất gần với \\\\(x^\\*\\\\)!\n",
    "Dưới đây là một ví dụ kinh điển trên Wikipedia về việc Newton's method cho một dãy số phân kỳ (divergence).\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/NewtonsMethodConvergenceFailure.svg/300px-NewtonsMethodConvergenceFailure.svg.png\" align = \"center\" width = \"400\">\n",
    " <div class = \"thecap\"> Nghiệm là một điểm gần -2. Tiếp tuyến của đồ thị hàm số tại điểm có hoành độ bằng 0 cắt trục hoành tại 1, và ngược lại. Trong trường hợp này, Newton's method không bao giờ hội tụ. (Nguồn: <a = href = \"https://en.wikipedia.org/wiki/Newton's_method\">Wikipedia</a>).\n",
    "</div>\n",
    "\n",
    "2. Nhận thấy rằng trong việc giải phương trình \\\\(f(x) = 0\\\\), chúng ta có đạo hàm ở mẫu số. Khi đạo hàm này gần với 0, ta sẽ được một đường thằng song song hoặc gần song song với trục hoành. Ta sẽ hoặc không tìm được giao điểm, hoặc được một giao điểm ở vô cùng. Đặc biệt, khi nghiệm chính là điểm có đạo hàm bằng 0, thuật toán gần như sẽ không tìm được nghiệm!\n",
    "\n",
    "3. Khi áp dụng Newton's method cho thuật toán GD, chúng ta cần tính định thức của đạo hàm bậc hai. Khi số chiều và số điểm dữ liệu lớn, đạo hàm bậc hai của hàm mất mát sẽ là một ma trận rất lớn, ảnh hưởng tới cả memory và tốc độ tính toán của hệ thống. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các thuật toán khác\n",
    "Ngoài hai thuật toán trên, có rất nhiều thuật toán nâng cao khác được sử dụng trong các bài toán thực tế, đặc biệt là các bài toán Deep Learning. Để tránh việc độc giả bị ngợp, tôi sẽ không đề cập đến các thuật toán đó trong bài này mà sẽ dành thời gian nói tới nếu có dịp trong tương lai, khi blog đã đủ lớn và đã trang bị cho các bạn một lượng kiến thức nhất định. \n",
    "\n",
    "Tuy nhiên, bạn đọc nào muốn đọc thêm có thể tìm được rất nhiều thông tin hữu ích trong bài này:\n",
    "[An overview of gradient descent optimization algorithms ](http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biến thể của Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tóm tắt các thuật toán bằng Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thảo luận\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD với Momentum\n",
    "1. [Newton's method - Wikipedia](https://en.wikipedia.org/wiki/Newton's_method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
