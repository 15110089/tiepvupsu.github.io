{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Giới thiệu\n",
    "\n",
    "### Nhắc lại hai mô hình tuyến tính đã đề cập\n",
    "Hai mô hình tuyến tính (linear models) [Linear Regression](/2016/12/28/linearregression/) và [Perceptron Learning Algorithm](/2017/01/21/perceptron/) (PLA) chúng ta đã biết đều có chung một dạng:\n",
    "\\\\[\n",
    "y = f(\\mathbf{w}^T\\mathbf{x})\n",
    "\\\\]\n",
    "\n",
    "trong đó \\\\(f()\\\\) được gọi là activation function, và \\\\(\\mathbf{x}\\\\) được hiểu là dữ liệu mở rộng với \\\\(x\\_0 = 1\\\\) được thêm vào để thuận tiện cho việc tính toán. Với Linear Regression thì \\\\(f(s) = s\\\\), với PLA thì \\\\(f(s) = \\text{sgn}(s)\\\\). Trong linear regression, tích vô hướng \\\\(\\mathbf{w}^T\\mathbf{x}\\\\) được trực tiếp sử dụng để dự đoán output \\\\(y\\\\), loại này phù hợp nếu chúng ta cần dự đoán một giá trị thực của đầu ra không bị chặn trên và dưới. Trong PLA, đầu ra chỉ nhận một trong hai giá trị \\\\(1\\\\) hoặc \\\\(-1 \\\\), phù hợp với các bài toán _binary classification_. \n",
    "\n",
    "Trong bài này, tôi sẽ giới thiệu mô hình thứ ba với activation khác, được sử dụng cho các bài toán _flexible_ hơn. Trong dạng này, đầu ra có thể được thể hiện dưới dạng xác suất (probability). Ví dụ: xác suất thi đỗ nếu biết thời gian ôn thi, xác suất ngày mai có mưa dựa trên những thông tin đo được trong ngày hôm nay,... Mô hình mới này của chúng ta có tên là _logistic regression_. Mô hình này giống với linear regression ở khía cạnh đầu ra là thực, và giống với PLA ở việc đầu ra bị chặn (trong đoạn \\\\([0, 1]\\\\)). Mặc dù trong tên có chứa từ _regression_, logistic regression thường được sử dụng nhiều hơn cho các bài toán classification.\n",
    "\n",
    "### Một ví dụ nhỏ \n",
    "Tôi xin được sử dụng [một ví dụ trên Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression):\n",
    "\n",
    "> Một nhóm 20 sinh viên dành thời gian trong khoảng từ 0 đến 6 giờ cho việc ôn thi. Thời gian ôn thi này ảnh hưởng đến xác suất sinh viên vượt qua kỳ thi như thế nào?_\n",
    "\n",
    "Kết quả thu được như sau:\n",
    "\n",
    "| Hours     | Pass   | Hours     | Pass   |\n",
    "| :-------: | ------ | :-------: | ------ |\n",
    "| .5        | 0      | 2.75      | 1      |\n",
    "| .75       | 0      | 3         | 0      |\n",
    "| 1         | 0      | 3.25      | 1      |\n",
    "| 1.25      | 0      | 3.5       | 0      |\n",
    "| 1.5       | 0      | 4         | 1      |\n",
    "| 1.75      | 0      | 4.25      | 1      |\n",
    "| 1.75      | 1      | 4.5       | 1      |\n",
    "| 2         | 0      | 4.75      | 1      |\n",
    "| 2.25      | 1      | 5         | 1      |\n",
    "| 2.5       | 0      | 5.5       | 1      |\n",
    "\n",
    "Mặc dù có một chút _bất công_ khi học 3.5 giờ thì trượt, còn học 1.75 giờ thì lại đỗ, nhưng nhìn chung, học càng nhiều thì khả năng đỗ càng cao. PLA không thể áp dụng được cho bài toán này vì không thể nói một người học bao nhiêu giờ thì 100% trượt hay đỗ, và thực tế là dữ liệu này cũng không _linear separable_ nên thuật toán PLA sẽ không làm việc ở đây. Chúng ta biểu diễn các điểm này trên đồ thị để thấy rõ hơn:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\LogisticRegression\\ex1.png\" align = \"center\" width = \"800\">\n",
    "<div class = \"thecap\">Hình 1: Ví dụ về kết quả thi dựa trên số giờ ôn tập.</div>\n",
    "</div> \n",
    "\n",
    "Nhận thấy rằng cả linear regression và PLA đều không phù hợp với bài toán này, chúng ta cần một mô hình _flexible_ hơn.\n",
    "\n",
    "### Mô hình Logistic Regression\n",
    "Đầu ra dự đoán của:\n",
    "\n",
    "* Linear Regression: \n",
    "\\\\[\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}\n",
    "\\\\]\n",
    "* PLA:\n",
    "\\\\[\n",
    "f(\\mathbf{x}) = \\text{sgn}(\\mathbf{w}^T\\mathbf{x})\n",
    "\\\\]\n",
    "\n",
    "Đầu ra dự đoán của logistic regression thường được viết chung dưới dạng:\n",
    "\\\\[\n",
    "f(\\mathbf{x}) = \\theta(\\mathbf{w}^T\\mathbf{x})\n",
    "\\\\]\n",
    "\n",
    "Trong đó \\\\(\\theta\\\\) được gọi là logistic function. Một số activation cho mô hình tuyến tính được cho trong hình dưới đây:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\LogisticRegression\\sigmoid.png\" align = \"center\" width = \"800\">\n",
    "<div class = \"thecap\">Hình 1: Các activation function khác nhau cho mô hình tuyến tính.</div>\n",
    "</div> \n",
    "\n",
    "* Đường màu vàng biểu diễn linear regression. Đường này không bị chặn nên không phù hợp cho bài toán này. Thậm chí nếu chúng ta _cắt_ phần nhỏ hơn 0 bằng cách cho chúng bằng 0, _cắt_ các phần lớn hơn 1 bằng cách cho chúng bằng 1, sau đó lấy điểm trên đường thẳng này có tung độ bằng 0.5 làm đường phân chia hai _class_, đây cũng không phải là một lựa chọn tốt. Hãy xem hình dưới đây:\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\LogisticRegression\\ex1_lr.png\" align = \"center\" width = \"800\">\n",
    "<div class = \"thecap\">Hình 3: .</div>\n",
    "</div> \n",
    "Giả sử có thêm vài bạn _sinh viên tiêu biểu_ ôn tập đến 20 giờ và, tất nhiên, thi đỗ. Khi áp dụng mô hình linear regression như hình trên và lấy mốc 0.5 để phân lớp, toàn bộ sinh viên thi trượt vẫn được dự đoán là trượt, nhưng rất nhiều sinh viên thi đỗ cũng được dự đoán là trượt (nếu ta coi điểm x màu xanh lục là _ngưỡng cứng_ để đưa ra kết luận). Rõ ràng đây là một mô hình không tốt.\n",
    "* Đường màu đỏ (chỉ khác với activation function của PLA ở chỗ  hai class là 0 và 1 thay vì -1 và 1) cũng thuộc dạng _ngưỡng cứng_ (hard threshold). PLA không hoạt động trong bài toán này vì dữ liệu đã cho không _linearly separable_. Chú ý rằng các điểm màu đỏ và xanh được vẽ ở hai tung độ khác nhau để tiện cho việc minh họa. Các điểm này được vẽ dùng cả đầu vào \\\\(\\mathbf{x}\\\\) và đầu ra \\\\(y). Khi ta nói _linearly seperable_ là khi ta chỉ dùng đầu vào \\\\(\\mathbf{x}\\\\). \n",
    "* Các đường màu xanh lam và xanh lục phù hợp với bài toán của chúng ta hơn. Chúng có một vài tính chất quan trọng sau:\n",
    "    \n",
    "    - Là hàm số liên tục nhận giá trị thực, bị chặn trong khoảng \\\\((0, 1)\\\\).\n",
    "    - Nếu coi điểm có tung độ là 1/2 làm điểm phân chia thì các điểm càng xa điểm này về phía bên trái có giá trị càng gần 0. Ngược lại, các điểm càng xa điểm này về phía phải có giá trị càng gần 1. Điều này _khớp_ với nhận xét rằng học càng nhiều thì xác suất đỗ càng cao và ngược lại. \n",
    "    - _Mượt_ (smooth) nên có đạo hàm mọi nơi, có thể được lợi trong việc tối ưu.\n",
    "    \n",
    "\n",
    "### Sigmoid function\n",
    "\n",
    "Trong số các hàm số có 3 tính chất nói trên thì hàm _sigmoid_:\n",
    "\\\\[\n",
    "f(s) = \\frac{1}{1 + e^{-s}} \\triangleq \\sigma(s)\n",
    "\\\\]\n",
    "được sử dụng nhiều nhất, vì nó bị chặn trong khoảng \\\\((0, 1)\\\\). Thêm nữa:\n",
    "\\\\[\n",
    "\\lim\\_{s \\rightarrow -\\infty}\\sigma(s) = 0; ~~ \\lim\\_{s \\rightarrow +\\infty}\\sigma(s) = 1 \n",
    "\\\\]\n",
    "Đặc biệt hơn nữa:\n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "\\sigma'(s) &=& \\frac{e^{-1}}{(1 + e^{-s})^2} \\\\\\\n",
    "&=& \\frac{1}{1 + e^{-s}} \\frac{e^{-s}}{1 + e^{-s}} \\\\\\\n",
    "&=& \\sigma(s)(1 - \\sigma(s))\n",
    "\\end{eqnarray}\n",
    "\\\\]\n",
    "Công thức đạo hàm đơn giản thế này giúp hàm số này được sử dụng rộng rãi. Ở phần sau, tôi sẽ chứng minh thêm cho các bạn _vì sao người ta đã tìm ra hàm số đặc biệt này_.\n",
    "\n",
    "## 2. Hàm mất mát và phương pháp tối ưu\n",
    "Chúng ta tạm quên hàm sigmoid đi và làm với công thức tổng quát cho các logistic function.\n",
    "\n",
    "### Xây dựng hàm mất mát\n",
    "\n",
    "Với mô hình như trên, ta có thể giả sử rằng xác suất để một điểm dữ liệu \\\\(\\mathbf{x}\\\\) rơi vào class 1 là \\\\(f(\\mathbf{w}^T\\mathbf{x})\\\\) và rơi vào class 0 là \\\\(1 - f(\\mathbf{w}^T\\mathbf{x})\\\\). Với mô hình được giả sử như vậy, với các điểm dữ liệu training (đã biết đầu ra \\\\(y\\\\)), ta có thể viết như sau:\n",
    "\n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "P(y\\_i = 1 | \\mathbf{x}\\_i; \\mathbf{w}) &=& &f(\\mathbf{w}^T\\mathbf{x}\\_i)  ~~(1) \\\\\\\n",
    "P(y\\_i = 0 | \\mathbf{x}\\_i; \\mathbf{w}) &=& 1 - &f(\\mathbf{w}^T\\mathbf{x}\\_i)  ~~(2) \\\\\\\n",
    "\\end{eqnarray}\n",
    "\\\\]\n",
    "trong đó \\\\( P(y\\_i = 1 | \\mathbf{x}\\_i; \\mathbf{w})\\\\) được hiểu là xác suất xảy ra sự kiện đầu ra \\\\(y_i = 1\\\\) khi biết tham số mô hình \\\\(\\mathbf{w}\\\\) và dữ liệu đầu vào \\\\(\\mathbf{x}_i\\\\). Bạn đọc có thể đọc thêm [Xác suất có điều kiện](https://vi.wikipedia.org/wiki/Xác_suất_có_điều_kiện). Mục đích của chúng ta là tìm các hệ số \\\\(\\mathbf{w}\\\\) sao cho \\\\(f(\\mathbf{w}^T\\mathbf{x}\\_i)\\\\) càng gần với 1 càng tốt với các điểm dữ liệu thuộc class 1 và càng gần với 0 càng tốt với những điểm thuộc class 0.\n",
    "\n",
    "Ký hiệu \\\\(z_i = f(\\mathbf{w}^T\\mathbf{x}\\_i)\\\\) và viết gộp lại hai biểu thức bên trên ta có:\n",
    "\\\\[\n",
    "P(y\\_i| \\mathbf{x}\\_i; \\mathbf{w}) = z\\_i^{y_i}\\(1 - z\\_i\\)^{1- y_i}\n",
    "\\\\]\n",
    "\n",
    "Biểu thức này là tương đương với hai biểu thức \\\\((1)\\\\) và \\\\((2)\\\\) ở trên vì khi \\\\(y\\_i=1\\\\), phần thứ hai của vế phải sẽ triệt tiêu, khi \\\\(y\\_i = 0\\\\), phần thứ nhất sẽ bị triệt tiêu! Chúng ta muốn xác suất này đạt giá trị cao nhất, tức là mô hình gần với dữ liệu đã cho nhất.\n",
    "\n",
    "Xét toàn bộ training set với \\\\(\\mathbf{X} = [\\mathbf{x}\\_1,\\mathbf{x}\\_2, \\dots, \\mathbf{x}\\_N]\\\\) và \\\\(\\mathbf{y} = [y\\_1, y\\_2, \\dots, y\\_N]\\\\), chúng ta cần tìm \\\\(\\mathbf{w}\\\\) để biểu thức sau đây đạt giá trị lớn nhất:\n",
    "\\\\[\n",
    "P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})\n",
    "\\\\]\n",
    "ở đây, ta cũng ký hiệu \\\\(\\mathbf{X, y}\\\\) như các [biến ngẫu nhiên](https://vi.wikipedia.org/wiki/Biến_ngẫu_nhiên) (random variables). Nói cách khác:\n",
    "\\\\[\n",
    "\\mathbf{w} = \\arg\\max_{\\mathbf{w}} P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})\n",
    "\\\\]\n",
    "\n",
    "<a name = \"maximun likelihood estimation\"></a>\n",
    "\n",
    "Bài toán tìm tham số để mô hình gần với dữ liệu nhất trên đây có tên gọi chung là bài toán [_maximum likelihood estimation_](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) với hàm số đằng sau \\\\(\\arg\\max\\\\) được gọi là _likelihood function_. Khi làm việc với các bài toán Machine Learning sử dụng các mô hình xác suất thống kê, chúng ta sẽ gặp lại các bài toán thuộc dạng này, hoặc [_maximum a posteriori estimation_](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), rất nhiều. Tôi sẽ dành 1 bài khác để nói về hai dạng bài toán này.\n",
    "\n",
    "Giả sử thêm rằng các điểm dữ liệu được sinh ra một cách ngẫu nhiên độc lập với nhau (independent), ta có thể viết:\n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) &=& \\prod_{i=1}^N P(y\\_i| \\mathbf{x}\\_i; \\mathbf{w}) \\\\\\\n",
    "&=& \\prod_{i=1}^N z\\_i^{y\\_i}(1 - z\\_i)^{1- y\\_i}\n",
    "\\end{eqnarray}\n",
    "\\\\]\n",
    "với \\\\(\\prod\\\\) là ký hiệu của tích. \n",
    "\n",
    "Trực tiếp tối ưu hàm số này theo \\\\(\\mathbf{w}\\\\) là rất khó! Hơn nữa, khi \\\\(N\\\\) lớn, tích của \\\\(N\\\\) số nhỏ hơn 1 có thể dẫn tới sai số trong tính toán (numerial error) vì tích là một số quá nhỏ. Một phương pháp thường được sử dụng đó là lấy logarit tự nhiên (cơ số \\\\(e\\\\)) của  _likelihood function_ để tránh việc số quá nhỏ và biến phép nhân thành phép cộng, sau đó lấy ngược dấu để được một hàm và coi nó là hàm mất mát. Lúc này bài toán tìm giá trị lớn nhất (maximum likelihood) trở thành bài toán tìm giá trị nhỏ nhất của hàm mất mát:\n",
    "\\\\[\n",
    "J(\\mathbf{w}) = -\\log P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) = -\\sum\\_{i=1}^N(y\\_i \\log {z}\\_i + (1-y\\_i) \\log (1 - {z}\\_i))\n",
    "\\\\]\n",
    "với chú ý rằng \\\\(\\theta_i\\\\) là một hàm số của \\\\(\\mathbf{w}\\\\). Bạn đọc tạm nhớ biểu thức vế phải có tên gọi là _cross entropy_, thường được sử dụng để đo _khoảng cách_ giữa hai phân phối (distributions). Trong bài toán đan gxét của chúng ta, một phân phối chính là dữ liệu được cho, với xác suất chỉ là 0 hoặc 1; phân phối còn lại được tính theo mô hình logistic regression. _Khoảng cách_ giữa hai phân phối nhỏ đồng nghĩa với việc (_có vẻ hiển nhiên là_) hai phân phối đó rất gần nhau. Tính chất cụ thể của hàm số này sẽ được đề cập trong một bài khác mà tầm quan trọng của nó lớn hơn.\n",
    "\n",
    "**Chú ý:** Trong machine learning, logarit thập phân ít được dùng, vì vậy \\\\(\\log\\\\) thường được dùng để ký hiệu logarit tự nhiên.\n",
    "\n",
    "\n",
    "### Tối ưu hàm mất mát với hàm _sigmoid_\n",
    "\n",
    "Với hàm _sigmoid_ \\\\(\\sigma\\\\), nhắc lại tính chất đặt biệt đã chứng minh ở trên: \\\\(\\sigma'(s) = \\sigma(s) ( 1 - \\sigma(s))\\\\) và \\\\(z\\_i = \\sigma(\\mathbf{w}^T\\mathbf{x}\\_i)\\\\), ta có:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial z\\_i}{\\partial \\mathbf{w}} = z\\_i (1 - z\\_i)\\mathbf{x}\\_i\n",
    "\\\\]\n",
    "và:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial \\log z\\_i}{\\partial \\mathbf{w}} = \\frac{1}{z\\_i}\\frac{\\partial z\\_i}{\\partial \\mathbf{w}} = (1-z\\_i)\\mathbf{x}\\_i\n",
    "\\\\]\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial \\log(1 - z\\_i)}{\\partial \\mathbf{w}} = -\\frac{1}{1 - z\\_i}\\frac{\\partial z\\_i}{\\partial \\mathbf{w}} = -z\\_i\\mathbf{x}\\_i\n",
    "\\\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta lại sử dụng phương pháp [Stochastic Gradient Descent](/2017/01/16/gradientdescent2/#-stochastic-gradient-descent)(SGD) ở đây (_Bạn đọc được khuyến khích đọc SGD để có thể hiểu hơn_) . Hàm mất mát với chỉ một điểm dữ liệu \\\\((\\mathbf{x}\\_i, y\\_i)\\\\) là:\n",
    "\\\\[\n",
    "J(\\mathbf{w}; \\mathbf{x}\\_i, y\\_i) = -(y\\_i \\log {z}\\_i + (1-y\\_i) \\log (1 - {z}\\_i))\n",
    "\\\\]\n",
    "\n",
    "Với đạo hàm:\n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}\\_i, y\\_i)}{\\partial \\mathbf{w}} &=& -(\\frac{y\\_i}{z\\_i} - \\frac{1- y\\_i}{1 - z\\_i} ) \\frac{\\partial z\\_i}{\\partial \\mathbf{w}} \\\\\\\n",
    "&=& \\frac{z\\_i - y\\_i}{z\\_i(1 - z\\_i)} \\frac{\\partial z\\_i}{\\partial \\mathbf{w}} ~~ (3)\n",
    "\\end{eqnarray}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để cho biểu thức này trở nên _gọn_ và _đẹp_ hơn, chúng ta sẽ tìm hàm \\\\(z = f(\\mathbf{w}^T\\mathbf{x})\\\\) sao cho mẫu số bị triệt tiêu. Nếu đặt \\\\(s = \\mathbf{w}^T\\mathbf{x}\\\\), chúng ta sẽ có:\n",
    "\\\\[\n",
    "\\frac{\\partial z\\_i}{\\partial \\mathbf{w}} = \\frac{\\partial z\\_i}{\\partial s} \\frac{\\partial s}{\\partial \\mathbf{w}} = \\frac{\\partial z\\_i}{\\partial s} \\mathbf{x}\n",
    "\\\\]\n",
    "Một cách trực quan nhất, ta sẽ tìm hàm số \\\\(z = f(s)\\\\) sao cho:\n",
    "\\\\[\n",
    "\\frac{\\partial z\\_i}{\\partial s} = z(1 - z) ~~ (4)\n",
    "\\\\]\n",
    "để triệt tiêu mẫu số trong biểu thức \\\\((3)\\\\). Chúng ta cùng khởi động một chút với phương trình vi phân đơn giản này. Giải phương trình \\\\((4)\\\\):\n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial z}{z(1-z)} &=& \\partial s \\\\\\\n",
    "\\Leftrightarrow (\\frac{1}{z} + \\frac{1}{1 - z})\\partial z &=&\\partial s \\\\\\\n",
    "\\Rightleftarrow \\log z - \\log(1 - z) &=& s \\\\\\\n",
    "\\Rightleftarrow \\log \\frac{z}{1 - z} &=& s \\\\\\\n",
    "\\Rightleftarrow \\frac{z}{1 - z} &=& e^s \\\\\\\n",
    "\\Rightleftarrow z &=& e^s (1 - z) \\\\\\\n",
    "\\Rightleftarrow z &=& \\frac{e^s}{1 +e^s} =\\frac{1}{1 + e^{-s}} = \\sigma{s}\n",
    "\\end{eqnarray}\n",
    "\\\\]\n",
    "Đến đây các bạn đã hiểu hàm số _sigmoid_ được tạo ra như thế nào rồi chứ!\n",
    "\n",
    "### Công thức cập nhật cho logistic sigmoid regression\n",
    "Tới đây, bạn đọc có thể kiểm tra rằng, với hàm activation là hàm sigmoid:\n",
    "\\\\[\n",
    "\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}\\_i, y\\_i)}{\\partial \\mathbf{w}} = -(y\\_i - z\\_i)\\mathbf{x}\\_i\n",
    "\\\\]\n",
    "Qúa đẹp!\n",
    "\n",
    "Và công thức cập nhật (theo thuật toán [SGD](/2017/01/16/gradientdescent2/#-stochastic-gradient-descent)) cho logistic sigmoid regression là: \n",
    "\\\\[\n",
    "\\mathbf{w} = \\mathbf{w} + \\eta(y\\_i - z\\_i)\\mathbf{x}\\_i\n",
    "\\\\]\n",
    "Và, như thường lệ, chúng ta đã có thể lập trình Python cho ví dụ nêu ở đầu bài.\n",
    "\n",
    "## 3. Ví dụ với Python\n",
    "Trước tiên ta cần khai báo vài thư viện và tạo dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2)\n",
    "\n",
    "X = np.array([[0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, \n",
    "              2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# extened data \n",
    "X = np.concatenate((np.ones((1, X.shape[1])), X), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các hàm cần thiết cho logistic sigmoid regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.092695  ]\n",
      " [ 1.55277242]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "def logistic_sigmoid_regression(X, y, w_init, eta, tol = 1e-4, max_count = 10000):\n",
    "    w = [w_init]    \n",
    "    it = 0\n",
    "    N = X.shape[1]\n",
    "    d = X.shape[0]\n",
    "    count = 0\n",
    "    check_w_after = 20\n",
    "    while count < max_count:\n",
    "#         it += 1\n",
    "        # mix data \n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[:, i].reshape(d, 1)\n",
    "            yi = y[i]\n",
    "            zi = sigmoid(np.dot(w[-1].T, xi))\n",
    "            w_new = w[-1] + eta*(yi - zi)*xi\n",
    "            count += 1\n",
    "            # stopping criteria\n",
    "            if count%check_w_after == 0:                \n",
    "                if np.linalg.norm(w_new - w[-check_w_after]) < tol:\n",
    "                    return w\n",
    "            w.append(w_new)\n",
    "    return w\n",
    "eta = .05 \n",
    "d = X.shape[0]\n",
    "w_init = np.random.randn(d, 1)\n",
    "\n",
    "w = logistic_sigmoid_regression(X, y, w_init, eta)\n",
    "print(w[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Với kết quả tìm được, đầu ra \\\\(y\\\\) có thể được dự đoán theo công thức: `y = sigmoid(-4.1 + 1.537*x)`. Với dữ liệu trong tập training, kết quả là:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03281144  0.04694533  0.06674738  0.09407764  0.13102736  0.17961209\n",
      "   0.17961209  0.24121129  0.31580406  0.40126557  0.49318368  0.58556493\n",
      "   0.67229611  0.74866712  0.86263755  0.90117058  0.92977426  0.95055357\n",
      "   0.96541314  0.98329067]]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(np.dot(w[-1].T, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biểu diễn kết quả này trên đồ thị ta có:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAAF5CAYAAABAyVr6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a906ad210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X0 = X[1, np.where(y == 0)][0]\n",
    "y0 = y[np.where(y == 0)]\n",
    "X1 = X[1, np.where(y == 1)][0]\n",
    "y1 = y[np.where(y == 1)]\n",
    "\n",
    "plt.plot(X0, y0, 'ro', markersize = 8)\n",
    "plt.plot(X1, y1, 'bs', markersize = 8)\n",
    "\n",
    "xx = np.linspace(0, 6, 1000)\n",
    "w0 = w[-1][0][0]\n",
    "w1 = w[-1][1][0]\n",
    "threshold = -w0/w1\n",
    "yy = sigmoid(w0 + w1*xx)\n",
    "plt.axis([-2, 8, -1, 2])\n",
    "plt.plot(xx, yy, 'g-', linewidth = 2)\n",
    "plt.plot(threshold, .5, 'y^', markersize = 8)\n",
    "plt.xlabel('studying hours')\n",
    "plt.ylabel('predicted probability of pass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như chỉ có hai output là 'fail' hoặc 'pass', điểm trên đồ thị của hàm sigmoid tương ứng với xác suất 0.5 được chọn làm _hard threshold_ (ngưỡng cứng). Việc này có thể chứng minh khá dễ dàng (tôi sẽ bàn ở phần dưới). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAFsCAYAAABYeho/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a90878bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "# list of points \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "np.random.seed(22)\n",
    "\n",
    "means = [[2, 2], [4, 2]]\n",
    "cov = [[.7, 0], [0, .7]]\n",
    "N = 20\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = 1)\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = 1)\n",
    "plt.axis('equal')\n",
    "plt.ylim(0, 4)\n",
    "plt.xlim(0, 5)\n",
    "\n",
    "# hide tikcs \n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_ticks([])\n",
    "cur_axes.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "plt.xlabel('$x_1$', fontsize = 20)\n",
    "plt.ylabel('$x_2$', fontsize = 20)\n",
    "plt.savefig('pla1.png', bbox_inches='tight', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.40520562]\n",
      " [ 2.18391091]\n",
      " [ 1.392856  ]]\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X0, X1), axis = 0).T\n",
    "y = np.concatenate((np.zeros((1, N)), np.ones((1, N))), axis = 1).T\n",
    "# Xbar \n",
    "X = np.concatenate((np.ones((1, 2*N)), X), axis = 0)\n",
    "\n",
    "eta = .05 \n",
    "d = X.shape[0]\n",
    "w_init = np.random.randn(d, 1)\n",
    "\n",
    "w = logistic_sigmoid_regression(X, y, w_init, eta, tol = 1e-4, max_count= 10000)\n",
    "print(w[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 16)\n"
     ]
    }
   ],
   "source": [
    "# Make data.\n",
    "xm = np.arange(0, 4, 0.25)\n",
    "xlen = len(xm)\n",
    "ym = np.arange(0, 5, 0.25)\n",
    "ylen = len(ym)\n",
    "xm, ym = np.meshgrid(xm, ym)\n",
    "w0 = w[-1][0][0]\n",
    "w1 = w[-1][1][0]\n",
    "w2 = w[-1][2][0]\n",
    "zm = sigmoid(w0 + w1*xm + w2*ym)\n",
    "print(zm.shape)\n",
    "\n",
    "plt.matshow(zm, cmap='jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
