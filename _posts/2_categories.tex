Có hai cách phổ biến phân nhóm các thuật toán Machine learning. Một là
dựa trên phương thức học (learning style), hai là dựa trên chức năng
(function) (của mỗi thuật toán).

\textbf{Trong trang này:}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{-phan-nhom-dua-tren-phuong-thuc-hoc}{1. Phân nhóm
  dựa trên phương thức học}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{supervised-learning-hoc-co-giam-sat}{Supervised
    Learning (Học có giám sát)}

    \begin{itemize}
    \tightlist
    \item
      \protect\hyperlink{classification-phan-loai}{Classification (Phân
      loại)}
    \item
      \protect\hyperlink{regression-hoi-quy}{Regression (Hồi quy)}
    \end{itemize}
  \item
    \protect\hyperlink{unsupervised-learning-hoc-khong-giam-sat}{Unsupervised
    Learning (Học không giám sát)}

    \begin{itemize}
    \tightlist
    \item
      \protect\hyperlink{clustering-phan-nhom}{Clustering (phân nhóm)}
    \item
      \protect\hyperlink{association}{Association}
    \end{itemize}
  \item
    \protect\hyperlink{semi-supervised-learning-hoc-ban-giam-sat}{Semi-Supervised
    Learning (Học bán giám sát)}
  \item
    \protect\hyperlink{reinforcement-learning-hoc-cung-co}{Reinforcement
    Learning (Học Củng Cố)}
  \end{itemize}
\item
  \protect\hyperlink{-phan-nhom-dua-tren-chuc-nang}{2. Phân nhóm dựa
  trên chức năng}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{regression-algorithms}{Regression Algorithms}
  \item
    \protect\hyperlink{classification-algorithms}{Classification
    Algorithms}
  \item
    \protect\hyperlink{instance-based-algorithms}{Instance-based
    Algorithms}
  \item
    \protect\hyperlink{regularization-algorithms}{Regularization
    Algorithms}
  \item
    \protect\hyperlink{bayesian-algorithms}{Bayesian Algorithms}
  \item
    \protect\hyperlink{clustering-algorithms}{Clustering Algorithms}
  \item
    \protect\hyperlink{artificial-neural-network-algorithms}{Artificial
    Neural Network Algorithms}
  \item
    \protect\hyperlink{dimensionality-reduction-algorithms}{Dimensionality
    Reduction Algorithms}
  \item
    \protect\hyperlink{ensemble-algorithms}{Ensemble Algorithms}
  \end{itemize}
\item
  \protect\hyperlink{-tai-lieu-tham-khao}{3. Tài liệu tham khảo}
\end{itemize}

\subsection{1. Phân nhóm dựa trên phương thức
học}\label{phuxe2n-nhuxf3m-dux1ef1a-truxean-phux1b0ux1a1ng-thux1ee9c-hux1ecdc}

Theo phương thức học, các thuật toán Machine Learning thường được chia
làm 4 nhóm: Supervise learning, Unsupervised learning, Semi-supervised
lerning và Reinforcement learning. \emph{Có một số cách phân nhóm không
có Semi-supervised learning hoặc Reinforcement learning.}

\subsubsection{Supervised Learning (Học có giám
sát)}\label{supervised-learning-hux1ecdc-cuxf3-giuxe1m-suxe1t}

Supervised learning là thuật toán dự đoán đầu ra (outcome) của một dữ
liệu mới (new input) dựa trên các cặp (\emph{input, outcome}) đã biết từ
trước. Cặp dữ liệu này còn được gọi là (\emph{data. label}), tức
(\emph{dữ liệu, nhãn}). Supervised learning là nhóm phổ biến nhất trong
các thuật toán Machine Learning.

Một cách toán học, Supervised learning là khi chúng ra có một tập hợp
biến đầu vào \textbackslash{}( \mathcal{X} =
\textbackslash{}\{\mathbf{x}\_1, \mathbf{x}\_2, \dots,
\mathbf{x}\_N\textbackslash{}\} \textbackslash{}) và một tập hợp nhãn
tương ứng \textbackslash{}( \mathcal{Y} =
\textbackslash{}\{\mathbf{y}\_1, \mathbf{y}\_2, \dots,
\mathbf{y}\_N\textbackslash{}\} \textbackslash{}), trong đó
\textbackslash{}( \mathbf{x}\_i, \mathbf{y}\_i \textbackslash{}) là các
vector. Các cặp dữ liệu biết trước \textbackslash{}( (\mathbf{x}\_i,
\mathbf{y}\_i) \in \mathcal{X} \times \mathcal{Y} \textbackslash{}) được
gọi là tập \emph{training data} (dữ liệu huấn luyện). Từ tập traing data
này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập
\textbackslash{}(\mathcal{X}\textbackslash{}) sang một phần tử (xấp xỉ)
tương ứng của tập \textbackslash{}(\mathcal{Y}\textbackslash{}):

\textbackslash{}{[} \mathbf{y}\_i \approx f(\mathbf{x}\_i),
\textasciitilde{}\textasciitilde{} \forall i = 1, 2, \dots,
N\textbackslash{}{]} Mục đích là xấp xỉ hàm số
\textbackslash{}(f\textbackslash{}) thật tốt để khi có một dữ liệu
\textbackslash{}(\mathbf{x}\textbackslash{}) mới, chúng ta có thể tính
được nhãn tương ứng của nó \textbackslash{}( \mathbf{y} = f(\mathbf{x})
\textbackslash{}).

\textbf{Ví dụ 1:} trong nhận dạng chữ viết tay, ta có ảnh của hàng nghìn
ví dụ của mỗi chữ số được viết bởi nhiều người khác nhau. Chúng ta đưa
các bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh
tương ứng với chữ số nào. Sau khi thuật toán tạo ra (sau khi \emph{học})
một mô hình, tức một hàm số mà đầu vào là một bức ảnh và đầu ra là một
chữ số, khi nhận được một bức ảnh mới mà mô hình \textbf{chưa nhìn thấy
bao giờ}, nó sẽ dự đoán bức ảnh đó chứa chữ số nào.

\begin{verbatim}
<img src="http://www.rubylab.io/img/mnist.png" width = "600">
<!-- <img src="/assets/rl/mdp.png" height="206"> -->
\end{verbatim}

MNIST: bộ cơ sở dữ liệu của chữ số viết tay. (Nguồn: Simple Neural
Network implementation in Ruby)

Ví dụ này khá giống với cách học của con người khi còn nhỏ. Ta đưa bảng
chữ cái cho một đứa trẻ và chỉ cho chúng đây là chữ A, đây là chữ B. Sau
một vài lần được dạy thì trẻ có thể nhận biết được đâu là chữ A, đâu là
chữ B trong một cuốn sách mà chúng chưa nhìn thấy bao giờ.

\textbf{Ví dụ 2:} Thuật toán dò các khuôn mặt trong một bức ảnh đã được
phát triển từ rất lâu. Thời gian đầu, facebook sử dụng thuật toán này để
chỉ ra các khuôn mặt trong một bức ảnh và yêu cầu người dùng \emph{tag
friends} - tức gán nhãn cho mỗi khuôn mặt. Số lượng cặp dữ liệu
(\emph{khuôn mặt, tên người}) càng lớn, độ chính xác ở những lần tự động
\emph{tag} tiếp theo sẽ càng lớn.

\textbf{Ví dụ 3:} Bản thân thuật toán dò tìm các khuôn mặt trong 1 bức
ảnh cũng là một thuật toán Supervised learning với training data (dữ
liệu học) là hàng ngàn cặp (\emph{ảnh, mặt người}) và (\emph{ảnh, không
phải mặt người}) được đưa vào. Chú ý là dữ liệu này chỉ phân biệt
\emph{mặt người} và \emph{không phải mặt ngưòi} mà không phân biệt khuôn
mặt của những người khác nhau.

Thuật toán supervised learning còn được tiếp tục chia nhỏ ra thành hai
loại chính:

\paragraph{Classification (Phân
loại)}\label{classification-phuxe2n-loux1ea1i}

Một bài toán được gọi là \emph{classification} nếu các \emph{label} của
\emph{input data} được chia thành một số hữu hạn nhóm. Ví dụ: Gmail xác
định xem một email có phải là spam hay không; các hãng tín dụng xác định
xem một khách hàng có khả năng thanh toán nợ hay không. Ba ví dụ phía
trên được chia vào loại này.

\paragraph{Regression (Hồi quy)}\label{regression-hux1ed3i-quy}

(tiếng Việt dịch là \emph{Hồi quy}, tôi không thích cách dịch này vì bản
thân không hiểu nó nghĩa là gì)

Nếu \emph{label} không được chia thành các nhóm mà là một giá trị thực
cụ thể. Ví dụ: một căn nhà rộng \textbackslash{}(x \textasciitilde{}
\text{m}\^{}2\textbackslash{}), có \textbackslash{}(y\textbackslash{})
phòng ngủ và cách trung tâm thành phố
\textbackslash{}(z\textasciitilde{} \text{km}\textbackslash{}) sẽ có giá
là bao nhiêu?

Gần đây \href{http://how-old.net/}{Microsoft có một ứng dụng dự đoán
giới tính và tuổi dựa trên khuôn mặt}. Phần dự đoán giới tính có thể coi
là thuật toán \textbf{Classification}, phần dự đoán tuổi có thể coi là
thuật toán \textbf{Regression}. \emph{Chú ý rằng phần dự đoán tuổi cũng
có thể coi là \textbf{Classification} nếu ta coi tuổi là một số nguyên
dương không lớn hơn 150, chúng ta sẽ có 150 class (lớp) khác nhau.}

\subsubsection{Unsupervised Learning (Học không giám
sát)}\label{unsupervised-learning-hux1ecdc-khuxf4ng-giuxe1m-suxe1t}

Trong thuật toán này, chúng ta không biết được \emph{outcome} hay
\emph{nhãn} mà chỉ có dữ liệu đầu vào. Thuật toán unsupervised learning
sẽ dựa vào cấu trúc của dữ liệu để thực hiện một công việc nào đó, ví dụ
như phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimention
reduction) để thuận tiện trong việc lưu trữ và tính toán.

Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu
vào \textbackslash{}(\mathcal{X} \textbackslash{}) mà không biết
\emph{nhãn} \textbackslash{}(\mathcal{Y}\textbackslash{}) tương ứng.

Những thuật toán loại này được gọi là Unsupervised learning vì không
giống như Supervised learning, chúng ta không biết câu trả lời chính xác
cho mỗi dữ liệu đầu vào. Giống như khi ta học, không có thầy cô giáo nào
chỉ cho ta biết đó là chữ A hay chữ B. Cụm \emph{không giám sát} được
đặt tên theo nghĩa này.

Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai
loại:

\paragraph{Clustering (phân nhóm)}\label{clustering-phuxe2n-nhuxf3m}

Một bài toán phân nhóm toàn bộ dữ liệu
\textbackslash{}(\mathcal{X}\textbackslash{}) thành các nhóm nhỏ dựa
trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ: phân nhóm
khách hàng dựa trên hành vi mua hàng. Điều này cũng giống như việc ta
đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình thù và màu sắc khác
nhau, ví dụ tam giác, vuông, tròn với màu xanh và đỏ, sau đó yêu cẩu trẻ
phân chúng thành từng nhóm. Mặc dù không cho trẻ biết mảnh nào tương ứng
với hình nào hoặc màu nào, nhiều khả năng chúng vẫn có thể phân loại các
mảnh ghép theo màu hoặc hình dạng.

\hypertarget{association}{\paragraph{Association}\label{association}}

Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ
liệu cho trước. Ví dụ: những khách hàng nam mua quần áo thường có xu
hướng mua thêm đồng hồ hoặc thắt lưng; những khán giả xem phim Spider
Man thường có xu hướng xem thêm phim Bat Man, dựa vào đó tạo ra một hệ
thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua
sắm.

\subsubsection{Semi-Supervised Learning (Học bán giám
sát)}\label{semi-supervised-learning-hux1ecdc-buxe1n-giuxe1m-suxe1t}

Các bài toán khi chúng ta có một lượng lớn dữ liệu
\textbackslash{}(\mathcal{X}\textbackslash{}) nhưng chỉ một phần trong
chúng được gán nhãn được gọi là Semi-Supervised Learning. Những bài toán
thuộc nhóm này nằm giữa hai nhóm được nêu bên trên.

Một ví dụ điển hình của nhóm này là chỉ có một phần ảnh hoặc văn bản
được gán nhãn (ví dụ bức ảnh về người, động vật hoặc các văn bản khoa
học, chính trị) và phần lớn các bức ảnh/văn bản khác chưa được gán nhãn
được thu thập từ internet. Thực tế cho thấy rất nhiều các bài toàn
Machine Learning thuộc vào nhóm này vì việc thu thập dữ liệu có nhãn tốn
rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí
cần phải có chuyên gia mới gán nhãn được (ảnh y học chẳng hạn). Ngược
lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ
internet.

\subsubsection{Reinforcement Learning (Học Củng
Cố)}\label{reinforcement-learning-hux1ecdc-cux1ee7ng-cux1ed1}

Reinforcement learning là các bài toán giúp cho một hệ thống tự động xác
định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximizing
the performance). Hiện tại, Reinforcement learning chủ yếu được áp dụng
vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nưóc
đi tiếp theo để đạt được điểm số cao nhất.

AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của
Reinforcement learning. (Nguồn: AlphaGo AI Defeats Sedol Again, With
`Near Perfect Game')

\textbf{Ví dụ 1:}
\href{https://gogameguru.com/tag/deepmind-alphago-lee-sedol/}{AlphaGo
gần đây nổi tiếng với việc chơi cờ vây thắng cả con người}.
\href{https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/}{Cờ
vây được xem là có độ phức tạp cực kỳ cao} với tổng số nước đi là xấp xỉ
\textbackslash{}(10\^{}\{761\} \textbackslash{}), so với cờ vua là
\textbackslash{}(10\^{}\{120\} \textbackslash{}) và tổng số nguyên tử
trong toàn vũ trụ là khoảng
\textbackslash{}(10\^{}\{80\}\textbackslash{})!! Vì vậy, thuật toán phải
chọn ra 1 nước đi tối ưu trong số hàng nhiều tỉ tỉ lựa chọn, và tất
nhiên, không thể áp dụng thuật toán tương tự như
\href{https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)}{IBM Deep
Blue} (IBM Deep Blue đã thắng con người trong môn cờ vua 20 năm trước).
Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning
và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ
các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy
nhiên, mục đích cuối cùng của AlphaGo không phải là chơi như con người
mà phải thậm chí thắng cả con người. Vì vậy, sau khi \emph{học} xong các
ván cờ của con người, AlphaGo tự chơi với chính nó với hàng triệu ván
chơi để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi
này được xếp vào loại Reinforcement learning. (Xem thêm tại
\href{https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/}{Google
DeepMind's AlphaGo: How it works}).

\textbf{Ví dụ 2:}
\href{https://www.youtube.com/watch?v=qv6UVOQ0F44}{Huấn luyện cho máy
tính chơi game Mario}. Đây là một chương trình thú vị dạy máy tính chơi
game Mario. Game này đơn giản hơn cờ vây vì tại một thời điểm, người
chơi chỉ phải bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn)
hoặc không cần bấm nút nào. Đồng thời, phản ứng của máy cũng đơn giản
hơn và lặp lại ở mỗi lần chơi (tại thời điểm cụ thể sẽ xuất hiện một
chướng ngại vật cố định ở một vị trí cố định). Đầu vào của thuật toán là
sơ đồ của màn hình tại thời điểm hiện tại, nhiệm vụ của thuật toán là
với đầu vào đó, tổ hợp phím nào nên được bấm. Việc huấn luyện này được
dựa trên điểm số cho việc di chuyển được bao xa trong thời gian bao lâu
trong game, càng xa và càng nhanh thì được điểm thưởng càng cao (điểm
thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập
trình tạo ra). Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối
ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu
công chúa.

Huấn luyện cho máy tính chơi game Mario

\subsection{2. Phân nhóm dựa trên chức
năng}\label{phuxe2n-nhuxf3m-dux1ef1a-truxean-chux1ee9c-nux103ng}

Có một cách phân nhóm thứ hai dựa trên chức năng của các thuật toán.
Trong phần này, tôi xin chỉ liệt kê các thuật toán. Thông tin cụ thể sẽ
được trình bày trong các bài viết khác tại blog này. Trong quá trình
viết, tôi có thể sẽ thêm bớt một số thuật toán.

\hypertarget{regression-algorithms}{\subsubsection{Regression
Algorithms}\label{regression-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{/2016/12/28/linearregression/}{Linear Regression}
\item
  \href{/2017/01/27/logisticregression/\#sigmoid-function}{Logistic
  Regression}
\item
  Stepwise Regression
\end{enumerate}

\hypertarget{classification-algorithms}{\subsubsection{Classification
Algorithms}\label{classification-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear Classifier
\item
  Support Vector Machine (SVM)
\item
  Kernel SVM
\item
  Sparse Represntation-based classification (SRC)
\end{enumerate}

\hypertarget{instance-based-algorithms}{\subsubsection{Instance-based
Algorithms}\label{instance-based-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{/2017/01/08/knn/}{k-Nearest Neighbor (kNN)}
\item
  Learnin Vector Quantization (LVQ)
\end{enumerate}

\hypertarget{regularization-algorithms}{\subsubsection{Regularization
Algorithms}\label{regularization-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ridge Regression
\item
  Least Absolute Shringkage and Selection Operator (LASSO)
\item
  Least-Angle Regression (LARS)
\end{enumerate}

\hypertarget{bayesian-algorithms}{\subsubsection{Bayesian
Algorithms}\label{bayesian-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Naive Bayes
\item
  Gausian Naive Bayes
\end{enumerate}

\hypertarget{clustering-algorithms}{\subsubsection{Clustering
Algorithms}\label{clustering-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{/2017/01/01/kmeans/}{k-Means clustering}
\item
  k-Medians
\item
  Expectation Maximisation (EM)
\end{enumerate}

\hypertarget{artificial-neural-network-algorithms}{\subsubsection{Artificial
Neural Network Algorithms}\label{artificial-neural-network-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{/2017/01/21/perceptron/}{Perceptron}
\item
  \href{/2017/02/17/softmax/}{Softmax Regression}
\item
  \href{/2017/02/24/mlp/}{Multi-layer Perceptron}
\item
  \href{/2017/02/24/mlp/\#-backpropagation}{Back-Propagation}
\end{enumerate}

\hypertarget{dimensionality-reduction-algorithms}{\subsubsection{Dimensionality
Reduction Algorithms}\label{dimensionality-reduction-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Principal Component Analysis (PCA)
\item
  Linear Discriminant Analysis (LDA)
\end{enumerate}

\hypertarget{ensemble-algorithms}{\subsubsection{Ensemble
Algorithms}\label{ensemble-algorithms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Boosting
\item
  AdaBoost
\item
  Random Forest
\end{enumerate}

Và còn rất nhiều các thuật toán khác.

\subsection{3. Tài liệu tham
khảo}\label{tuxe0i-liux1ec7u-tham-khux1ea3o}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \href{http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/}{A
  Tour of Machine Learning Algorithms}
\item
  \href{https://ongxuanhong.wordpress.com/2015/10/22/diem-qua-cac-thuat-toan-machine-learning-hien-dai/}{Điểm
  qua các thuật toán Machine Learning hiện đại}
\end{enumerate}
